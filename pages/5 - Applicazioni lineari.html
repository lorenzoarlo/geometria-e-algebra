<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="icon" href="../resources/logo.png" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
    <script defer id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script defer src="../scripts/TypewritingText.js"></script>
    <link rel="stylesheet" href="../styles/style.css" />
    <script src="../scripts/proof-event.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-78NHLXDQD8"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-78NHLXDQD8');
    </script>
    <title>5 - Applicazioni lineari</title>
</head>

<body>
    <header class="header-container">
        <div class="logo-wrapper">
        </div>
        <div class="header-title">
            <h1>Geometria e algebra</h1>
            <span>5 - Applicazioni lineari</span>
        </div>
        <div class="material-symbols-outlined header-title settings-button">
            <!-- settings -->
        </div>
    </header>
    <div class="main-container">
        <div class="content-container">
            <div class="section part" id="sec5" ><span class="section-header part-title">5 - Applicazioni lineari</span><div class="subsection part" id="subsec5-1" ><span class="subsection-header part-title">5.1 - Applicazioni lineari come funzioni</span><div class="definition environment" id="def5-1" ><span class="definition-header environment-title">Definizione 5.1 - Applicazione lineare tra spazi vettoriali</span>     Una applicazione lineare è una funzione del tipo     <span class="math-tag">\[         f : V \to W     \]</span>     dove <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span> sono spazi vettoriali la cui:     <ul ><li >immagine della somma è uguale alla somma delle immagini         <span class="math-tag">\[             f(v + u) = f(v) + f (y)         \]</span></li><li >immagine del prodotto per uno scalare è uguale al prodotto per uno scalare dell'immagine         <span class="math-tag">\[             f(\lambda \cdot v) = \lambda \cdot f(v)         \]</span></li></ul><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - La condizione in breve</span>         Le due condizioni possono essere riassunte anche nella condizione unica         <span class="math-tag">\[             f(\alpha \cdot v + \beta \cdot u) = \alpha \cdot f(v) + \beta \cdot f(u)         \]</span></div></div><div class="demonstration environment" id="dem5-1" ><span class="demonstration-header environment-title">Dimostrazione 5.1 - Immagine del vettore nullo come condizione necessaria e sufficiente per verificare che una funzione sia un'applicazione lineare</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando l'applicazione lineare         <span class="math-tag">\[             f : V \to W         \]</span>         allora l'immagine del vettore nullo di <span class="math-tag">\( V\)</span> è il vettore nullo di <span class="math-tag">\( W\)</span>, ovvero         <span class="math-tag">\[             f(0_{V}) = 0_{W}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare la proposizione consideriamo l'uguaglianza          <span class="math-tag">\begin{aligned}             & f(0_{V} + 0_{V}) = f(0_{V}) + f(0_{V})             & \iff         \end{aligned}</span>         che è valida per la linearità.         <br ></br>         Considerando quindi <span class="math-tag">\( x \in W\)</span> come l'opposto di <span class="math-tag">\( f(0_{V})\)</span>, aggiungiamolo ad entrambe le parti         <span class="math-tag">\begin{aligned}             & x + f(0_{V} + 0_{V}) = f(0_{V}) + f(0_{V}) + x             & \iff         \end{aligned}</span>         Ora, dato che <span class="math-tag">\( 0_{V} + 0_{V} = 0_{V}\)</span>, è possibile scrivere         <span class="math-tag">\begin{aligned}             & x + f(0_{V}) = f(0_{V}) + f(0_{V}) + x             & \iff         \end{aligned}</span>         e per l'associatività è lecito scrivere         <span class="math-tag">\begin{aligned}             & x + f(0_{V}) = f(0_{V}) + (f(0_{V}) + x)             & \iff         \end{aligned}</span>         Applicando quindi la definizione di opposto, si ha che <span class="math-tag">\( x + f(0_{V}) = 0_{W}\)</span> e quindi         <span class="math-tag">\begin{aligned}             & 0_{W} = f(0_{V}) + 0_{W}             & \iff         \end{aligned}</span>         e dato che qualsiasi elemento sommato all'elemento nullo è l'elemento stesso, si ha che         <span class="math-tag">\[             0_{W} = f(0_{V})         \]</span>         che dimostra la proposizione.     </div></div></div><div class="myexample environment" id="example27" ><span class="myexample-header environment-title">Esempio 27 - Applicazione lineare</span>     Un esempio di applicazione lineare è il limite per successioni convergenti, definibile come     <span class="math-tag">\[         f : \{ a_{n} \} \to \mathbb{R}     \]</span>     dove <span class="math-tag">\( \{ a_{n} \}\)</span> indica l'insieme infinito di tutte le successioni convergenti.     <br ></br>     Considerando i seguenti limiti     <span class="math-tag">\begin{aligned}         & \lim_{n \to + \infty} a_{n} = l         & \lim_{n \to + \infty} b_{n} = m     \end{aligned}</span>     si ha, per l'algebra dei limiti finiti, che      <span class="math-tag">\[         \lim_{n \to + \infty} (\alpha \cdot a_{n} + \beta \cdot b_{n}) = \alpha \cdot l + \beta \cdot m     \]</span>     che dimostra la linearità. </div><div class="definition environment" id="def5-2" ><span class="definition-header environment-title">Definizione 5.2 - Applicazioni lineari come endomorfismi</span>      Un'applicazione lineare il cui dominio è uguale al codominio, ovvero      <span class="math-tag">\[         f: V \to V      \]</span>      è detta endomorfismo. </div><div class="demonstration environment" id="dem5-2" ><span class="demonstration-header environment-title">Dimostrazione 5.2 - Teorema fondamentale delle applicazioni lineari</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due spazi vettoriali <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span>, una base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> per <span class="math-tag">\( V\)</span> e <span class="math-tag">\( n\)</span> vettori <span class="math-tag">\( w_{1}, \ \ldots \ , w_{n}\)</span> di <span class="math-tag">\( W\)</span>, allora esiste ed è unica un'applicazione lineare         <span class="math-tag">\[             f: V \to W         \]</span>         tale che          <span class="math-tag">\[             f(v_{1}) = w_{1}             \quad , \quad               \ldots              \quad , \quad             f(v_{n}) = w_{n}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, è necessario dimostrare:         <ul ><li >esiste tale applicazione lineare. Per dimostrarlo, consideriamo quindi un vettore <span class="math-tag">\( v \in V\)</span> che è combinazione lineare dei vettori di <span class="math-tag">\( B_{V}\)</span><span class="math-tag">\[                 v = a_{1} \cdot v_{1} + \ \ldots \ + a_{n} \cdot v_{n}             \]</span>             Significa quindi che ogni vettore è associato ad una <span class="math-tag">\( n\)</span>-upla di coordinate. Consideriamo quindi la seguente funzione             <span class="math-tag">\[                 f(v) = a_{1} \cdot w_{1} + \ \ldots \ + a_{n} \cdot w_{n}             \]</span>             dove <span class="math-tag">\( (a_{1}, \ \ldots \ , a_{n})\)</span> sono le coordinate di <span class="math-tag">\( v\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>. Inoltre, dato che <span class="math-tag">\( (a_{1}, \ \ldots \ , a_{n})\)</span> sono proprio le coordinate rispetto ai vettori della base, si ha che saranno del tipo             <span class="math-tag">\[                 f(v_{1}) = f(1 \cdot v_{1} + 0 \cdot v_{2} + \ \ldots \ + 0 \cdot v_{n}) = 1 \cdot w_{1}             \]</span>             che dimostra quindi l'esistenza della funzione richiesta.              <br ></br>             &Egrave; ora necessario che tale funzione sia un'applicazione lineare, ovvero che             <ul ><li ><span class="math-tag">\( f(v + u) = f(v) + f(u)\)</span>: per farlo consideriamo le immagini dei vettori, ovvero                 <span class="math-tag">\begin{aligned}                     & f(v) = \sum_{i = 1}^{n} a_{i} \cdot v_{i} \\                     & f(u) = \sum_{i = 1}^{n} b_{i} \cdot v_{i} \\                     & f(v + u) = \sum_{i = 1}^{n} a_{n} \cdot v_{1} + b_{n} \cdot v_{i}                 \end{aligned}</span>                 da cui, grazie alle proprietà delle sommatorie, è facile dimostrare la linearità della somma                 <span class="math-tag">\[                     \sum_{i = 1}^{n} a_{n} \cdot v_{1} + b_{n} \cdot v_{i} = \sum_{i = 1}^{n} a_{i} \cdot v_{i} + \sum_{i = 1}^{n} b_{i} \cdot v_{i}                 \]</span></li><li ><span class="math-tag">\( f(\alpha \cdot v) = \alpha \cdot f(v)\)</span>: per farlo consideriamo le immagini dei vettori, ovvero                 <span class="math-tag">\begin{aligned}                     & f(v) = \sum_{i = 1}^{n} a_{i} \cdot v_{i} \\                     & \alpha \cdot f(v) = \alpha \cdot \sum_{i = 1}^{n} a_{i} \cdot v_{i} \\                     & f(\alpha \cdot v) = \sum_{i = 1}^{n} \alpha \cdot a_{i} \cdot v_{i}                 \end{aligned}</span>                 da cui, grazie alla proprietà distributiva delle sommatorie, è facile dimostrare la linearità del prodotto per uno scalare                 <span class="math-tag">\[                     \sum_{i = 1}^{n} \alpha \cdot a_{i} \cdot v_{i} = \alpha \cdot \sum_{i = 1}^{n} a_{i} \cdot v_{i}                  \]</span></li></ul>             Si è quindi dimostrata la linearità di <span class="math-tag">\( f\)</span>;             </li><li >tale applicazione lineare è unica. Per dimostrarlo consideriamo quindi due applicazioni lineari <span class="math-tag">\( f_{1}\)</span> e <span class="math-tag">\( f_{2}\)</span> tali che             <span class="math-tag">\[                 f_{1}(v_{i}) = w_{i} = f_{2}(v_{i})             \]</span>             Dato che un qualsiasi vettore <span class="math-tag">\( v \in V\)</span> può essere scritto come combinazione lineare di <span class="math-tag">\( B_{V}\)</span> si ha che             <span class="math-tag">\begin{aligned}                 & f_{1}(v) = f_{1}(\sum_{i = 1}^{n} a_{i} \cdot v_{i})                 & \iff             \end{aligned}</span>             e per la linearità di <span class="math-tag">\( f_{1}\)</span> si ha che              <span class="math-tag">\begin{aligned}                 & f_{1}(\sum_{i = 1}^{n} a_{i} \cdot v_{i}) = \sum_{i = 1}^{n} a_{i} \cdot f_{1}(v_{i}) = \sum_{i = 1}^{n} a_{i} \cdot w_{i}                 &             \end{aligned}</span>             Ora, dato che la combinazione lineare di un vettore rispetto ad una base è unica, ripetendo l'analogo ragionamento si otterrebbero sempre i coefficienti <span class="math-tag">\( a_{i}\)</span>, e per questo motivo l'applicazione lineare è unica.         </li></ul>         Si è quindi dimostrato il teorema.     </div></div></div><div class="definition environment" id="def5-3" ><span class="definition-header environment-title">Definizione 5.3 - Immagine di un'applicazione lineare</span>     Considerando un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>, la sua immagine è l'insieme dei vettori di <span class="math-tag">\( W\)</span> a cui corrisponde almeno un elemento di <span class="math-tag">\( V\)</span>, ovvero     <span class="math-tag">\[         Im(f) = \{ w \in W          \ : \          \exists f(v) = w          \}         \quad         \text{con } v \in V     \]</span></div><div class="definition environment" id="def5-4" ><span class="definition-header environment-title">Definizione 5.4 - Applicazione lineare suriettiva</span>     Un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span> si dice suriettiva se ogni elemento del codominio è immagine di almeno un elemento del dominio, ovvero che     <span class="math-tag">\[         W = Im(f)     \]</span></div><div class="definition environment" id="def5-5" ><span class="definition-header environment-title">Definizione 5.5 - Applicazione lineare iniettiva</span>     Un' applicazione lineare <span class="math-tag">\( f: V \to W\)</span> si dice iniettiva se ogni <span class="math-tag">\( y \in im(f)\)</span> è immagine solo di un elemento <span class="math-tag">\( v \in V\)</span>, ovvero se ad ogni <span class="math-tag">\( v \in V\)</span> corrisponde un distinto elemento <span class="math-tag">\( w \in W\)</span>.     <br ></br>     Ciò implica che se <span class="math-tag">\( v_{1}\)</span> è diverso da <span class="math-tag">\( v_{2}\)</span>, allora l'immagine di <span class="math-tag">\( v_{1}\)</span> sarà diversa dall'immagine di <span class="math-tag">\( v_{2}\)</span><span class="math-tag">\[         v_{1} \neq v_{2}          \qquad \implies \qquad         f(v_{1}) \neq f(v_{2})     \]</span></div><div class="definition environment" id="def5-6" ><span class="definition-header environment-title">Definizione 5.6 - Nucleo di un'applicazione lineare</span>    Considerando un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>, il suo nucleo (<span class="math-tag">\( ker(f)\)</span>, dall'inglese "kernel") è l'insieme di tutti i vettori la cui immagine è il vettore nullo di <span class="math-tag">\( W\)</span>, ovvero     <span class="math-tag">\[         ker(f) = \{ v \in V \ : \ f(v) = 0_{W} \}     \]</span></div><div class="myexample environment" id="example28" ><span class="myexample-header environment-title">Esempio 28 - Calcolare il kernel di un'applicazione lineare</span>     Considerando l'applicazione lineare      <span class="math-tag">\[         f: \mathbb{R}^{2} \to \mathbb{R}^{3}     \]</span>     definita come      <span class="math-tag">\[         f(x, y) = (x + 2y, 2x + 4y, 3x + 6y)     \]</span>     allora il nucleo di <span class="math-tag">\( f\)</span> sarà uguale a     <span class="math-tag">\[         ker(f) = \{ (x, y) \in \mathbb{R}^{2}          \ : \          f(x,y) = (0, 0, 0) \}     \]</span>     ovvero tutti i vettori in cui      <span class="math-tag">\[         \left\{         \begin{array}{ccccc}             x & + & 2y & = & 0  \\             2x & + & 4y & = & 0  \\             3x & + & 6y & = & 0         \end{array}         \right.         \qquad         \iff         \qquad         \left\{         \begin{array}{ccc}             x & = & -2t \\             y & = & t         \end{array}         \right.     \]</span>     ovvero si ha che il nucleo di <span class="math-tag">\( f\)</span> sarà l'insieme     <span class="math-tag">\[         \left(         \begin{array}{c}             x \\             y         \end{array}         \right)         =         t \cdot          \left(         \begin{array}{c}             -2 \\             1         \end{array}         \right)         \qquad         \implies         \qquad         ker(f) = \ <(-2, 1)>     \]</span></div><div class="demonstration environment" id="dem5-3" ><span class="demonstration-header environment-title">Dimostrazione 5.3 - Nucleo di <span class="math-tag">\( f: V \to W\)</span> come sottospazio vettoriale di <span class="math-tag">\( V\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>, allora il nucleo di <span class="math-tag">\( f\)</span> (ovvero <span class="math-tag">\( ker(f)\)</span>) è un sottospazio vettoriale di <span class="math-tag">\( V\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che         <ul ><li >la somma di due vettori <span class="math-tag">\( v_{1}, v_{2} \in ker(f)\)</span> è sempre interna al nucleo di <span class="math-tag">\( f\)</span>, ovvero             <span class="math-tag">\[                 f(v_{1}) = 0_{W}                  \text{ e }                 f(v_{2}) = 0_{W}                  \quad                 \implies                 \quad                 f(v_{1} + v_{2}) = 0_{W}             \]</span>             Per dimostrarlo consideriamo quindi la linearità di <span class="math-tag">\( f\)</span><span class="math-tag">\begin{aligned}                 & f(v_{1} + v_{2}) = f(v_{1}) + f(v_{2})                  & \iff             \end{aligned}</span>             e sostituendoli con la loro immagine si ha che             <span class="math-tag">\begin{aligned}                 & f(v_{1}) + f(v_{2}) = 0_{W} + 0_{W} = 0_{W}                 &             \end{aligned}</span>             ovvero che la somma è interna;             </li><li >il prodotto per uno scalare è interno al nucleo di <span class="math-tag">\( f\)</span>, ovvero considerando un vettore <span class="math-tag">\( v \in V\)</span><span class="math-tag">\[                 f(v) = 0_{W}                 \qquad                 \implies                 \qquad                 f(\alpha \cdot v) = 0_{W}             \]</span>             Per dimostrarlo consideriamo quindi la linearità di <span class="math-tag">\( f\)</span><span class="math-tag">\begin{aligned}                 & f(\alpha \cdot v) = \alpha \cdot f(v)                 & \iff             \end{aligned}</span>             e sostituendo <span class="math-tag">\( v\)</span> con la sua immagine si ha che             <span class="math-tag">\begin{aligned}                 & \alpha \cdot f(v)= \alpha \cdot 0_{W} = 0_{W}                 &             \end{aligned}</span>             ovvero che il prodotto per uno scalare è interno.         </li></ul>         Si è quindi dimostrato che il nucleo di <span class="math-tag">\( f\)</span> è un sottospazio vettoriale di <span class="math-tag">\( V\)</span>.     </div></div></div><div class="demonstration environment" id="dem5-4" ><span class="demonstration-header environment-title">Dimostrazione 5.4 - Immagine di <span class="math-tag">\( f: V \to W\)</span> come sottospazio vettoriale di <span class="math-tag">\( W\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Data un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>, allora l'immagine di <span class="math-tag">\( f\)</span> (ovvero <span class="math-tag">\( Im(f)\)</span>) è un sottospazio vettoriale di <span class="math-tag">\( W\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che         <ul ><li >la somma di due vettori <span class="math-tag">\( w_{1}, w_{2} \in Im(f)\)</span> è sempre interna all'immagine di <span class="math-tag">\( f\)</span>.             <br ></br>             Per dimostrarlo consideriamo che da definizione <span class="math-tag">\( w_{1} = f(v_{1})\)</span> e che <span class="math-tag">\( w_{2} = f(v_{2})\)</span>, quindi             <span class="math-tag">\begin{aligned}                 & w_{1} + w_{2} = f(v_{1}) + f(v_{2})                 & \iff             \end{aligned}</span>             quindi, per linearità di <span class="math-tag">\( f\)</span> si ha che             <span class="math-tag">\begin{aligned}                 & f(v_{1}) + f(v_{2}) = f(v_{1} + v_{2})                 &             \end{aligned}</span>             ovvero che il vettore somma è ancora un vettore appartenente all'immagine;             </li><li >il prodotto per uno scalare è interno all'immagine di <span class="math-tag">\( f\)</span>.             <br ></br>             Per dimostrarlo consideriamo quindi un vettore <span class="math-tag">\( w \in Im(f)\)</span> e consideriamo che tale vettore è per definizione <span class="math-tag">\( w = f(v)\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & \alpha \cdot w = \alpha \cdot f(v)                 & \iff             \end{aligned}</span>             e per linearità di <span class="math-tag">\( f\)</span> si ha che             <span class="math-tag">\begin{aligned}                 & \alpha \cdot f(v) = f(\alpha \cdot v)                 &             \end{aligned}</span>             ovvero che il prodotto per uno scalare è interno.         </li></ul>         Si è quindi dimostrato che l'immagine di <span class="math-tag">\( f\)</span> è un sottospazio vettoriale di <span class="math-tag">\( W\)</span>.     </div></div></div><div class="demonstration environment" id="dem5-5" ><span class="demonstration-header environment-title">Dimostrazione 5.5 - Nucleo di un'applicazione lineare iniettiva composto solo dal vettore nullo</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f : V \to W\)</span>, <span class="math-tag">\( f\)</span>, essa è iniettiva se e solo se il suo nucleo è composto solamente dal vettore nullo, ovvero         <span class="math-tag">\[             ker(f) = \{ 0_{V} \}         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Presenza del vettore nullo</span>         &Egrave; necessario notare che il vettore nullo è sempre presente nel nucleo, in quanto l'immagine del vettore nullo attraverso un'applicazione lineare è sempre il vettore nullo.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che:         <ul ><li >"Se <span class="math-tag">\( f\)</span> è iniettiva, allora il nucleo di <span class="math-tag">\( f\)</span> sarà composto solo dal vettore nullo", infatti considerando per assurdo un vettore <span class="math-tag">\( v \in ker(f)\)</span> non nullo (<span class="math-tag">\( v \neq 0_{V}\)</span>), otterremmo             <span class="math-tag">\[                 f(v) = 0_{W} = f(0_{V})             \]</span>             che renderebbe <span class="math-tag">\( f\)</span> non iniettiva in quanto il vettore nullo <span class="math-tag">\( 0_{W}\)</span> sarebbe immagine di due vettori distinti.             </li><li >"Se nel nucleo di <span class="math-tag">\( f\)</span> è presente solo il vettore nullo, allora <span class="math-tag">\( f\)</span> è iniettiva". Consideriamo per assurdo che due vettori <span class="math-tag">\( v_{1}, v_{2} \in V\)</span> abbiano la stessa immagine, ovvero             <span class="math-tag">\begin{aligned}                 f(v_{1}) = f(v_{2})             \end{aligned}</span>             &Egrave; quindi possibile aggiungere l'opposto di <span class="math-tag">\( f(v_{2})\)</span> da entrambi i lati e, utilizzando la linearità di <span class="math-tag">\( f\)</span>, scrivere              <span class="math-tag">\begin{aligned}                 & f(v_{1}) - f(v_{2}) = 0_{W}                 & \iff \\                 & f(v_{1} - v_{2}) = 0_{W}                 &             \end{aligned}</span>             Tale uguaglianza ci permette di dire che il vettore <span class="math-tag">\( v_{1} - v_{2}\)</span> è un vettore del nucleo e, dato che per ipotesi nel nucleo è presente solo il vettore nullo, possiamo dire che quel vettore è proprio il vettore nullo <span class="math-tag">\( 0_{V}\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & v_{1} - v_{2} = 0_{V}                  & \iff             \end{aligned}</span>             Ora, "portando a destra" <span class="math-tag">\( v_{2}\)</span><span class="math-tag">\begin{aligned}                 & v_{1} = v_{2}                 &             \end{aligned}</span>             si ha che se due elementi hanno la stessa immagine (e nel nucleo è presente il solo vettore nullo), allora tali elementi sono uguali (che è la definizione di iniettività).          </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-6" ><span class="demonstration-header environment-title">Dimostrazione 5.6 - Equazione dimensionale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f : V \to W\)</span>, si ha che la dimensione di <span class="math-tag">\( V\)</span> è uguale alla somma delle dimensioni dell'immagine e del nucleo di <span class="math-tag">\( f\)</span>, ovvero         <span class="math-tag">\[             dim(V) = dim(Im(f)) + dim(ker(f))         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - "Non posso né scendere né salire"</span>         Ciò significa che fissata la dimensione di <span class="math-tag">\( V\)</span>, nel caso cresca la dimensione dell'immagine di <span class="math-tag">\( f\)</span>, la dimensione del nucleo dovrà diminuire (e viceversa). Tutto ciò deve avvenire ricordandosi che tali valori devono essere positivi o nulli.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, consideriamo <span class="math-tag">\( n = dim(V)\)</span> e una base per il nucleo di <span class="math-tag">\( f\)</span><span class="math-tag">\( B_{ker}\)</span><span class="math-tag">\[             B_{ker} = (v_{1}, \ \ldots \ , v_{r})         \]</span>         composta da <span class="math-tag">\( r\)</span> vettori.         <br ></br>         Applichiamo il teorema del completamento ad una base per ottenere una base <span class="math-tag">\( B_{V}\)</span> per <span class="math-tag">\( V\)</span>, ovvero         <span class="math-tag">\[             B_{V} = (v_{1}, \ \ldots \ , v_{r}, v_{r + 1}, \ \ldots \ , v_{n})         \]</span>         composta da <span class="math-tag">\( n\)</span> vettori.         <br ></br>         Consideriamo ora le immagini dei vettori aggiunti, ovvero          <span class="math-tag">\[             f(v_{r + 1})             \quad , \quad             \ldots              \quad , \quad              f(v_{n})         \]</span>         di cui è possibile dire che sono una base dell'immagine di <span class="math-tag">\( f\)</span>, in quanto:         <ul ><li >sono un sistema di generatori per <span class="math-tag">\( Im(f)\)</span>. Infatti, considerando un qualsiasi vettore <span class="math-tag">\( f(v) \in Im(f)\)</span>, è possibile scrivere <span class="math-tag">\( v\)</span> come combinazione lineare di <span class="math-tag">\( B_{V}\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & f(v) = f(x_{1} \cdot v_{1} + \ \ldots \ + x_{r} \cdot v_{r} + x_{r + 1} \cdot v_{r + 1} + \ \ldots \ + x_{n} \cdot v_{n})                 & \iff             \end{aligned}</span>              Ora, per la linearità di <span class="math-tag">\( f\)</span> è possibile scrivere l'immagine di un generico vettore <span class="math-tag">\( v\)</span> come              <span class="math-tag">\begin{aligned}                 & x_{1} \cdot f(v_{1}) + \ \ldots \ + x_{r} \cdot f(v_{r}) + x_{r + 1} \cdot f(v_{r + 1}) + \ \ldots \ + x_{n} \cdot f(v_{n})                 & \iff             \end{aligned}</span>             Ricordando che i vettori <span class="math-tag">\( (v_{1}, \ \ldots \ , v_{r})\)</span> sono elementi del nucleo (e quindi la loro immagine è nulla), è possibile scrivere             <span class="math-tag">\begin{aligned}                 & f(v) = x_{r + 1} \cdot f(v_{r + 1}) + \ \ldots \ + x_{n} \cdot f(v_{n})                 &             \end{aligned}</span>             che dimostra il fatto tali vettori siano un sistema di generatori per <span class="math-tag">\( Im(f)\)</span>.             </li><li >sono vettori linearmente indipendenti. Per dimostrare ciò supponiamo             <span class="math-tag">\begin{aligned}                 & a_{r+1} \cdot f(v_{r + 1}) + \ \ldots \ + a_{n} \cdot f(v_{n}) = 0_{W}                 & \iff             \end{aligned}</span>             Grazie alla linearità di <span class="math-tag">\( f\)</span>, è possibile scrivere             <span class="math-tag">\begin{aligned}                 & f(a_{r + 1} \cdot v_{r + 1} + \ \ldots \ + a_{n} \cdot v_{n}) = 0_{W}                 &              \end{aligned}</span>             che ci permette di dire che il vettore <span class="math-tag">\( a_{r + 1} \cdot v_{r + 1} + \ \ldots \ + a_{n} \cdot v_{n}\)</span> appartiene al nucleo di <span class="math-tag">\( f\)</span> (in quanto la sua immagine è il vettore nullo). Tale vettore può quindi essere rappresentato come combinazione lineare di <span class="math-tag">\( B_{ker}\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & a_{r + 1} \cdot v_{r + 1} + \ \ldots \ + a_{n} \cdot v_{n} = a_{1} \cdot v_{1} + \ \ldots \ + a_{r} \cdot v_{r}                 & \iff             \end{aligned}</span>             e aggiungendo da entrambi lati l'opposto della combinazione lineare di <span class="math-tag">\( B_{ker}\)</span> si ottiene             <span class="math-tag">\begin{aligned}                - a_{1} \cdot v_{1} - \ \ldots \ - a_{r} \cdot v_{r} + a_{r + 1} \cdot v_{r + 1} + \ \ldots \ + a_{n} \cdot v_{n} = 0_{V}                 & \iff             \end{aligned}</span>             i cui coefficienti possono essere solo nulli, in quanto l'ipotesi iniziale era che tali vettori fossero linearmente indipendenti (in quanto sono i vettori della base <span class="math-tag">\( B_{V}\)</span>).              <br ></br>             Si è dimostrata quindi la lineare indipendenza.         </li></ul>        Dunque si è dimostrato che i vettori <span class="math-tag">\( (f(v_{r + 1}), \ \ldots \ , f(v_{n}))\)</span> sono una base per <span class="math-tag">\( Im(f)\)</span>: ciò implica che la sua dimensione sia        <span class="math-tag">\[             dim(Im(f)) = n - r        \]</span>        Ricordando che la dimensione di <span class="math-tag">\( B_{ker} = r\)</span>, è ora semplice dimostrare che la dimensione di <span class="math-tag">\( V\)</span> sia <span class="math-tag">\( n\)</span> (come da ipotesi):        <span class="math-tag">\[             \begin{array}{ccccc}                 dim(V) & = & dim(Im(f)) & + & dim(ker(f)) \\                 n & = & (n - r) & + & r             \end{array}        \]</span>        ovvero,        <span class="math-tag">\[             n = n        \]</span>        che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-7" ><span class="demonstration-header environment-title">Dimostrazione 5.7 - Iniettività e suriettività di un'applicazione lineare nel caso la dimensione degli spazi vettoriali sia uguale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due spazi vettoriali <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span> della stessa dimensione, ovvero         <span class="math-tag">\[             dim(V) = dim(W)         \]</span>         e un'applicazione lineare <span class="math-tag">\( f : V \to W\)</span> allora <span class="math-tag">\( f\)</span> è iniettiva se e solo se è suriettiva.      </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che, considerando la dimensione di <span class="math-tag">\( V\)</span> uguale alla dimensione di <span class="math-tag">\( W\)</span>":         <ul ><li >"se <span class="math-tag">\( f\)</span> è iniettiva allora è anche suriettiva".              <br ></br>             Per dimsotrare ciò si consideri l'equazione dimensionale, ovvero             <span class="math-tag">\[                 dim(V) = dim(Im(f)) + dim(ker(f))             \]</span>             e, ricordando che il nucleo delle applicazioni lineari iniettive è composto solo dal vettore nullo (ovvero che la dimensione del nucleo è <span class="math-tag">\( 0\)</span>)             <span class="math-tag">\[                 dim(ker(f)) = 0             \]</span>             è semplice ottenere che la dimensione di <span class="math-tag">\( V\)</span> è uguale alla dimensione dell'immagine di <span class="math-tag">\( f\)</span><span class="math-tag">\[                 dim(V) = dim(Im(f)) + 0             \]</span>             e, dato che la dimensione di <span class="math-tag">\( V\)</span> è uguale alla dimensione di <span class="math-tag">\( W\)</span>, si ottiene la definizione di suriettività, ovvero             <span class="math-tag">\[                 dim(Im(f)) = dim(W)             \]</span>             che dimostra la prima parte della proposizione.             </li><li >"se <span class="math-tag">\( f\)</span> è suriettiva allora è anche iniettiva". Per dimostrare consideriamo l'equazione dimensionale, ovvero             <span class="math-tag">\[                 dim(V) = dim(Im(f)) + dim(ker(f))             \]</span>             e, ricordando la definizione di suriettività, ovvero             <span class="math-tag">\[                 Im(f) = W             \]</span>             è semplice ottenere (per il fatto che la dimensione di <span class="math-tag">\( V\)</span> sia uguale alla dimensione di <span class="math-tag">\( W\)</span>) che la dimensione del nucleo è <span class="math-tag">\( 0\)</span>, ovvero             <span class="math-tag">\[                 dim(ker(f)) = 0             \]</span>             Provando ciò, si è dimostrata l'iniettività grazie al fatto che se il nucleo di <span class="math-tag">\( f\)</span> è nullo, allora <span class="math-tag">\( f\)</span> è iniettiva. Ciò dimostra la seconda implicazione.         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-8" ><span class="demonstration-header environment-title">Dimostrazione 5.8 - Suriettività di <span class="math-tag">\( f: V \to W\)</span> e relazione tra le dimensioni di <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f : V \to W\)</span> suriettiva, allora si ha che la dimensione di <span class="math-tag">\( V\)</span> è maggiore o uguale alla dimensione di <span class="math-tag">\( W\)</span>, ovvero         <span class="math-tag">\[             dim(V) \geq dim(W)         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Altra prospettiva</span>         Questa proposizione implica che un'applicazione lineare non può essere suriettiva se la dimensione di <span class="math-tag">\( V\)</span> è minore della dimensione di <span class="math-tag">\( W\)</span>,     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando la definizione di suriettività         <span class="math-tag">\[             Im(f) = W         \]</span>         e l'equazione dimensionale, si ha che         <span class="math-tag">\[             \begin{array}{ccccc}                 dim(V) & = & dim(Im(f)) & + & dim(ker(f)) \\                 dim(V) & = & dim(W) & + & dim(ker(f))             \end{array}         \]</span>         che dimostra la proposizione, in quanto la dimensione del nucleo di <span class="math-tag">\( f\)</span> è al minimo nulla.     </div></div></div><div class="demonstration environment" id="dem5-9" ><span class="demonstration-header environment-title">Dimostrazione 5.9 - Iniettività di <span class="math-tag">\( f: V \to W\)</span> e relazione tra le dimensioni di <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f : V \to W\)</span>, iniettiva, allora si ha che la dimensione di <span class="math-tag">\( V\)</span> è minore o uguale alla dimensione di <span class="math-tag">\( W\)</span>, ovvero         <span class="math-tag">\[             dim(V) \leq dim(W)         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Altra prospettiva</span>         Questa proposizione implica che un'applicazione lineare non può essere iniettiva se la dimensione di <span class="math-tag">\( V\)</span> è maggiore della dimensione di <span class="math-tag">\( W\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando che il nucleo di un'applicazione lineare iniettiva contiene solo il vettore nullo, si ha che         <span class="math-tag">\[             dim(ker(f)) = 0         \]</span>         e considerando l'equazione dimensionale si ha che         <span class="math-tag">\[             \begin{array}{ccccc}                 dim(V) & = & dim(Im(f)) & + & dim(ker(f)) \\                 dim(V) & = & dim(Im(f)) & + & 0             \end{array}         \]</span>         Dato che l'immagine di <span class="math-tag">\( f\)</span> è inclusa in <span class="math-tag">\( W\)</span>, ovvero         <span class="math-tag">\[             dim(Im(f)) \leq dim(W)         \]</span>         si ha che          <span class="math-tag">\[             dim(V) \leq dim(W)         \]</span>         che dimostra la proposizione.     </div></div></div><div class="definition environment" id="def5-7" ><span class="definition-header environment-title">Definizione 5.7 - Applicazione lineare biunivoca come isomorfismo e spazi vettoriali isomorfi</span>     Un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span> biunivoca è detta isomorfismo.      <br ></br>     Se esiste un isomorfismo <span class="math-tag">\( f : V \to W\)</span> diciamo che <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span> sono isomorfi, ovvero     <span class="math-tag">\[         V \cong W     \]</span></div><div class="myexample environment" id="example29" ><span class="myexample-header environment-title">Esempio 29 - Spazi vettoriali isomorfi</span>     Considerando lo spazio vettoriale dei polinomi a coefficienti reali nella variabile <span class="math-tag">\( x\)</span> di grado minore o uguale a tre , ovvero <span class="math-tag">\( \mathbb{R}^{\leq 3}[x]\)</span> e lo spazio vettoriale delle matrici quadrate <span class="math-tag">\( 2 \times 2\)</span>, ovvero <span class="math-tag">\( M_{2 \times 2}(\mathbb{R})\)</span>, allora esiste il seguente isomorfismo      <span class="math-tag">\[         f(a x^{3} + b x^{2} + c x + d) =          \left(         \begin{array}{cc}             a & b \\             c & d         \end{array}         \right)     \]</span>     che rende i due spazi vettoriali isomorfi. </div><div class="demonstration environment" id="dem5-10" ><span class="demonstration-header environment-title">Dimostrazione 5.10 - Funzione inversa di un isomorfismo come isomorfismo</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un isomorfismo <span class="math-tag">\( f: V \to W\)</span>, allora anche l'applicazione lineare          <span class="math-tag">\[             f^{-1} : W \rightarrow V         \]</span>         è un isomorfismo.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare che <span class="math-tag">\( f^{-1}\)</span> è un isomorfismo è sufficiente dimostrare la sua linearità (infatti la suriettività e l'iniettività sono già dimostrate in quanto è una funzione inversa). Per dimostrarlo è necessario dimostrare che         <ul ><li >la somma è lineare, ovvero che             <span class="math-tag">\[                 f^{-1}(w_{1} + w_{2}) = f^{-1}(w_{1}) + f^{-1}(w_{2})             \]</span>             Consideriamo quindi che ogni vettore <span class="math-tag">\( w_{1}, w_{2} \in W\)</span> è immagine un distinto vettore <span class="math-tag">\( v_{1}, v_{2} \in V\)</span> attraverso <span class="math-tag">\( f\)</span>, ovvero             <span class="math-tag">\begin{aligned}                & f^{-1}(w_{1} + w_{2}) = f^{-1}(f(v_{1}) + f(v_{2}))                & \iff             \end{aligned}</span>             e per la linearità di <span class="math-tag">\( f\)</span>, si ha che             <span class="math-tag">\begin{aligned}                & f^{-1}(w_{1} + w_{2}) = f^{-1}(f(v_{1} + v_{2}))                & \iff             \end{aligned}</span>             e dato che la composizione con l'inversa annulla la funzione si ha che             <span class="math-tag">\begin{aligned}                & f^{-1}(w_{1} + w_{2}) = v_{1} + v_{2}                & \iff             \end{aligned}</span>             Infine, ricordando che <span class="math-tag">\( v_{1} = f^{-1}(w_{1})\)</span> si ottiene             <span class="math-tag">\begin{aligned}                & f^{-1}(w_{1} + w_{2}) = f^{-1}(w_{1}) + f^{-1}(w_{2})                &             \end{aligned}</span>             che dimostra la proprietà;             </li><li >il prodotto per uno scalare è lineare, ovvero che             <span class="math-tag">\[                 f^{-1}(\alpha \cdot w) = \alpha \cdot f^{-1}(w)              \]</span>             Consideriamo quindi che ogni vettore <span class="math-tag">\( w \in W\)</span> è immagine un distinto vettore <span class="math-tag">\( v \in V\)</span> attraverso <span class="math-tag">\( f\)</span>, ovvero             <span class="math-tag">\begin{aligned}                & f^{-1}(\alpha \cdot w) = f^{-1}(\alpha \cdot f(v))                 & \iff             \end{aligned}</span>             e per la linearità di <span class="math-tag">\( f\)</span>, si ha che             <span class="math-tag">\begin{aligned}                & f^{-1}(\alpha \cdot w) = f^{-1}(f(\alpha \cdot v))                & \iff             \end{aligned}</span>             e dato che la composizione con l'inversa annulla la funzione si ha che             <span class="math-tag">\begin{aligned}                & f^{-1}(\alpha \cdot w) = \alpha \cdot v                & \iff             \end{aligned}</span>             Infine, ricordando che <span class="math-tag">\( v = f^{-1}(w)\)</span> si ottiene             <span class="math-tag">\begin{aligned}                & f^{-1}(\alpha \cdot w) = \alpha \cdot f^{-1}(w)                &             \end{aligned}</span>             che dimostra la proprietà;         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-11" ><span class="demonstration-header environment-title">Dimostrazione 5.11 - Composizione di applicazioni lineari come applicazione lineare</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due applicazioni lineari <span class="math-tag">\( f: V \to U\)</span> e <span class="math-tag">\( g: U \to W\)</span>  allora anche la loro composizione, ovvero         <span class="math-tag">\[             g \circ f: V \to W         \]</span>         è lineare.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione considerando <span class="math-tag">\( v_{1}, v_{2} \in V\)</span> e la funzione <span class="math-tag">\( h = g \circ f\)</span>, è necessario dimostrare la seguente proprietà         <span class="math-tag">\[             h(\alpha \cdot v_{1} + \beta \cdot v_{2}) = \alpha \cdot h(v_{1}) + \beta \cdot h(v_{2}         \]</span>         Innanzitutto è possibile riscrivere la funzione composta <span class="math-tag">\( h\)</span> come         <span class="math-tag">\begin{aligned}            & h(\alpha \cdot v_{1} + \beta \cdot v_{2}) = g(f(\alpha \cdot v_{1} + \beta \cdot v_{2}))            & \iff         \end{aligned}</span>         e per la linearità di <span class="math-tag">\( f\)</span>, si può scrivere         <span class="math-tag">\begin{aligned}             & g(f(\alpha \cdot v_{1} + \beta \cdot v_{2})) = g(\alpha \cdot f(v_{1}) + \beta \cdot f(v_{2}))             & \iff         \end{aligned}</span>         mentre per la linearità di <span class="math-tag">\( g\)</span>, si può scrivere         <span class="math-tag">\begin{aligned}             & g(\alpha \cdot f(v_{1}) + \beta \cdot f(v_{2})) = \alpha \cdot g(f(v_{1}) + \beta \cdot g(f(v_{2}))             & \iff         \end{aligned}</span>         che è equivalente a         <span class="math-tag">\begin{aligned}             & \alpha \cdot h(v_{1}) + \beta \cdot h(v_{2})         \end{aligned}</span>         che dimostra la proprietà.     </div></div></div><div class="demonstration environment" id="dem5-12" ><span class="demonstration-header environment-title">Dimostrazione 5.12 - Corollario - Composizioni di isomorfismi</span>     Dato il corollario     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due isomorfismi <span class="math-tag">\( f: V \to U\)</span> e <span class="math-tag">\( g: U \to W\)</span>  allora anche la loro composizione, ovvero         <span class="math-tag">\[             g \circ f: V \to W         \]</span>         è un isomorfismo.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò è sufficiente ricordare che la composizione di applicazioni lineari è anch'essa una funzione lineare e che la composizione di due funzioni biunivoche è anch'essa una funzione biunivoca.     </div></div></div><div class="demonstration environment" id="dem5-13" ><span class="demonstration-header environment-title">Dimostrazione 5.13 - Isomorfismo associato alla <span class="math-tag">\( n\)</span>-upla delle coordinate</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando uno spazio vettoriale <span class="math-tag">\( V\)</span> di dimensione <span class="math-tag">\( n\)</span> e una base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> per <span class="math-tag">\( V\)</span>, allora è sempre possibile costruire un isomorfismo         <span class="math-tag">\[             f : V \to \mathbb{R}^{n}         \]</span>         tale che         <span class="math-tag">\[             f(v) = (x_{1}, \ \ldots \ , x_{n})         \]</span>         dove <span class="math-tag">\( x_{i}\)</span> sono le coordinate di <span class="math-tag">\( v\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>.     </div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Spazi vettoriali di dimensione <span class="math-tag">\( n\)</span> isomorfi a <span class="math-tag">\( \mathbb{R}^{n}\)</span></span>         Dato che tale isomorfismo esiste, significa che ogni spazio vettoriale <span class="math-tag">\( V\)</span> di dimensione <span class="math-tag">\( n\)</span> è isomorfo a <span class="math-tag">\( \mathbb{R}^{n}\)</span>, ovvero         <span class="math-tag">\[             n = dim(V)              \qquad             \implies             \qquad             V \cong \mathbb{R}^{n}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario provare che         <ul ><li >tale funzione è lineare, ovvero considerando <span class="math-tag">\( u, w \in V\)</span><span class="math-tag">\[                 f(\alpha \cdot u + \beta \cdot w) = \alpha \cdot f(u) + \beta \cdot f(w)             \]</span>             Per farlo, si parterà dalle due espressioni separatamente per poi raggiungere lo stesso risultato.              <br ></br>             Consideriamo quindi             <span class="math-tag">\begin{aligned}                 & f(\alpha \cdot u + \beta \cdot w)                  & \iff             \end{aligned}</span>             e che <span class="math-tag">\( u, w\)</span> possono essere scritti come combinazione lineare di <span class="math-tag">\( B_{V}\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & f(\alpha \cdot \sum_{i = 1}^{n} a_{i} \cdot v_{i} + \beta \cdot \sum_{i = 1}^{n} b_{i} \cdot v_{i})                 & \iff             \end{aligned}</span>             e, per le proprietà delle sommatorie,             <span class="math-tag">\begin{aligned}                 & f(\sum_{i = 1}^{n} (\alpha \cdot a_{i} + \beta \cdot b_{i}) \cdot v_{i})                 & \iff             \end{aligned}</span>             da cui si ottiene la seguente <span class="math-tag">\( n\)</span>-upla di coordinate             <span class="math-tag">\[                 (\alpha \cdot a_{1} + \beta \cdot b_{1}, \ \ldots \ , \alpha \cdot a_{n} + \beta \cdot b_{n})             \]</span>             Ora, considerando l'espressione a destra             <span class="math-tag">\begin{aligned}                 & \alpha \cdot f(u) + \beta \cdot f(w)                 & \iff             \end{aligned}</span>             è possibile applicare l'immagine dei vettori             <span class="math-tag">\begin{aligned}                 & = \alpha \cdot (a_{1}, \ \ldots \ , a_{n}) + \beta \cdot (b_{1}, \ \ldots \ , b_{n})                 & \iff             \end{aligned}</span>             da cui si ottiene la seguente <span class="math-tag">\( n\)</span>-upla             <span class="math-tag">\[                 (\alpha \cdot a_{1} + \beta \cdot b_{1}, \ \ldots \ , \alpha \cdot a_{n} + \beta \cdot b_{n})             \]</span>             Si è quindi dimostrata la linearità.             </li><li >tale funzione è biunivoca. Per dimostrarlo è sufficiente considerare per la suriettività che <span class="math-tag">\( B_{V}\)</span> è un sistema di generatori (ovvero che ogni vettori di <span class="math-tag">\( V\)</span> è combinazione lineare di <span class="math-tag">\( B_{V}\)</span>) e per l'iniettività che ogni vettore corrisponde ad un'unica <span class="math-tag">\( n\)</span>-upla di coordinate rispetto ad una base;         </li></ul>         Si è  quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-14" ><span class="demonstration-header environment-title">Dimostrazione 5.14 - Spazi vettoriali della stessa dimensione isomorfi</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due spazi vettoriali <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span>, questi hanno la stessa dimensione se e solo se sono isomorfi.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che:          <ul ><li >"se due spazi vettoriali hanno la stessa dimensione allora sono isomorfi". Considerando <span class="math-tag">\( n\)</span> la dimensione di tali spazi, si ha che <span class="math-tag">\( V\)</span> è isomorfo a <span class="math-tag">\( \mathbb{R}^{n}\)</span> e che anche <span class="math-tag">\( W\)</span> è isomorfo a <span class="math-tag">\( \mathbb{R}^{n}\)</span>, per cui esisteranno le seguenti applicazioni lineari             <span class="math-tag">\begin{aligned}                 & V \cong \mathbb{R}^{n}                 \qquad                 \implies                 \qquad                 f: V \to \mathbb{R}^{n}                 & \\                 & W \cong \mathbb{R}^{n}                 \qquad                 \implies                 \qquad                 g: W \to \mathbb{R}^{n}                 &             \end{aligned}</span>             da cui è possibile dire che esiste l'isomorfismo <span class="math-tag">\( g^{-1}: \mathbb{R}^{n} \to W\)</span> e che esisterà l'isomorfismo <span class="math-tag">\( h = g^{-1} \circ f\)</span><span class="math-tag">\[                 h: V \to W             \]</span>             per cui è possibile dire che <span class="math-tag">\( V\)</span> è isomorfo a <span class="math-tag">\( W\)</span>.             <br ></br>             Si è quindi dimostrata l'implicazione.             </li><li >"se due spazi vettoriali sono isomorfi allora hanno la stessa dimensione". Per farlo, consideriamo l'equazioni dimensionale di <span class="math-tag">\( f: V \to W\)</span> e, consci del fatto che <span class="math-tag">\( f\)</span> è biettiva (ovvero suriettiva e iniettiva) si ha che             <span class="math-tag">\[                 \begin{array}{ccccc}                     dim(V) & = & dim(ker(f)) & + & dim(Im(f))  \\                     dim(V) & = & 0 & + & dim(W)                 \end{array}             \]</span>             in quanto <span class="math-tag">\( dim(ker(f)) = 0\)</span> perchè <span class="math-tag">\( f\)</span> è iniettiva e <span class="math-tag">\( dim(Im(f)) = dim(W)\)</span> perchè <span class="math-tag">\( f\)</span> è suriettiva.             <br ></br>             Si è quindi dimostrata l'implicazione.         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div></div><div class="subsection part" id="subsec5-2" ><span class="subsection-header part-title">5.2 - Applicazioni lineari come matrici</span><div class="demonstration environment" id="dem5-15" ><span class="demonstration-header environment-title">Dimostrazione 5.15 - Matrice associata ad un'applicazione lineare</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando         <ul ><li >un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>;             </li><li >la base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> di <span class="math-tag">\( V\)</span>;             </li><li >la base <span class="math-tag">\( B_{W} = (w_{1}, \ \ldots \ , w_{m})\)</span> di <span class="math-tag">\( W\)</span>;             </li><li >il vettore <span class="math-tag">\( (x_{1}, \ \ldots \ , x_{n})\)</span> delle coordinate di un qualsiasi vettore <span class="math-tag">\( v \in V\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>;             </li><li >il vettore <span class="math-tag">\( (y_{1}, \ \ldots \ , y_{m})\)</span> delle coordinate di <span class="math-tag">\( f(v)\)</span> rispetto a <span class="math-tag">\( B_{W}\)</span></li></ul>         allora esiste una matrice <span class="math-tag">\( A\)</span> associata ad <span class="math-tag">\( f\)</span> rispetto alle basi <span class="math-tag">\( B_{V}\)</span> e <span class="math-tag">\( B_{W}\)</span>, tale che         <span class="math-tag">\[             \left(             \begin{array}{c}                 y_{1} \\                 \vdots \\                 y_{m}             \end{array}             \right)             =              \left(             \begin{array}{ccc}                 a_{1, 1} & \cdots & a_{1, n} \\                 \vdots & \ddots & \vdots \\                 a_{m, 1} & \cdots & a_{m,n}             \end{array}             \right)             \odot              \left(             \begin{array}{c}                 x_{1} \\                 \vdots \\                 x_{n}             \end{array}             \right)         \]</span>         ovvero         <span class="math-tag">\[             A = M_{B_{V}, B_{W}}(f)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò è necessario provare che le coordinate di un qualsiasi vettore <span class="math-tag">\( f(v)\)</span> rispetto a <span class="math-tag">\( B_{W}\)</span> possono essere calcolate utilizzando le coordinate di <span class="math-tag">\( v\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>.          <br ></br>         Consideriamo la combinazione lineare di un generico vettore <span class="math-tag">\( v \in V\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>, ovvero         <span class="math-tag">\[             v = \sum_{j = 1}^{n} x_{j} \cdot v_{j}         \]</span>         e la combinazione lineare dell'immagine di un vettore della base <span class="math-tag">\( B_{V}\)</span>, ovvero         <span class="math-tag">\[             f(v_{j}) = \sum_{i = 1}^{m} a_{i, j} \cdot w_{i}         \]</span>         dove <span class="math-tag">\( a_{i, j}\)</span> indica il valore della coordinata <span class="math-tag">\( i\)</span> nella combinazione lineare rispetto a <span class="math-tag">\( B_{W}\)</span> del vettore <span class="math-tag">\( v_{j}\)</span>.         <br ></br>         Allora si ha che <span class="math-tag">\( f(v)\)</span> può essere scritto come         <span class="math-tag">\begin{aligned}             & f(v) = f(\sum_{j = 1}^{n} x_{j} \cdot v_{j})             & \iff         \end{aligned}</span>         e, per la linearità di <span class="math-tag">\( f\)</span> si può scrivere         <span class="math-tag">\begin{aligned}             & f(v) = \sum_{j = 1}^{n} x_{j} \cdot f(v_{j})             & \iff         \end{aligned}</span>         Ora, è possibile sostituire <span class="math-tag">\( f(v_{j})\)</span> con la sua combinazione lineare rispetto a <span class="math-tag">\( B_{W}\)</span>, ovvero         <span class="math-tag">\begin{aligned}             & f(v) = \sum_{j = 1}^{n} x_{j} \cdot \sum_{i = 1}^{m} a_{i, j} \cdot w_{i}             & \iff         \end{aligned}</span>         Grazie alla proprietà distributiva delle sommatorie è possibile "portare all'interno della sommatoria più interna" <span class="math-tag">\( x_{j}\)</span>, ottenendo         <span class="math-tag">\begin{aligned}             & f(v) = \sum_{j = 1}^{n} \sum_{i = 1}^{m} a_{i, j} \cdot  x_{j} \cdot w_{i}             & \iff         \end{aligned}</span>         ed è ora possibile, sempre per le proprietà delle sommatorie, scambiarle         <span class="math-tag">\begin{aligned}             & f(v) = \sum_{i = 1}^{m} \sum_{j = 1}^{n} a_{i, j} \cdot  x_{j} \cdot w_{i}             & \iff         \end{aligned}</span>         e grazie alla proprietà associativa         <span class="math-tag">\begin{aligned}             & f(v) = \sum_{i = 1}^{m}              \left( \sum_{j = 1}^{n} a_{i, j} \cdot x_{j} \right) \cdot w_{i}             & \iff         \end{aligned}</span>         Infine si può esplicitare la sommatoria ottenendo         <span class="math-tag">\begin{aligned}             & f(v) = \left( \sum_{j = 1}^{n} a_{1, j} \cdot x_{j} \right) \cdot w_{1}             , \ \ldots \ ,             \left( \sum_{j = 1}^{n} a_{m, j} \cdot x_{j} \right) \cdot w_{m}             &         \end{aligned}</span>         ovvero <span class="math-tag">\( f(v)\)</span> come combinazione lineare di <span class="math-tag">\( B_{W}\)</span>. Tali coordinate, sono esattamente la formula del prodotto tra matrici, e ciò dimostra il teorema.      </div></div></div><div class="definition environment" id="def5-8" ><span class="definition-header environment-title">Definizione 5.8 - Costruzione della matrice associata ad un'applicazione lineare</span>     Considerando     <ul ><li >un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>;         </li><li >la base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> di <span class="math-tag">\( V\)</span>;         </li><li >la base <span class="math-tag">\( B_{W} = (w_{1}, \ \ldots \ , w_{m})\)</span> di <span class="math-tag">\( W\)</span>;         </li><li >il vettore <span class="math-tag">\( (x_{1}, \ \ldots \ , x_{n})\)</span> delle coordinate di un qualsiasi vettore <span class="math-tag">\( v \in V\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>;         </li><li >il vettore <span class="math-tag">\( (y_{1}, \ \ldots \ , y_{m})\)</span> delle coordinate di <span class="math-tag">\( f(v)\)</span> rispetto a <span class="math-tag">\( B_{W}\)</span></li></ul>     per costruire la matrice associata si deve calcolare per ogni vettore della base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> la sua immagine rispetto a <span class="math-tag">\( f\)</span> e si deve scrivere tale vettore come combinazione lineare rispetto a <span class="math-tag">\( B_{W}\)</span>.      <br ></br>     Le coordinate della combinazione lineare saranno una colonna della matrice.     <div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - </span>        &Egrave; chiaro che il procedimento di creazione della matrice di <span class="math-tag">\( A\)</span> è molto più semplice utilizzando la base canonica, tuttavia nell'utilizzo della matrice può risultare più adatto (e più semplice) utilizzare alcune basi specifiche.     </div></div><div class="myexample environment" id="example30" ><span class="myexample-header environment-title">Esempio 30 - Calcolare l'immagine di un vettore tramite la matrice associata all'applicazione lineare</span>     Per scrivere la matrice associata all'applicazione lineare     <span class="math-tag">\begin{aligned}         & f: \mathbb{R}^{2} \to \mathbb{R}^{3}          & f((x, y)) = ((2x + 4y), (x + 2y), (4x - 3y))     \end{aligned}</span>     si considerino le basi     <span class="math-tag">\begin{aligned}         & B_{\mathbb{R}^{2}} = ((2,0 ), (0, -3)) \\         & B_{\mathbb{R}^{3}} = ((1 ,0, 0), (0, 2, 0), (0, 0, 3))     \end{aligned}</span>     Calcoliamo quindi l'immagine dei vettori di <span class="math-tag">\( B_{\mathbb{R}^{2}}\)</span>:     <span class="math-tag">\begin{aligned}         & f(v_{1}) = (4, 2, 8) \\         & f(v_{2}) = (-12, -6, 9)     \end{aligned}</span>     e scriviamoli combinazione lineare di <span class="math-tag">\( B_{\mathbb{R}^{3}}\)</span> (per trovare le coordinate si utilizza un sistema lineare).     <br ></br>     Si ha quindi che le coordinate di <span class="math-tag">\( f(v_{1})\)</span> rispetto a <span class="math-tag">\( B_{\mathbb{R}^{3}}\)</span><span class="math-tag">\begin{aligned}         & f(v_{1}) = (4, 2, 8) = a \cdot (1, 0, 0) + b \cdot (0, 2, 0) + c \cdot (0, 0, 3)         & \iff \\         &         \left\{         \begin{array}{ccccccc}             4 & = & a \cdot 1 & + & b \cdot 0 & + & c \cdot 0  \\             2 & = & a \cdot 0 & + & b \cdot 2 & + & c \cdot 0  \\             8 & = & a \cdot 0 & + & b \cdot 0 & + & c \cdot 3          \end{array}         \right.         \qquad          \iff          \qquad         \left\{         \begin{array}{ccc}             a & = & 4 \\             b & = & 1 \\             c & = & \frac{8}{3}         \end{array}         \right.         &     \end{aligned}</span>     e le coordinate di <span class="math-tag">\( f(v_{2})\)</span><span class="math-tag">\begin{aligned}         & f(v_{2}) = (-12, -6, 9) = a \cdot (1, 0, 0) + b \cdot (0, 2, 0) + c \cdot (0, 0, 3)         & \iff \\         &         \left\{         \begin{array}{ccccccc}             -12 & = & a \cdot 1 & + & b \cdot 0 & + & c \cdot 0  \\             -6 & = & a \cdot 0 & + & b \cdot 2 & + & c \cdot 0  \\             9 & = & a \cdot 0 & + & b \cdot 0 & + & c \cdot 3          \end{array}         \right.         \qquad          \iff          \qquad         \left\{         \begin{array}{ccc}             a & = & -12 \\             b & = & -3 \\             c & = & 3         \end{array}         \right.         &     \end{aligned}</span>     Andando a combinare le due colonne si avrà la matrice <span class="math-tag">\( 3 \times 2\)</span><span class="math-tag">\begin{aligned}         A =          \left(         \begin{array}{cc}             4 & -12 \\             1 & -3 \\             \frac{8}{3} & 3         \end{array}         \right)     \end{aligned}</span>     Ora, nel caso si volesse ottenere l'immagine del vettore <span class="math-tag">\( (1, 1)\)</span> è necessario prima calcolare le sue coordinate rispetto a <span class="math-tag">\( B_{\mathbb{R}^{2}}\)</span> (di cui si salterà il passaggio)     <span class="math-tag">\[         v \underset{B_{\mathbb{R}^{2}}}{\equiv} (\frac{1}{2}, -\frac{1}{3})     \]</span>     e moltiplicando quindi <span class="math-tag">\( A \odot X\)</span> si avrà     <span class="math-tag">\begin{aligned}         \left(         \begin{array}{cc}             4 & -12 \\             1 & -3 \\             \frac{8}{3} & 3         \end{array}         \right)         \odot          \left(         \begin{array}{c}             \frac{1}{2} \\             -\frac{1}{3}         \end{array}         \right)         =         \left(         \begin{array}{c}             6             \frac{3}{2} \\             \frac{1}{3} \\         \end{array}         \right)     \end{aligned}</span>     Per calcolare quindi il vettore immagine corrispondente a queste coordinate si avrà     <span class="math-tag">\[         f((1, 1)) = 6 \cdot (1, 0, 0) + \frac{3}{2} \cdot (0, 2, 0) + \frac{1}{3} \cdot (0, 0, 3) = (6, 3, 1)     \]</span>     che è l'immagine del vettore scelto. </div><div class="demonstration environment" id="dem5-16" ><span class="demonstration-header environment-title">Dimostrazione 5.16 - Dimensione dell'immagine di un'applicazione lineare come rango della matrice associata</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>, una base <span class="math-tag">\( B_{V}\)</span> per lo spazio vettoriale <span class="math-tag">\( V\)</span>, una base <span class="math-tag">\( B_{W}\)</span> per lo spazio vettoriale <span class="math-tag">\( W\)</span>, la matrice associata ad <span class="math-tag">\( f\)</span> rispetto <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span>, allora la dimensione dell'immagine di <span class="math-tag">\( f\)</span> è uguale al rango di <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span>, ovvero         <span class="math-tag">\[             dim(Im(f)) = r(M_{B_{V}, B_{W}}(f))         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò consideriamo la matrice <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span> scritta per colonne, ovvero         <span class="math-tag">\begin{aligned}             M_{B_{V}, B_{W}}(f) =              \left(              \begin{array}{ccccc}                 C_{1} & \left|\right. & \cdots & \left|\right. & C_{n}             \end{array}             \right)         \end{aligned}</span>         Consideriamo l'applicazione lineare          <span class="math-tag">\[             L_{V}: v \to (x_{1}, \ \ldots \ , x_{n})         \]</span>         che trasforma un qualsiasi vettore <span class="math-tag">\( v \in V\)</span> nella <span class="math-tag">\( n\)</span>-upla delle sue coordinate rispetto a <span class="math-tag">\( B_{V}\)</span> e l'applicazione lineare         <span class="math-tag">\[             L_{W}: w \rightarrow (y_{1}, \ \ldots \ , y_{m})         \]</span>         che trasforma un qualsiasi vettore <span class="math-tag">\( w \in W\)</span> nella <span class="math-tag">\( m\)</span>-upla delle sue coordinate rispetto a <span class="math-tag">\( B_{W}\)</span>: entrambe queste applicazioni lineari sono isomorfismi.         <br ></br>         Il rango è quindi la dimensione delle colonne di <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span> mentre la dimensione dell'immagine è la dimensione dello spazio generato dalla chiusura lineare <span class="math-tag">\( <f(v_{1}), \ \ldots \ , f(v_{n})>\)</span>. Questi due spazi, ovvero quello delle colonne di <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span> e quello della chiusura lineare di <span class="math-tag">\( <f(v_{1}), \ \ldots \ , f(v_{n})>\)</span>, sono isomorfi perchè è possibile vedere che l'applicazione <span class="math-tag">\( L_{W}\)</span> non fa altro che restituire una colonna della matrice <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span>, ovvero         <span class="math-tag">\[            L_{W}(f(v_{1})) = C_{1}             \qquad            \ldots            \qquad            L_{W}(f(v_{n})) = C_{n}          \]</span>         Dato che due isomorfismi hanno sempre la stessa dimensione, possiamo dire che la proposizione è dimostrata.     </div></div></div><div class="definition environment" id="def5-9" ><span class="definition-header environment-title">Definizione 5.9 - Corollario - Equazione dimensionale con il rango della matrice associata</span>     Considerando un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span> e la sua matrice associata <span class="math-tag">\( A\)</span>, allora la dimensione del nucleo di <span class="math-tag">\( f\)</span> è uguale alla differenza tra la dimensione di <span class="math-tag">\( V\)</span> e il rango di <span class="math-tag">\( A\)</span>, ovvero     <span class="math-tag">\[         dim(ker(f)) = dim(V) - r(A)     \]</span></div><div class="definition environment" id="def5-10" ><span class="definition-header environment-title">Definizione 5.10 - Matrice associata della composizione di applicazioni lineari</span>     Considerando due applicazioni lineari <span class="math-tag">\( f: V \to W\)</span> e <span class="math-tag">\( g: W \to U\)</span>, le matrici <span class="math-tag">\( A = M_{B_{V}, B_{W}}(f)\)</span> associata ad <span class="math-tag">\( f\)</span> e la matrice <span class="math-tag">\( B = M_{B_{W}, B_{U}}(g)\)</span> associata a <span class="math-tag">\( g\)</span>, è possibile calcolare la matrice associata dell'applicazione composta <span class="math-tag">\( g \circ f\)</span>, semplicemente moltiplicando la matrice <span class="math-tag">\( B\)</span> per la matrice <span class="math-tag">\( A\)</span>, ovvero     <span class="math-tag">\[           M_{B_{V}, B_{U}}(g \circ f) = M_{B_{W}, B_{U}}(g) \odot M_{B_{V}, B_{W}}(f) = B \odot A     \]</span></div><div class="demonstration environment" id="dem5-17" ><span class="demonstration-header environment-title">Dimostrazione 5.17 - Matrice associata all'applicazione lineare identità</span>     Considerando la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando l'applicazione lineare identità <span class="math-tag">\( \text{id}_{V}: V \to V\)</span> tale che <span class="math-tag">\( f(v) = v\)</span>, considerando per <span class="math-tag">\( V\)</span> una qualsiasi base <span class="math-tag">\( B_{V}\)</span>, allora si ha che la matrice <span class="math-tag">\( M_{B_{V}, B_{V}(\text{id}_{V}})\)</span> è la matrice identica.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione consideriamo il metodo con cui si costruisce la matrice associata ad un'applicazione lineare. Considerando la generica base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> e il fatto che stiamo trattando l'applicazione lineare identità, si ha che le coordinate di tali vettori rispetto a <span class="math-tag">\( B_{V}\)</span> sono         <span class="math-tag">\[             ((1, 0, \ \ldots \ , 0), (0, 1, \ \ldots \ , 0), \ \ldots \ , (0, 0, \ \ldots \ , 1)         \]</span>         e ponendoli sulle colonne della matrice associata si ottiene         <span class="math-tag">\[             \left(             \begin{array}{cccc}                 1 & 0 & \cdots & 0 \\                 0 & 1 & \cdots & 0 \\                 \vdots & \vdots & \ddots & 0 \\                 0 & 0 & \cdots & 1             \end{array}             \right)         \]</span>         che è esattamente la matrice identica.     </div></div></div><div class="definition environment" id="def5-11" ><span class="definition-header environment-title">Definizione 5.11 - Matrice associata all'inversa di un'applicazione lineare</span>     Considerando un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span> associata ad una matrice <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span>, se esiste l'applicazione lineare inversa <span class="math-tag">\( f^{-1}: W \to V\)</span>, essa è associata all'inversa della matrice <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span>, ovvero alla matrice <span class="math-tag">\( M_{B_{W}, B_{V}}(f^{-1})\)</span>, ovvero     <span class="math-tag">\[         M_{B_{V}, B_{W}}(f) \odot M_{B_{W}, B_{V}}(f^{-1}) = I     \]</span></div><div class="definition environment" id="def5-12" ><span class="definition-header environment-title">Definizione 5.12 - Matrice per il cambiamento di base</span>     Considerando l'endomorfismo identità <span class="math-tag">\( \text{id}_{V} : V \to V\)</span> e due basi (<span class="math-tag">\( B_{1}\)</span> per lo spazio vettoriale "partenza" e <span class="math-tag">\( B_{2}\)</span> per lo spazio vettoriale "arrivo"), la matrice     <span class="math-tag">\[         M_{B_{1}, B_{2}}(\text{id}_{V})      \]</span>     si dirà "matrice per il cambiamento di base" in quanto trasforma le coordinate di un vettore <span class="math-tag">\( v \in V\)</span> rispetto alla base <span class="math-tag">\( B_{1}\)</span> nelle coordinate di <span class="math-tag">\( v\)</span> rispetto alla base <span class="math-tag">\( B_{2}\)</span>. </div><div class="definition environment" id="def5-13" ><span class="definition-header environment-title">Definizione 5.13 - Costruzione della matrice cambiamento di base utilizzando le basi canoniche</span>     Per costruire una matrice cambiamento di base da una base <span class="math-tag">\( B_{1}\)</span> a una base <span class="math-tag">\( B_{2}\)</span> è necessario calcolare le coordinate dell'immagine dei vettori di <span class="math-tag">\( B_{1}\)</span> rispetto a <span class="math-tag">\( B_{2}\)</span>: tale calcolo non è immediato a meno che <span class="math-tag">\( B_{2}\)</span> non è la base canonica <span class="math-tag">\( \varepsilon\)</span>. Generalizzando, è possibile costruire una qualsiasi matrice cambiamento di base senza effettuare tale calcolo.     <br ></br>     Consideriamo quindi la matrice cambio di base <span class="math-tag">\( M_{B_{1}, \varepsilon}(\text{id}_{V})\)</span> che trasforma le coordinate rispetto a <span class="math-tag">\( B_{1}\)</span> in quelle rispetto alla base canonica. A questo punto, per ottenere la matrice <span class="math-tag">\( M_{B_{1}, B_{2}}(\text{id}_{V})\)</span> servirebbe la matrice <span class="math-tag">\( M_{\varepsilon, B_{2}}(\text{id}_{V})\)</span>, che non risolve il problema iniziale. Si ha tuttavia che la matrice <span class="math-tag">\( M_{\varepsilon, B_{2}}(\text{id}_{V})\)</span> è l'inversa della matrice <span class="math-tag">\( M_{B_{2}, \varepsilon}(\text{id}_{V})\)</span> e per questo motivo, si ha che     <span class="math-tag">\[         M_{B_{1}, B_{2}}(\text{id}_{V}) = (M_{B_{2}, \varepsilon}(\text{id}_{V}))^{-1} \odot M_{B_{1}, \varepsilon}(\text{id}_{V})     \]</span></div><div class="myexample environment" id="example31" ><span class="myexample-header environment-title">Esempio 31 - Costruzione della matrice cambiamento di base utilizzando le basi canoniche</span>     Considerando le basi <span class="math-tag">\( B_{1} = ((1,7), (3,-2))\)</span> e <span class="math-tag">\( B_{2} = ((1, 1), (1, -1))\)</span> per <span class="math-tag">\( \mathbb{R}_{2}\)</span>, creare la matrice cambiamento di base <span class="math-tag">\( M_{B_{1}, B_{2}}(\text{id}_{\mathbb{R}^{2}})\)</span>.     <br ></br>     Per creare tale matrice, occorre considerare la base canonica per <span class="math-tag">\( \mathbb{R}^{2}\)</span>, ovvero <span class="math-tag">\( \varepsilon = ((1, 0), (0, 1))\)</span>. Ora consideriamo la matrice <span class="math-tag">\( M_{B_{1}, \varepsilon}(\text{id}_{\mathbb{R}^{2}})\)</span>, che è la seguente     <span class="math-tag">\[         M_{B_{1}, \varepsilon}(\text{id}_{\mathbb{R}^{2}}) =          \left(         \begin{array}{cc}             1 & 3 \\             7 & -2         \end{array}         \right)     \]</span>     A questo punto, occorre calcolare la matrice <span class="math-tag">\( M_{B_{2}, \varepsilon}(\text{id}_{\mathbb{R}^{2}})\)</span>, che sarà     <span class="math-tag">\[         M_{B_{2}, \varepsilon}(\text{id}_{\mathbb{R}^{2}}) =         \left(         \begin{array}{cc}             1 & 1 \\             1 & -1         \end{array}         \right)     \]</span>     e occorre calcolarne l'inversa, per cui     <span class="math-tag">\[         (M_{B_{2}, \varepsilon}(\text{id}_{\mathbb{R}^{2}}))^{-1} =          \left(         \begin{array}{cc}             1 & 1 \\             1 & -1         \end{array}         \right)^{-1}         =         \left(         \begin{array}{ccccc}             1 & 0 & \left|\right. & \frac{1}{2} & \frac{1}{2} \\             0 & 1 & \left|\right. & \frac{1}{2} & -\frac{1}{2}         \end{array}         \right)         =          \left(         \begin{array}{cc}             \frac{1}{2} & \frac{1}{2} \\             \frac{1}{2} & -\frac{1}{2}         \end{array}         \right)     \]</span>     Infine, per ottenere la matrice cambiamento di base, è sufficiente moltiplicare     <span class="math-tag">\[         \begin{array}{ccccc}             (M_{B_{2}, \varepsilon}(\text{id}_{\mathbb{R}^{2}}))^{-1}              & \odot &              M_{B_{1}, \varepsilon}(\text{id}_{\mathbb{R}^{2}})               & = &              M_{B_{1}, B_{2}}(\text{id}_{\mathbb{R}^{2}})              \\             \\             \left(             \begin{array}{cc}                 \frac{1}{2} & \frac{1}{2} \\                 \frac{1}{2} & -\frac{1}{2}             \end{array}             \right)             & \odot &             \left(             \begin{array}{cc}                 1 & 3 \\                 7 & -2             \end{array}             \right)             & = &             \left(             \begin{array}{cc}                 4 & \frac{1}{2} \\                 -3 & \frac{5}{2}             \end{array}             \right)                     \end{array}     \]</span></div><div class="demonstration environment" id="dem5-18" ><span class="demonstration-header environment-title">Dimostrazione 5.18 - Matrice invertibile se e solo se il rango è massimo</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La matrice <span class="math-tag">\( A \in M_{n \times n}(\mathbb{K})\)</span> è invertibile se e solo se il suo rango è massimo, ovvero         <span class="math-tag">\[             r(A) = n         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Rango come lineare indipendenza</span>         Ciò significa che è invertibile se e solo se tutte le sue righe (e colonne) sono linearmente indipendenti tra loro.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare questa proposizione è necessario dimostrare che:         <ul ><li >"se <span class="math-tag">\( A\)</span> è invertibile, il suo rango è massimo", per farlo consideriamo l'applicazione lineare <span class="math-tag">\( f_{A}\)</span> che moltiplica una matrice <span class="math-tag">\( X\)</span> qualsiasi <span class="math-tag">\( n \times n\)</span> per la matrice <span class="math-tag">\( A\)</span>, ovvero             <span class="math-tag">\[                 f_{A}(X) = A \odot X             \]</span>             Ciò ci permette di dire che la matrice <span class="math-tag">\( A\)</span> è associata all'applicazione lineare <span class="math-tag">\( f_{A}\)</span> (in quanto il suo compito è moltiplicare per una matrice, e ciò è possibile solo se la matrice associata è proprio <span class="math-tag">\( A\)</span>. Ora, dato che <span class="math-tag">\( A\)</span> è una matrice associata ad un'applicazione lineare, si ha che la sua inversa (che esiste per ipotesi) è associata all'applicazione lineare inversa di <span class="math-tag">\( f_{A}\)</span>, e se <span class="math-tag">\( f_{A}\)</span> è invertibile significa che è biunivoca, in particolare suriettiva, ovvero che l'immagine coincide con il codominio e quindi la dimensione dell'immagine deve essere uguale alla dimensione del codominio             <span class="math-tag">\[                 r(A) = dim(Im(f))                 \quad                 \text{e}                  \quad                 dim(Im(f)) = n                 \quad                 \implies                  \quad                 r(A) = n             \]</span>             che dimostra la prima parte della proposizione;             </li><li >"se il rango di <span class="math-tag">\( A\)</span> è massimo, allora essa è invertibile", per farlo si consideri che la matrice <span class="math-tag">\( A\)</span> è composta da <span class="math-tag">\( n\)</span> colonne linearmente indipendenti: ciò implica che siano una base <span class="math-tag">\( B_{C}\)</span> per <span class="math-tag">\( \mathbb{K}^{n}\)</span>.             <br ></br>             Consideriamo ora che <span class="math-tag">\( A\)</span> può essere considerata come la matrice cambiamento di base <span class="math-tag">\( M_{B_{C}, \varepsilon}(\text{id}_{\mathbb{R}})\)</span> e dato che tale matrice è associata all'applicazione lineare identità che è biettiva, possiamo dire che <span class="math-tag">\( A\)</span> ammette inversa.         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="definition environment" id="def5-14" ><span class="definition-header environment-title">Definizione 5.14 - Cambio delle basi legate alla matrice associata ad un'applicazione lineare</span>     Considerando la matrice <span class="math-tag">\( A = M_{B_{V}, B_{W}}(f)\)</span> associata all'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>, allora considerando la seguente composizione di applicazioni lineari     <span class="math-tag">\[         \begin{array}{ccccccc}             V &               \xrightarrow[]{\text{id}_{V}} &              V &             \xrightarrow[]{\ \ f \ \ } &             W &              \xrightarrow[]{\text{id}_{W}} &              W         \end{array}     \]</span>     per poter cambiare le basi associate alla matrice associata ad una funzione, ovvero per passare dalla matrice <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span> alla matrice <span class="math-tag">\( M_{B_{V}', B_{W}'}(f)\)</span> si ha che     <span class="math-tag">\[         M_{B_{V}', B_{W}'}(f) =          M_{B_{W}, B_{W}'}(\text{id}_{W})         \odot          M_{B_{V}, B_{W}}(f)         \odot         M_{B_{V}', B_{V}}(\text{id}_{V})     \]</span>     ovvero si cambia prima la base associata a <span class="math-tag">\( V\)</span>, si "applica" la funzione e infine si cambia la base associata a <span class="math-tag">\( W\)</span>. </div></div><div class="subsection part" id="subsec5-3" ><span class="subsection-header part-title">5.3 - Matrici simili</span><div class="definition environment" id="def5-15" ><span class="definition-header environment-title">Definizione 5.15 - Matrici simili</span>     Due matrici <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span> si dicono simili (<span class="math-tag">\( \sim\)</span>) se esiste una matrice <span class="math-tag">\( E \in M_{n \times n}(\mathbb{K})\)</span> invertibile tale che      <span class="math-tag">\[         B = E^{-1} \odot A \odot E     \]</span>     La similarità è una relazione d'equivalenza, ovvero vale     <ul ><li >la riflessività;         </li><li >la simmetria;         </li><li >la transitività.     </li></ul><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Fun fact</span>         Due matrici sono simili se rappresentano lo stesso endomorfismo rispetto a basi diverse.     </div></div><div class="demonstration environment" id="dem5-19" ><span class="demonstration-header environment-title">Dimostrazione 5.19 - Riflessività di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Ogni matrice è simile a se stessa, ovvero         <span class="math-tag">\[             A \sim A         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare  tale proprietà verifichiamo la definizione di similarità, ovvero         <span class="math-tag">\begin{aligned}             & A = E^{-1} \odot A \odot E             & \iff          \end{aligned}</span>         Consideriamo infatti che esiste la matrice <span class="math-tag">\( I\)</span> che è invertibile e verifichiamo che         <span class="math-tag">\begin{aligned}             & A = I^{-1} \odot A \odot I         \end{aligned}</span>         che dimostra la proprietà.     </div></div></div><div class="demonstration environment" id="dem5-20" ><span class="demonstration-header environment-title">Dimostrazione 5.20 - Simmetria di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando le matrici <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span>, se <span class="math-tag">\( A\)</span> è simile a <span class="math-tag">\( B\)</span>, allora <span class="math-tag">\( B\)</span> è simile ad <span class="math-tag">\( A\)</span>, ovvero         <span class="math-tag">\[             A \sim B             \qquad             \implies             \qquad             B \sim A         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">        Per dimostrare tale proprietà è necessario provare l'esistenza di una matrice <span class="math-tag">\( F \in M_{n \times n}(\mathbb{K})\)</span> invertibile tale che        <span class="math-tag">\[            A = F^{-1} \odot B \odot F        \]</span>        Per farlo consideriamo la definizione di similarità, ovvero        <span class="math-tag">\begin{aligned}            & B = E^{-1} \odot A \odot E            & \iff        \end{aligned}</span>        e moltiplichiamo a sinistra "da entrambe le parti" per la matrice <span class="math-tag">\( E\)</span> e moltiplichiamo a destra per la matrice <span class="math-tag">\( E^{-1}\)</span>, ovvero        <span class="math-tag">\begin{aligned}            & E \odot B \odot E^{-1} = E \odot (E^{-1} \odot A \odot E) \odot E^{-1}            & \iff        \end{aligned}</span>        Ora, per l'associatività del prodotto tra matrici        <span class="math-tag">\begin{aligned}            & E \odot B \odot E^{-1} = (E \odot E^{-1}) \odot A \odot (E \odot E^{-1})            & \iff         \end{aligned}</span>         ed eseguendo i calcoli nelle parentesi         <span class="math-tag">\begin{aligned}             & E \odot B \odot E^{-1} = I \odot A \odot I             & \iff \\             & A = E \odot B \odot E^{-1}             &        \end{aligned}</span>        A questo punto è possibile porre <span class="math-tag">\( F = E^{-1}\)</span> e scrivendo le matrici come l'inversa dell'inversa, si ha che        <span class="math-tag">\begin{aligned}            & A = (E^{-1})^{-1} \odot B \odot ((E^{-1})^{-1})^{-1}            & \iff \\            & A = F^{-1} \odot B \odot F            &        \end{aligned}</span>        si dimostra la proprietà.     </div></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Inversa della matrice che verifica <span class="math-tag">\( A \sim B\)</span> come matrice che verifica <span class="math-tag">\( B \sim A\)</span></span>         Da tale dimostrazione si può dedurre che la matrice <span class="math-tag">\( F\)</span> che verifica la similarità tra <span class="math-tag">\( B \sim A\)</span> non è altro che la matrice <span class="math-tag">\( E^{-1}\)</span> che verifica la similarità <span class="math-tag">\( A \sim B\)</span>.     </div></div><div class="demonstration environment" id="dem5-21" ><span class="demonstration-header environment-title">Dimostrazione 5.21 - Transitività di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando le matrici <span class="math-tag">\( A, B, C \in M_{n \times n}(\mathbb{K})\)</span>, nel caso <span class="math-tag">\( A\)</span> sia simile a <span class="math-tag">\( B\)</span> e <span class="math-tag">\( B\)</span> sia simile a <span class="math-tag">\( C\)</span>, si ha che <span class="math-tag">\( A\)</span> è simile a <span class="math-tag">\( C\)</span><span class="math-tag">\[             A \sim B             \qquad             \text{e}              \qquad              B \sim C             \qquad             A \sim C         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando le definizioni di similarità e la simmetria tra matrici simili è possibile scrivere le matrici <span class="math-tag">\( A, B, C\)</span> come         <span class="math-tag">\[             \begin{array}{ccc}                 A \sim B & \implies & A = E^{-1} \odot B \odot E \\                 B \sim A & \implies & B = E \odot A \odot E^{-1} \\                 B \sim C & \implies & B = F^{-1} \odot C \odot F \\                 C \sim B & \implies & C = F \odot B \odot F^{-1}             \end{array}         \]</span>         Per dimostrare tale proprietà è necessario dimostrare l'esistenza di una matrice <span class="math-tag">\( H\)</span> tale che         <span class="math-tag">\begin{aligned}             A = H^{-1} \odot C \odot H             \qquad             \text{oppure}             \qquad             C = H \odot A \odot H^{-1}         \end{aligned}</span>         Considerando quindi la definizione         <span class="math-tag">\begin{aligned}             & B = E \odot A \odot E^{-1}             & \iff         \end{aligned}</span>         è possibile sostituire <span class="math-tag">\( B\)</span> con <span class="math-tag">\( (F^{-1} \odot C \odot F)\)</span><span class="math-tag">\begin{aligned}             & F^{-1} \odot C \odot F = E \odot A \odot E^{-1}             & \iff          \end{aligned}</span>         e moltiplicare per <span class="math-tag">\( F\)</span> a sinistra da "entrambe le parti"         <span class="math-tag">\begin{aligned}             & F \odot F^{-1} \odot C \odot F = F \odot E \odot A \odot E^{-1}             & \iff          \end{aligned}</span>         e poi moltiplicare per <span class="math-tag">\( F^{-1}\)</span> a destra da "entrambe le parti"         <span class="math-tag">\begin{aligned}             & F \odot F^{-1} \odot C \odot F \odot F^{-1} = F \odot E \odot A \odot E^{-1} \odot F^{-1}             & \iff          \end{aligned}</span>         &Egrave; quindi possibile per la proprietà associativa         <span class="math-tag">\begin{aligned}             & (F \odot F^{-1}) \odot C \odot (F \odot F^{-1}) = F \odot E \odot A \odot E^{-1} \odot F^{-1}             & \iff          \end{aligned}</span>         risolvere le moltiplicazioni fra matrici inverse e ottenere         <span class="math-tag">\begin{aligned}             & I \odot C \odot I = F \odot E \odot A \odot E^{-1} \odot F^{-1}             & \iff          \end{aligned}</span>         &Egrave; poi possibile riscrivere la moltiplicazione <span class="math-tag">\( (E^{-1} \odot F^{-1})\)</span> come <span class="math-tag">\( (F \odot E)^{-1}\)</span> ed ottenere         <span class="math-tag">\begin{aligned}             & C = (F \odot E) \odot A \odot (F \odot E)^{-1}             & \iff          \end{aligned}</span>         e riscrivendo <span class="math-tag">\( (F \odot E)\)</span> come <span class="math-tag">\( H\)</span> si ottiene         <span class="math-tag">\begin{aligned}             & C = H \odot A \odot (H)^{-1}             &         \end{aligned}</span>         che dimostra la proprietà.     </div></div></div><div class="demonstration environment" id="dem5-22" ><span class="demonstration-header environment-title">Dimostrazione 5.22 - Similarità tra matrici legate allo stesso endomorfismo rispetto a basi diverse</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f: V \to V\)</span> e le matrici ad essa associate <span class="math-tag">\( M_{B_{V}, B_{V}}(f)\)</span> e <span class="math-tag">\( M_{B_{V}', B_{V}'}(f)\)</span> (ovvero rispetto a basi diverse), si ha che         <span class="math-tag">\[             M_{B_{V}, B_{V}}(f) \sim M_{B_{V}', B_{V}'}(f)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò è necessario provare l'esistenza di una matrice <span class="math-tag">\( E\)</span> tale che         <span class="math-tag">\[             M_{B_{V}', B_{V}'}(f) = E^{-1} \odot M_{B_{V}, B_{V}}(f) \odot E           \]</span>         Riscrivendo il procedimento per il "cambio delle basi legate alla matrice associata ad un'applicazione lineare" si ha che         <span class="math-tag">\begin{aligned}             & M_{B_{V}', B_{V}'}(f) =              M_{B_{V}, B_{V}'}(\text{id}_{V})             \odot              M_{B_{V}, B_{V}}(f)             \odot             M_{B_{V}', B_{V}}(\text{id}_{V})             & \iff         \end{aligned}</span>         e dato che <span class="math-tag">\( M_{B_{V}, B_{V}'}(\text{id}_{W})\)</span> può essere scritta anche come <span class="math-tag">\( (M_{B_{V}', B_{V}}(\text{id}_{V}))^{-1}\)</span>, si ha che         <span class="math-tag">\begin{aligned}             & M_{B_{V}', B_{V}'}(f) =              (M_{B_{V}', B_{V}}(\text{id}_{V}))^{-1}             \odot              M_{B_{V}, B_{V}}(f)             \odot             M_{B_{V}', B_{V}}(\text{id}_{V})             &         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-23" ><span class="demonstration-header environment-title">Dimostrazione 5.23 - Traccia di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La traccia di due matrici <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span> simili è la stessa, ovvero         <span class="math-tag">\[             A \sim B \qquad \implies \qquad tr(A) = tr(B)          \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando la definizione di similarità, si può scrivere          <span class="math-tag">\begin{aligned}             & tr(B) = tr(E^{-1} \odot A \odot E)             & \iff         \end{aligned}</span>         e, considerando "l'invarianza della traccia rispetto alla permutazione ciclica del prodotto tra matrici" si ha che         <span class="math-tag">\begin{aligned}             & tr(E^{-1} \odot A \odot E) = tr(A \odot E \odot E^{-1})             & \iff         \end{aligned}</span>         e grazie alla proprietà associativa         <span class="math-tag">\begin{aligned}             & tr(E^{-1} \odot A \odot E) = tr(A \odot (E \odot E^{-1}))             & \iff         \end{aligned}</span>         e sostituendo <span class="math-tag">\( E \odot E^{-1}\)</span> con <span class="math-tag">\( I\)</span> e <span class="math-tag">\( E^{-1} \odot A \odot E\)</span> con <span class="math-tag">\( B\)</span>, si ottiene         <span class="math-tag">\begin{aligned}             & tr(B) = tr(A)             &         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-24" ><span class="demonstration-header environment-title">Dimostrazione 5.24 - Rango di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il rango di due matrici <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span> simili è lo stesso, ovvero         <span class="math-tag">\[             A \sim B \qquad \implies \qquad r(A) = r(B)          \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando la matrice <span class="math-tag">\( A\)</span> come la matrice associata ad un endomorfismo <span class="math-tag">\( f: V \to V\)</span>, sappiamo che il rango di <span class="math-tag">\( A\)</span> è la dimensione dell'immagine di <span class="math-tag">\( f\)</span>. Dato che due matrici simili rappresentano lo stesso endomorfismo <span class="math-tag">\( f\)</span>, si ha che anche il rango di <span class="math-tag">\( B\)</span> è uguale alla dimension dell'immagine di <span class="math-tag">\( f\)</span>, ovvero         <span class="math-tag">\[             r(A) = dim(Im(f))             \quad             \text{e}             \quad             r(B) = dim(Im(f))             \qquad             \implies             \qquad             r(A) = r(B)         \]</span>         Si è quindi dimostrata la proprietà.     </div></div></div></div></div>
        </div>
    </div>
    <footer class="footer-container">
        <span>
            Created by 
            <typewriting-text class="credits-subtitle" still-time="1000" erasing-speed="150" >
                <word>lorenzoarlo</word>
                <word>Lorenzo Arlotti</word>
            </typewriting-text>
        </span>
    </footer>
</body>

</html>