<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="icon" href="../resources/logo.png" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
    <script defer id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script defer src="../scripts/TypewritingText.js"></script>
    <link rel="stylesheet" href="../styles/style.css" />
    <script src="../scripts/proof-event.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-78NHLXDQD8"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-78NHLXDQD8');
    </script>
    <title>8 - Spazi vettoriali euclidei</title>
</head>

<body>
    <header class="header-container">
        <div class="logo-wrapper">
        </div>
        <div class="header-title">
            <h1>Geometria e algebra</h1>
            <span>8 - Spazi vettoriali euclidei</span>
        </div>
        <div class="material-symbols-outlined header-title settings-button">
            <!-- settings -->
        </div>
    </header>
    <div class="main-container">
        <div class="content-container">
            <div class="section part" id="sec8" ><span class="section-header part-title">8 - Spazi vettoriali euclidei</span><div class="subsection part" id="subsec8-1" ><span class="subsection-header part-title">8.1 - Forme bilineari e prodotto scalare</span><div class="definition environment" id="def8-1" ><span class="definition-header environment-title">Definizione 8.1 - Forma bilineare reali</span>     Considerando uno spazio vettoriale <span class="math-tag">\( V\)</span> finitamento generato, una forma bilineare reale è una funzione     <span class="math-tag">\[         f: V \times V \to \mathbb{R}     \]</span>     che è lineare in ogni variabile (fissata l'altra). </div><div class="demonstration environment" id="dem8-1" ><span class="demonstration-header environment-title">Dimostrazione 8.1 - Rappresentazione di una forma bilineare</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Scegliendo una base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> per <span class="math-tag">\( V\)</span>, è possibile rappresentare ogni forma bilineare su <span class="math-tag">\( V\)</span> con una matrice <span class="math-tag">\( n \times n\)</span>.       </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando due vettori <span class="math-tag">\( u, w \in V\)</span> questi potranno essere scritti come combinazione lineare di <span class="math-tag">\( B_{V}\)</span>, ovvero         <span class="math-tag">\begin{aligned}             & u = \sum_{i = 1}^{n} a_{i} \cdot v_{i}             & w = \sum_{j = 1}^{n} b_{j} \cdot v_{j}         \end{aligned}</span>         &Egrave; quindi possibile scrivere l'immagine di due vettori come         <span class="math-tag">\begin{aligned}             f(u, w) = f             \left(             \sum_{i = 1}^{n}a_{i} \cdot v_{i}, \sum_{j = 1}^{n} b_{j} \cdot v_{j}             \right)         \end{aligned}</span>         e grazie alla linearità <span class="math-tag">\( f\)</span> (fissando prima <span class="math-tag">\( a_{i})\)</span> e poi <span class="math-tag">\( b_{j}\)</span> si può scrivere         <span class="math-tag">\begin{aligned}             f(u, w) =              \sum_{i = 1}^{n} a_{i}             \cdot              \sum_{j = 1}^{n} b_{j}             \cdot f(v_{i}, v_{j})         \end{aligned}</span>         che è esattamente il calcolo da fare per risolvere tale prodotto matriciale         <span class="math-tag">\[             \left(             \begin{array}{ccc}                 a_{1} & \ldots & a_{n}             \end{array}             \right)             \odot             \left(             \begin{array}{ccc}                 f(v_{1}, v_{1}) & \ldots & f(v_{1}, v_{n}) \\                 \vdots & \ddots & \vdots \\                  f(v_{n}, v_{1}) & \ldots & f(v_{n}, v_{n})             \end{array}             \right)             \odot             \left(             \begin{array}{c}                 b_{1} \\                 \vdots \\                 b_{n}             \end{array}             \right)         \]</span>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="definition environment" id="def8-2" ><span class="definition-header environment-title">Definizione 8.2 - Prodotto scalare</span>     Si definisce prodotto scalare su <span class="math-tag">\( V\)</span> una forma bilineare     <span class="math-tag">\[         f: V \times V \to \mathbb{R}     \]</span>     che è     <ul ><li >simmetrica, ovvero che         <span class="math-tag">\[             f(u, w) = f(w, u)         \]</span>         (che equivale a dire che la matrice associata <span class="math-tag">\( A\)</span> deve essere simmetrica);         </li><li >definita positiva, ovvero che il prodotto scalare dello stesso vettore (<span class="math-tag">\( v \in V\)</span>) è sempre maggiore o uguale a <span class="math-tag">\( 0\)</span> (<span class="math-tag">\( f(v, v) \geq 0\)</span>) e se <span class="math-tag">\( v\)</span> non è nullo esso è sempre strettamente positivo (<span class="math-tag">\( v \neq 0 \implies f(v, v) \gt  0\)</span>).     </li></ul><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Notazione</span>         Di seguito un qualsiasi prodotto scalare sarà rappresentato tramite il simbolo <span class="math-tag">\( \star\)</span>.     </div></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Relazione tra definizione del prodotto scalare standard in fisica e in geometria</span>     Tale definizione è differente da quella fornita dai corsi di Fisica (che tratta i vettori del piano). Infatti, dati due vettori <span class="math-tag">\( v, w \in \mathbb{R}^{2}\)</span> le cui coordinate sono     <span class="math-tag">\begin{aligned}         & v = (x_{v}, y_{v})         & w = (x_{w}, y_{w})     \end{aligned}</span>     si ha che il prodotto scalare tra vettori è definito come     <span class="math-tag">\begin{aligned}         & u \star w = ||v|| \cdot ||w|| \cdot \cos{(\theta)}         & \iff      \end{aligned}</span>     dove con <span class="math-tag">\( ||v||\)</span> e <span class="math-tag">\( ||w||\)</span> si indica il modulo di tali vettori e con <span class="math-tag">\( \theta\)</span> si indica la differenza tra l'angolo <span class="math-tag">\( \alpha\)</span> di <span class="math-tag">\( v\)</span> e l'angolo <span class="math-tag">\( \beta\)</span> di <span class="math-tag">\( w\)</span> (<span class="math-tag">\( \alpha - \beta\)</span>). Il modulo è anche esprimibile secondo il teorema di pitagora come      <span class="math-tag">\begin{aligned}         & ||v|| = \sqrt{(x_{v})^{2} + (y_{v})^{2}}         & ||w|| = \sqrt{(x_{w})^{2} + (y_{w})^{2}}     \end{aligned}</span>     mentre, per le formule di differenza del coseno     <span class="math-tag">\[         \cos{(\theta)} = \cos{(\alpha - \beta)} = \cos{\alpha} \cdot \cos{\beta} + \sin{\alpha} \cdot \sin{\beta}     \]</span>     Quindi tale formula si può esprimere come     <span class="math-tag">\begin{aligned}         & u \star w = \sqrt{(x_{v})^{2} + (y_{v})^{2}} \cdot \sqrt{(x_{w})^{2} + (y_{w})^{2}} \cdot (\cos{\alpha} \cdot \cos{\beta} + \sin{\alpha} \cdot \sin{\beta})         & \iff     \end{aligned}</span>     Ricordando che <span class="math-tag">\( \cos{\alpha}\)</span>, <span class="math-tag">\( \sin{\alpha}\)</span>, <span class="math-tag">\( \cos{\beta}\)</span>, <span class="math-tag">\( \sin{\beta}\)</span> sono esprimibili anche rispetto alle coordinate, si ha che     <span class="math-tag">\begin{aligned}         & \cos{\alpha} = \frac{x_{v}}{(\sqrt{(x_{v})^{2} + (y_{v})^{2}}}         & \sin{\alpha} = \frac{y_{v}}{\sqrt{(x_{v})^{2} + (y_{v})^{2}}}         \\         & \cos{\beta} = \frac{x_{w}}{\sqrt{(x_{w})^{2} + (y_{w})^{2}}}         & \sin{\beta} = \frac{y_{w}}{\sqrt{(x_{w})^{2} + (y_{w})^{2}}}     \end{aligned}</span>     e allora, sostituendo e semplificando, si ottiene che <span class="math-tag">\( u \star w\)</span> è uguale a                    <span class="math-tag">\begin{aligned}         u \star w = x_{v} \cdot x_{w} + y_{v} \cdot y_{w}     \end{aligned}</span>     Tale formula è la stessa ottenibile dalla formula di moltiplicazione tra matrici <span class="math-tag">\( 1 \times 2\)</span> e <span class="math-tag">\( 2 \times 1\)</span> (la matrice identica è la matrice associata al prodotto scalare standard)     <span class="math-tag">\[         \left(         \begin{array}{cc}             x_{v} & y_{v}         \end{array}         \right)         \odot         I         \odot         \begin{array}{c}             x_{w} \\             y_{w}         \end{array}     \]</span>     il cui risultato è una matrice <span class="math-tag">\( 1 \times 1\)</span> contenente <span class="math-tag">\( x_{v} \cdot x_{w} + y_{v} \cdot y_{w}\)</span>.     <br ></br>     Tale ragionamento è estendibile a vettori in un qualsiasi spazio vettoriale <span class="math-tag">\( \mathbb{R}^{n}\)</span>. </div><div class="myexample environment" id="example45" ><span class="myexample-header environment-title">Esempio 45 - Prodotto scalare standard</span>     La forma bilineare del prodotto standard     <span class="math-tag">\[         \left(         \begin{array}{ccc}             x_{1} & \ldots & x_{n}         \end{array}         \right)         \odot         I         \odot         \left(         \begin{array}{c}             x_{1} \\             \vdots \\             x_{n}         \end{array}         \right)     \]</span>     è:     <ul ><li >simmetrica, infatti la matrice identica è simmetrica e <span class="math-tag">\( f(v, w) = f(w, v)\)</span>;         </li><li >definita positiva, infatti, ricordando che <span class="math-tag">\( v\)</span> è combinazione lineare di <span class="math-tag">\( B_{V}\)</span>, ovvero         <span class="math-tag">\[             v = x_{1} \cdot v_{1} + \ \ldots \ + x_{n} \cdot v_{n}         \]</span>         si ha che          <span class="math-tag">\[             f(v, v) = (x_{1})^{2} \cdot (v_{1})^{2} + (x_{n})^{2} (v_{n})^{2}         \]</span>         per cui tutti i coefficienti <span class="math-tag">\( x_{i}\)</span> sono positivi (o nulli).     </li></ul><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Interpretazione geometrica</span>         A livello geometrico, il prodotto scalare standard si può interpretare come la lunghezza della proiezione del vettore sull'altro.     </div></div><div class="definition environment" id="def8-3" ><span class="definition-header environment-title">Definizione 8.3 - Forma bilineare definita positiva associata ad una  matrice con tutti autovalori positivi</span>     Dire che una forma bilineare <span class="math-tag">\( f\)</span> è definita positiva, equivale a dire che la matrice <span class="math-tag">\( A\)</span> associata ad un endomorfismo <span class="math-tag">\( f\)</span> ha autovalori tutti positivi.     <div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Trick per le matrici <span class="math-tag">\( 2 \times 2\)</span></span>         Per le matrici <span class="math-tag">\( 2 \times 2\)</span> simmetriche, dire che gli autovalori di <span class="math-tag">\( A\)</span> sono tutti positivi equivale a dire che sia il suo determinante sia la sua traccia sono positivi, ovvero         <span class="math-tag">\begin{aligned}             & det(A) \gt  0             & tr(A) \gt  0         \end{aligned}</span></div></div></div><div class="subsection part" id="subsec8-2" ><span class="subsection-header part-title">8.2 - Spazi vettoriali euclidei</span><div class="definition environment" id="def8-4" ><span class="definition-header environment-title">Definizione 8.4 - Spazio vettoriale euclideo</span>     Uno spazio vettoriale <span class="math-tag">\( (\mathbb{K}, V, +, \cdot)\)</span> si definisce euclideo se è dotato di un prodotto scalare.     <div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Esempio di spazio vettoriale euclideo</span>         Un esempio di spazi vettoriale euclideo è <span class="math-tag">\( \mathbb{R}^{n}\)</span> (<span class="math-tag">\( (\mathbb{R}, \mathbb{R}^{2}, +, \cdot)\)</span>) con un qualsiasi prodotto scalare valido.     </div></div><div class="definition environment" id="def8-5" ><span class="definition-header environment-title">Definizione 8.5 - Norma di un vettore</span>     Considerando uno spazio vettoriale euclideo, si definisce come norma di un vettore <span class="math-tag">\( v\)</span> appartenente allo spazio vettoriale, il prodotto scalare tra il vettore e se stesso, ovvero lo scalare     <span class="math-tag">\[         ||v|| = \sqrt{v \star v}     \]</span><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Versori</span>         I vettori di norma <span class="math-tag">\( 1\)</span> sono detti versori.     </div></div><div class="myexample environment" id="example46" ><span class="myexample-header environment-title">Esempio 46 - Esempio di calcolo della norma nel caso del prodotto scalare standard</span>     Considerando lo spazio vettoriale euclideo <span class="math-tag">\( (\mathbb{R}^{2}, \star)\)</span>, dove <span class="math-tag">\( \star\)</span> è il prodotto scalare standard, allora la norma di un vettore <span class="math-tag">\( v = (1, 1)\)</span> sarà     <span class="math-tag">\[         ||v|| = \sqrt{1^{2} + 1^{2}}  = \sqrt{2}      \]</span>     che è equivalente al calcolo del modulo di un vettore tramite il teorema di pitagora. </div><div class="myexample environment" id="example47" ><span class="myexample-header environment-title">Esempio 47 - Esempio del calcolo della norma nel caso di un prodotto scalare qualsiasi</span>     Considerando lo spazio vettoriale euclideo <span class="math-tag">\( (\mathbb{R}^{2}, \star)\)</span>, dove <span class="math-tag">\( \star\)</span> è il prodotto scalare associato alla seguente matrice     <span class="math-tag">\[         \left(         \begin{array}{cc}             2 & 1 \\             1 & 1         \end{array}         \right)     \]</span>     allora la norma di un vettore <span class="math-tag">\( v = (1, 1)\)</span> sarà     <span class="math-tag">\[         ||v|| =         \sqrt         {             \left(             \begin{array}{cc}                 1 & 1             \end{array}             \right)             \odot             \left(             \begin{array}{cc}                 2 & 1 \\                 1 & 1             \end{array}             \right)             \odot              \left(             \begin{array}{c}                 1 \\                 1             \end{array}              \right)         }         =         \sqrt{5}     \]</span></div><div class="definition environment" id="def8-6" ><span class="definition-header environment-title">Definizione 8.6 - Angolo tra due vettori</span>     In un spazio vettoriale euclideo, si definisce angolo <span class="math-tag">\( \widehat{vw}\)</span> tra due vettori <span class="math-tag">\( v, w\)</span> dello spazio vettoriale, l'arcocoseno del valore ottenuto dal prodotto scalare tra i due vettori diviso il prodotto delle norme, ovvero     <span class="math-tag">\[         \cos(\widehat{vw}) = \frac{v \star w}{||v|| \cdot ||w|| }     \]</span></div><div class="myexample environment" id="example48" ><span class="myexample-header environment-title">Esempio 48 - Esempio di calcolo dell'angolo tra due vettori nel caso del prodotto scalare standard</span>     Considerando lo spazio vettoriale euclideo <span class="math-tag">\( (\mathbb{R}^{2}, \star)\)</span>, dove <span class="math-tag">\( \star\)</span> è il prodotto scalare standard, allora l'angolo tra due vettori <span class="math-tag">\( v = (1, 0)\)</span> e <span class="math-tag">\( w = (1, 1)\)</span> sarà     <span class="math-tag">\[         \cos(\widehat{vw}) = \frac{(1, 0) \star (1, 1)}{||(1, 0)|| \cdot ||(1, 1)||} = \frac{2}{2\sqrt{2}} = \frac{\sqrt{2}}{2}     \]</span>     ovvero l'angolo <span class="math-tag">\( \frac{\pi}{4}\)</span>. </div><div class="definition environment" id="def8-7" ><span class="definition-header environment-title">Definizione 8.7 - Norma di un vettore sempre maggiore o uguale a <span class="math-tag">\( 0\)</span></span>     La norma di un vettore è sempre maggiore o uguale a <span class="math-tag">\( 0\)</span>, proprio per la proprietà del prodotto scalare di essere definito positivo, ovvero     <span class="math-tag">\[         ||v|| \geq 0     \]</span></div><div class="demonstration environment" id="dem8-2" ><span class="demonstration-header environment-title">Dimostrazione 8.2 - Norma del multiplo di un vettore</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La norma del multiplo di un vettore <span class="math-tag">\( v\)</span> (<span class="math-tag">\( \alpha \cdot v\)</span>), è il valore assoluto di <span class="math-tag">\( \alpha\)</span> moltiplicata per la norma di <span class="math-tag">\( v\)</span>, ovvero         <span class="math-tag">\[             ||\alpha \cdot v|| = \left| \alpha \right| \cdot ||v||         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, consideriamo la definizione di norma, ovvero         <span class="math-tag">\begin{aligned}             & ||\alpha \cdot v|| = \sqrt{(\alpha \cdot v) \star (\alpha \cdot v)}             & \iff         \end{aligned}</span>         e per la bilinearità del prodotto scalare si può scrivere         <span class="math-tag">\begin{aligned}             & \sqrt{\alpha^{2} \cdot (v \star v)}             & \iff \\             & \left| \alpha \right| \cdot \sqrt{v \star v}              & \iff \\             & \left| \alpha \right| \cdot ||v||             &         \end{aligned}</span>         Che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem8-3" ><span class="demonstration-header environment-title">Dimostrazione 8.3 - Quadrato della norma della somma di due vettori</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due vettori <span class="math-tag">\( v, w\)</span>, il quadrato della norma del vettore somma <span class="math-tag">\( v + w\)</span> è          <span class="math-tag">\[             ||v + w||^{2} = ||v||^{2} + 2 \cdot (v \star w) + ||w||^{2}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando il quadrato della norma del vettore somma <span class="math-tag">\( v + w\)</span>, si ha che         <span class="math-tag">\begin{aligned}             & ||v + w||^{2} = (v + w) \star (v + w)             & \iff         \end{aligned}</span>         e per linearità si ha che         <span class="math-tag">\begin{aligned}             & (v \star v) + (v \star w) + (w \star v) + (w \star w)             & \iff \\             & ||v||^{2} + 2 \cdot (v \star w) + ||w||^{2}             &         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem8-4" ><span class="demonstration-header environment-title">Dimostrazione 8.4 - Disuguaglianza di Cauchy-Schwarz</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il valore assoluto del prodotto scalare di due vettori <span class="math-tag">\( v, w\)</span> è minore o uguale al prodotto tra le due norme, ovvero         <span class="math-tag">\[             \left| v \star w \right| \leq ||v|| \cdot ||w||         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale disuguaglianza, consideriamo la norma elevata al quadrato del vettore somma <span class="math-tag">\( v + tw\)</span> (dove <span class="math-tag">\( t\)</span> è una qualsiasi variabile reale) che per definizione è         <span class="math-tag">\begin{aligned}             & ||v + tw||^{2} = (v + tw) \star (v + tw)             & \iff         \end{aligned}</span>         Per la bilinearità si può scrivere         <span class="math-tag">\begin{aligned}             & v \star v + t \cdot (v \star w) + t \cdot (v \star w) + t^{2} \cdot (w \star w)             & \iff \\             & t^{2} \cdot (w \star w) + 2t \cdot (v \star w) + (v \star v)             & \iff \\             & t^{2} \cdot ||w||^{2} + 2t \cdot (v \star w) + ||v||^{2}             & \iff         \end{aligned}</span>         che possiamo vedere come un polinomio di secondo grado nella variabile <span class="math-tag">\( t\)</span>. Dato che siamo partiti dalla norma <span class="math-tag">\( ||v + tw||^{2}\)</span>, un valore sempre positivo o nullo         <span class="math-tag">\[             t^{2} \cdot ||w||^{2} + 2t \cdot (v \star w) + ||v||^{2} \geq 0         \]</span>         possiamo dire che il discriminante di tale polinomio è minore o uguale a <span class="math-tag">\( 0\)</span> (infatti si avrebbe una parabola che può essere solo tangente all'asse delle ascisse, ma mai secante dato che avrebbe una zona in cui è minore di <span class="math-tag">\( 0\)</span>), ovvero         <span class="math-tag">\begin{aligned}             & (2 \cdot (v \star w))^{2} - 4 \cdot ||w||^{2} \cdot ||v||^{2}) \leq 0             & \iff         \end{aligned}</span>         e risolvendo la disequazione         <span class="math-tag">\begin{aligned}             & 4 \cdot (v \star w)^{2} \leq 4 \cdot ||w||^{2} \cdot ||v||^{2}             & \iff \\         \end{aligned}</span>         e eliminando l'esponente con la radice da entrambe le parti         <span class="math-tag">\begin{aligned}             & \left| v \star w \right| \leq ||v|| \cdot ||w||              &         \end{aligned}</span>         si dimostra la proposizione.     </div></div></div><div class="definition environment" id="def8-8" ><span class="definition-header environment-title">Definizione 8.8 - Vettori ortogonali</span>     Due vettori si definiscono ortogonali tra loro se e solo se il loro prodotto scalare è nullo, ovvero     <span class="math-tag">\[         v \star w = 0         \qquad \iff \qquad         \text{$v$ e $w$ sono ortogonali}     \]</span></div><div class="definition environment" id="def8-9" ><span class="definition-header environment-title">Definizione 8.9 - Base ortogonale</span>     Una base (in uno spazio vettoriale euclideo) si definisce ortogonale se i suoi vettori sono a due a due ortgonali tra loro.     <div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Esempio di base ortogonale</span>         Un esempio di base ortogonale per <span class="math-tag">\( \mathbb{R}^{2}\)</span> considerando il prodotto scalare standard, è formata da un vettore ed uno dei due vettori perpendicolari ad esso.     </div></div><div class="definition environment" id="def8-10" ><span class="definition-header environment-title">Definizione 8.10 - Base ortonormale</span>     Considerando una base ortogonale, se i suoi vettori sono tutti di norma <span class="math-tag">\( 1\)</span>, essa si definisce una base ortonormale.     <div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Esempio di base ortonormale</span>         Un esempio di base ortonormale per <span class="math-tag">\( \mathbb{R}^{n}\)</span> considerando il prodotto scalare standard, è la matrice identica.     </div></div><div class="demonstration environment" id="dem8-5" ><span class="demonstration-header environment-title">Dimostrazione 8.5 - Coordinate di un vettore rispetto ad una base ortonormale</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando <span class="math-tag">\( B = (v_{1}, \ \ldots \ , v_{n})\)</span> una base ortonormale, per ottenere la <span class="math-tag">\( i\)</span>-esima coordinata di un qualsiasi vettore <span class="math-tag">\( v\)</span> rispetto a tale base         <span class="math-tag">\[             v = x_{1} \cdot v_{1} + \ \ldots \ + x_{i} \cdot v_{i}  + \ \ldots \ + x_{n} \cdot v_{n}         \]</span>         è sufficiente fare il prodotto scalare tra <span class="math-tag">\( v\)</span> e l'<span class="math-tag">\( i\)</span>-esimo vettore della base, ovvero         <span class="math-tag">\[             x_{i} = v \star v_{i}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò, consideriamo la combinazione lineare del vettore <span class="math-tag">\( v\)</span> rispetto alla base <span class="math-tag">\( B\)</span><span class="math-tag">\[             v = x_{1} \cdot v_{1} + \ \ldots \ + x_{i} \cdot v_{i}  + \ \ldots \ + x_{n} \cdot v_{n}         \]</span>         Effettuando il prodotto scalare tra un qualsiasi vettore e l'<span class="math-tag">\( i\)</span>-esimo vettore della base <span class="math-tag">\( B\)</span>, si avrebbe che         <span class="math-tag">\begin{aligned}             & v \star v_{i} = (x_{1} \cdot v_{1} + \ \ldots \ + x_{i} \cdot v_{i}  + \ \ldots \ + x_{n} \cdot v_{n}) \star v_{i}             & \iff         \end{aligned}</span>         che per la linearità del prodotto scalare è         <span class="math-tag">\begin{aligned}             &= x_{1} \cdot v_{1} \star v_{i} + \ \ldots \ + x_{i} \cdot v_{i} \star v_{i} + \ \ldots \ + x_{n} \cdot v_{n} \star v_{i}             &          \end{aligned}</span>         Scrivendo in questo modo, è evidente che tutti i prodotto scalari tra vettori si annullano (in quanto sono vettori ortogonali tra loro) ad eccezione del vettore <span class="math-tag">\( v_{i}\)</span>, il cui prodotto scalare è per definizione la sua norma (ovvero <span class="math-tag">\( 1\)</span>).         <span class="math-tag">\begin{aligned}             & x_{i} \cdot v_{i} \star v_{i}             &          \end{aligned}</span>         Ottenendo quindi la <span class="math-tag">\( i\)</span>-esima coordinata.     </div></div></div><div class="myexample environment" id="example49" ><span class="myexample-header environment-title">Esempio 49 - Calcolo delle coordinate di un vettore rispetto ad una base ortonormale</span>     Considerando lo spazio vettoriale euclideo <span class="math-tag">\( \mathbb{R}^{2}\)</span> associato al prodotto scalare standard, consideriamo la base ortonormale <span class="math-tag">\( B = ((\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}), (\frac{\sqrt{2}}{2}, -\frac{\sqrt{2}}{2}))\)</span>: si vogliono calcolare le coordinate del vettore <span class="math-tag">\( (3, 7)\)</span> rispetto a <span class="math-tag">\( B\)</span>.      <br ></br>     Per ottenere la soluzione se non avessimo una base ortonormale, si dovrebbe risolvere il seguente sistema lineare     <span class="math-tag">\[         \left\{          \begin{array}{ccccc}             \frac{\sqrt{2}}{2} x_{1} & + & \frac{\sqrt{2}}{2} x_{2} & = & 3  \\             \frac{\sqrt{2}}{2} x_{1} & - & \frac{\sqrt{2}}{2} x_{2} & = & 7  \\         \end{array}         \right.     \]</span>     mentre, grazie al teorema, è possibile calcolare la coordinata come il risultato del prodotto scalare tra i vettori, ovvero     <span class="math-tag">\begin{aligned}         & x_{1} = (3, 7) \star (\frac{\sqrt{2}}{2}) = 5 \cdot \sqrt{2} \\         & x_{2} = (3, 7) \star (-\frac{\sqrt{2}}{2}) = -2 \cdot \sqrt{2}     \end{aligned}</span>     che sono esattamente i valori che si otterrebbero dalla risoluzione del sistema </div><div class="demonstration environment" id="dem8-6" ><span class="demonstration-header environment-title">Dimostrazione 8.6 - Metodo di ortonormalizzazione di Gram-Schmidt</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         &Egrave; sempre possibile rendere una qualsiasi base <span class="math-tag">\( B = (v_{1}, \ \ldots \ , v_{n})\)</span> ortonormale (ottenendo così la base <span class="math-tag">\( B' = (w_{1}, \ \ldots \ , w_{n})\)</span>).          <br ></br>         Per farlo è necessario considerare il primo vettore <span class="math-tag">\( v_{1}\)</span> e renderlo di norma <span class="math-tag">\( 1\)</span>, ovvero dividerlo per la sua norma         <span class="math-tag">\[             w_{1} = \frac{v_{1}}{||v_{1}||}         \]</span>         A questo punto è necessario rendere i vettori della base ortogonali a <span class="math-tag">\( w_{1}\)</span> e per farlo sono necessari due passaggi: il primo è ottenere il vettore <span class="math-tag">\( \widehat{w_{k}}\)</span>, che è il vettore ortogonale e poi renderlo di norma <span class="math-tag">\( 1\)</span>. Quindi si ha che         <span class="math-tag">\[             \left\{             \begin{array}{lcl}                 \widehat{w_{k}} & = & v_{k} - (v_{k} \star w_{k - 1}) w_{k - 1} - \ \ldots \ - (v_{k} \star w_{1}) w_{1}                  \\                 w_{k} & = & \frac{\widehat{w_{k}}}{||\widehat{w_{k}}||}             \end{array}             \right.         \]</span>         Ciò significa che per ottenere il vettore <span class="math-tag">\( w_{k}\)</span> è necessario calcolare tutti i vettori <span class="math-tag">\( w_{k - 1}, \ \ldots \ , w_{1}\)</span> precedenti.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         &Egrave; quindi necessario dimostrare che i vettori <span class="math-tag">\( w_{1}, \ \ldots \ , w_{k}\)</span> sono ortogonali a due a due tra loro (<span class="math-tag">\( w_{1} \perp w_{2}\)</span>, <span class="math-tag">\( w_{1} \perp w_{k}\)</span>), ovvero che il prodotto scalare tra loro è nullo.         <br ></br>         Per il metodo utilizzato, dire che <span class="math-tag">\( w_{1} \perp w_{2}\)</span> è equivalente a dire che <span class="math-tag">\( w_{1} \perp \widehat{w_{2}}\)</span>, ovvero         <span class="math-tag">\begin{aligned}             & w_{1} \perp w_{2}             & \iff \\             & w_{1} \perp \widehat{w_{2}}              & \iff \\             & w_{1} \perp (v_{2} - (v_{2} \star w_{1}) \cdot w_{1})             & \iff \\             & w_{1} \star (v_{2} - (v_{2} \star w_{1}) \cdot w_{1}) = 0             & \iff         \end{aligned}</span>         che per la linearità del prodotto scalare è uguale a         <span class="math-tag">\begin{aligned}             & (w_{1} \star v_{2}) - (v_{2} \star w_{1}) \cdot (w_{1} \star w_{1}) = 0             & \iff         \end{aligned}</span>         Ora, noi sappiamo che <span class="math-tag">\( (w_{1} \star w_{1})\)</span> è la norma di <span class="math-tag">\( w_{1}\)</span>, il cui valore è proprio <span class="math-tag">\( 1\)</span>: ciò significa che nella moltiplicazione si semplifica e rimane la seguente differenza          <span class="math-tag">\begin{aligned}             & (w_{1} \star v_{2}) - (v_{2} \star w_{1}) = 0             & \iff         \end{aligned}</span>         che è proprio <span class="math-tag">\( 0\)</span> in quanto il prodotto scalare è simmetrico.         <span class="math-tag">\begin{aligned}             & 0 = 0             &         \end{aligned}</span>         Tale dimostrazione si può ripetere per tutti gli altri vettori, dimostrando così la proposizione.     </div></div></div><div class="myexample environment" id="example50" ><span class="myexample-header environment-title">Esempio 50 - Ortonormalizzazione di una base con Gram-Schmidt</span>     Si vuole trovare una base ortonormale per il sottospazio vettoriale di <span class="math-tag">\( \mathbb{R}^{3}\)</span> di equazione <span class="math-tag">\( x + y + z = 0\)</span>.     <br ></br>     Innanzitutto troviamo una base, per farlo è sufficiente ottenere la soluzione parametrica dal sistema di sola equazione <span class="math-tag">\( x + y + z = 0\)</span>, ovvero     <span class="math-tag">\begin{aligned}         \left\{          \begin{array}{ccccccc}             x & + & y & + & z & = & 0         \end{array}         \right.         \qquad         \implies          \qquad         \left\{         \begin{array}{ccccc}             x & = & s \\             y & = & t \\             z & = & -s & - & t         \end{array}         \right.     \end{aligned}</span>     che implica quindi che la soluzione sia      <span class="math-tag">\[         \left(         \begin{array}{c}             x \\ y \\ z         \end{array}         \right)         =         s          \cdot         \left(         \begin{array}{c}             1 \\ 0 \\ -1         \end{array}         \right)         +         t         \cdot         \left(         \begin{array}{c}             0 \\ 1 \\ -1         \end{array}         \right)     \]</span>     La base per tale sottospazio è quindi <span class="math-tag">\( ((1, 0, -1), (0, 1, -1))\)</span>.     <br ></br>     Per ottenere una base ortonormale applichiamo Gram-Schmidt. Si ha quindi che la base è composta da due vettori:     <span class="math-tag">\begin{aligned}         & v_{1} = (1, 0, -1)         & v_{2} = (0, 1, -1)     \end{aligned}</span>     Iniziamo normalizzando <span class="math-tag">\( v_{1}\)</span>, ovvero     <span class="math-tag">\[         w_{1} = \frac{v_{1}}{||v_{1}||} = \frac{(1, 0, -1)}{\sqrt{2}} = (\frac{1}{\sqrt{2}}, 0, -\frac{1}{\sqrt{2}})      \]</span>     Ora per ottenere <span class="math-tag">\( \widehat{w_{2}}\)</span>, si deve calcolare     <span class="math-tag">\[         \begin{array}{lcl}         \widehat{w_{2}} & = & v_{2} - (v_{2} \star w_{1}) \cdot w_{1} \\         & = & (0, 1, -1) - ((0, 1, -1) \star (\frac{1}{\sqrt{2}}, 0, -\frac{1}{\sqrt{2}})) \cdot (\frac{1}{\sqrt{2}}, 0, -\frac{1}{\sqrt{2}}) \\         & = & (0, 1, -1) - (\frac{1}{\sqrt{2}}) \cdot (\frac{1}{\sqrt{2}}, 0, -\frac{1}{\sqrt{2}}) \\         & = & (0, 1, -1) - (\frac{1}{2}, 0, -\frac{1}{2}) \\         & = & (-\frac{1}{2}, 1, -\frac{1}{2})          \end{array}     \]</span>     &Egrave; semplice ora ottenere <span class="math-tag">\( w_{2}\)</span>, ovvero     <span class="math-tag">\[         \begin{array}{lcl}             w_{2} & = & \frac{\widehat{w_{2}}}{||\widehat{w_{2}}||} \\             & = & \frac{(-\frac{1}{2}, 1, -\frac{1}{2})}{\sqrt{\frac{3}{2}}} \\             & = & \sqrt{\frac{2}{3}} \cdot (-\frac{1}{2}, 1, -\frac{1}{2})          \end{array}     \]</span>     Si ha quindi la nuova base ortonormale     <span class="math-tag">\[         B = \left((\frac{1}{\sqrt{2}}, 0, -\frac{1}{\sqrt{2}}), \sqrt{\frac{2}{3}} \cdot (-\frac{1}{2}, 1, -\frac{1}{2})\right)     \]</span></div><div class="demonstration environment" id="dem8-7" ><span class="demonstration-header environment-title">Dimostrazione 8.7 - Esistenza di una base ortogonale (e ortonormale) per ogni spazio vettoriale euclideo</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Ogni spazio vettoriale euclideo ammette almeno una base ortogonale (e ortonormale).     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando che ogni spazio vettoriale ha diritto ad avere una base e che si può applicare il procedimento di ortonormalizzazione di Gram-Schmidth, si è dimostrato il teorema.     </div></div></div><div class="demonstration environment" id="dem8-8" ><span class="demonstration-header environment-title">Dimostrazione 8.8 - Lineare indipendenza tra vettori ortogonali</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando <span class="math-tag">\( n\)</span> vettori <span class="math-tag">\( v_{1}, \ \ldots \ , v_{n}\)</span> a due a due ortogonali non nulli, essi sono linearmente indipendenti.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostare ciò è necessario dimostrare che l'unica combinazione lineare del vettore nullo utilizzando i vettori <span class="math-tag">\( v_{1}, \ \ldots \ , v_{n}\)</span> sia quella con solo coordinate nulle, ovvero         <span class="math-tag">\begin{aligned}             & x_{1} \cdot v_{1} + \ \ldots \ + x_{n} v_{n} = 0             & \iff          \end{aligned}</span>         Facendo il prodotto scalare tra questi vettori e il vettore <span class="math-tag">\( v_{1}\)</span> si ottiene         <span class="math-tag">\begin{aligned}             & (x_{1} \cdot v_{1} + \ \ldots \ + x_{n} v_{n}) \star v_{1}  = 0             & \iff          \end{aligned}</span>         e per la linearità del prodotto scalare si ottiene         <span class="math-tag">\begin{aligned}             & x_{1} \cdot (v_{1} \star v_{1}) + \ \ldots \ + x_{n} (v_{n} \star v_{1}) = 0             & \iff          \end{aligned}</span>         e, dato che il prodotto scalare tra vettori orgonali è <span class="math-tag">\( 0\)</span>, si ha che          <span class="math-tag">\begin{aligned}             & x_{1} \cdot (v_{1} \star v_{1}) = 0             & \iff         \end{aligned}</span>         e dato che <span class="math-tag">\( v_{1} \star v_{1}\)</span> non può essere nullo per ipotesi, allora abbiamo obbligatoriamente che <span class="math-tag">\( x_{1}\)</span> lo deve essere.         <br ></br>         Ripetendo tale procedimento per ogni vettore, si ha che tutte le coordinate <span class="math-tag">\( x_{1}, \ \ldots \ , x_{n}\)</span> sono nulle, dimostrando quindi la lineare indipendenza.     </div></div></div><div class="demonstration environment" id="dem8-9" ><span class="demonstration-header environment-title">Dimostrazione 8.9 - Teorema di completamento a una base ortogonale (o ortonormale)</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Se <span class="math-tag">\( v_{1}, \ \ldots \ , v_{k}\)</span> sono vettori a due due ortogonali non nulli (e quindi sono linearmente indipendenti), allora è possibile aggiungere dei vettori <span class="math-tag">\( v_{k + 1}, \ \ldots \ , v_{n}\)</span> in modo da ottenere una base ortogonale (o ortonormale se la norma di ogni vettore è <span class="math-tag">\( 1\)</span>)     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò occorre considerare che <span class="math-tag">\( v_{1}, \ \ldots \ , v_{k}\)</span> sono linearmente indipendenti. Per il teorema di completamento a una base, è possibile aggiungere altri vettori <span class="math-tag">\( v_{k + 1}, \ \ldots \ , v_{n}\)</span>. A questo punto, è possibile applicare Gram-Schmidt per ottenere una base ortogonale (o ortonormale).     </div></div></div><div class="definition environment" id="def8-11" ><span class="definition-header environment-title">Definizione 8.11 - Associazione di ogni prodotto scalare alla matrice identica</span>     Considerando la matrice associata ad un prodotto scalare non standard, si ha che, considerando una base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> la matrice associata al prodotto scalare è la seguente     <span class="math-tag">\[         \left(         \begin{array}{ccc}             f(v_{1}, v_{1}) & \cdots & f(v_{1}, v_{n}) \\             \vdots & \ddots & \vdots \\             f(v_{n}, v_{1}) & \cdots & f(v_{n}, v_{n})         \end{array}         \right)     \]</span>     Nel caso la base sia ortonormale, si ha che il prodotto scalare di vettori diversi è nullo, mentre il prodotto scalare di vettori uguali (ovvero quelli sulla diagonale) è <span class="math-tag">\( 1\)</span>, per cui     <span class="math-tag">\[         \left(         \begin{array}{ccc}             1 & \cdots & 0 \\             \vdots & \ddots & \vdots \\             0 & \cdots & 1         \end{array}         \right)     \]</span><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - In altre parole</span>         Ciò significa che qualunque prodotto scalare, nel caso si scelga una base ortonormale da cui dipenderanno le coordinate dei due vettori, può essere associato alla matrice identica (ovvero al prodotto scalare standard).     </div></div><div class="definition environment" id="def8-12" ><span class="definition-header environment-title">Definizione 8.12 - Complemento ortogonale di un sottospazio vettoriale euclideo</span>     Considerando un sottospazio vettoriale <span class="math-tag">\( W\)</span> di uno spazio vettoriale euclideo <span class="math-tag">\( V\)</span>, si definisce complemento ortogonale <span class="math-tag">\( W^{\perp}\)</span> l'insieme dei vettori di <span class="math-tag">\( V\)</span> che sono ortogonali a tutti i vettori di <span class="math-tag">\( W\)</span>, ovvero     <span class="math-tag">\[         W^{\perp} = \{          v \in V          \quad : \quad         v \star w = 0 \quad         \forall w \in W \}     \]</span></div><div class="demonstration environment" id="dem8-10" ><span class="demonstration-header environment-title">Dimostrazione 8.10 - Complemento ortogonale come sottospazio vettoriale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il complemento ortogonale di un sottospazio vettoriale <span class="math-tag">\( W\)</span> di <span class="math-tag">\( V\)</span> (<span class="math-tag">\( W^{\perp}\)</span>), è anch'esso un sottospazio vettoriale di <span class="math-tag">\( V\)</span>.      </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per verificare tale proposizione è necessario dimostrare che dati i vettori <span class="math-tag">\( u_{1}, u_{2} \in W^{\perp}\)</span>, la loro somma e il prodotto per uno scalare sia interna a <span class="math-tag">\( W\)</span>, ovvero per ogni <span class="math-tag">\( w \in W\)</span><span class="math-tag">\[             w \star (\alpha \cdot u_{1} + \beta \cdot u_{2}) = 0         \]</span>         Partendo quindi da          <span class="math-tag">\begin{aligned}             & w \star (\alpha \cdot u_{1} + \beta \cdot u_{2})             & \iff         \end{aligned}</span>         e applicando la linearità si ottiene         <span class="math-tag">\begin{aligned}             & \alpha \cdot (w \star u_{1}) + \beta \cdot (w \star u_{2}) = 0             &          \end{aligned}</span>         che è uguale a <span class="math-tag">\( 0\)</span> in quanto <span class="math-tag">\( u_{1}\)</span> e <span class="math-tag">\( u_{2}\)</span> appartengono già a <span class="math-tag">\( W^{\perp}\)</span> e il prodotto scalare con <span class="math-tag">\( w\)</span> è nullo.     </div></div></div><div class="definition environment" id="def8-13" ><span class="definition-header environment-title">Definizione 8.13 - Calcolare una base per il complemento ortogonale di un sottospazio vettoriale</span>     Dato che il complemento ortogonale <span class="math-tag">\( W^{\perp}\)</span> di un sottospazio vettoriale <span class="math-tag">\( W\)</span> è anch'esso un sottospazio vettoriale, esso ammette una base. Per trovarla è quindi sufficiente imporre la definizione di complemento ortogonale ad un generico vettore <span class="math-tag">\( v \in V\)</span> rispetto a tutti i vettori della base <span class="math-tag">\( B_{W} (w_{1}, \ \ldots \ , w_{n})\)</span>  e risolvere il sistema lineare che ne deriva, ovvero     <span class="math-tag">\[         \left\{         \begin{array}{ccccc}             v & \star & w_{1} & = & 0 \\             \vdots &  & \\             v & \star & w_{n} & = & 0         \end{array}         \right.     \]</span></div><div class="myexample environment" id="example51" ><span class="myexample-header environment-title">Esempio 51 - Calcolo di una base per il complemento ortogonale di un sottospazio vettoriale</span>     Considerando un sottospazio vettoriale <span class="math-tag">\( W\)</span> di <span class="math-tag">\( \mathbb{R}^{3}\)</span> generato dalla base <span class="math-tag">\( B_{W} = ((1,2,3), (2, 2, 2))\)</span>. Utilizzando il prodotto vettoriale standard <span class="math-tag">\( \star\)</span> e un generico vettore <span class="math-tag">\( v = (x_{1}, x_{2}, x_{3})\)</span>, imponiamo le seguenti relazioni     <span class="math-tag">\[         \left\{         \begin{array}{ccccc}            (x_{1}, x_{2}, x_{3}) & \star & (1, 2, 3) & = & 0  \\            (x_{1}, x_{2}, x_{3}) & \star & (2, 2, 2) & = & 0         \end{array}         \right.     \]</span>     che genera il seguente sistema lineare     <span class="math-tag">\[         \left\{         \begin{array}{ccccccc}             1 x_{1} & + & 2 x_{2} & + & 3 x_{3} & = & 0 \\             2 x_{1} & + & 2 x_{2} & + & 2 x_{3} & = & 0         \end{array}         \right.     \]</span>     la cui soluzione parametrica è     <span class="math-tag">\[         \left\{         \begin{array}{ccccccc}             x & = & t \\             y & = & -2t \\             z & = & t \\         \end{array}         \right.         \quad         \implies         \quad         \left(         \begin{array}{c}             x \\ y \\ z         \end{array}         \right)         =         t \cdot         \left(         \begin{array}{c}             1 \\ 2 \\ 1         \end{array}         \right)     \]</span>     Dunque si ottiene che una base per il complemento ortogonale del sottospazio vettoriale è <span class="math-tag">\( B_{W^{\perp}} = ((1, 2, 1))\)</span>.     <div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Nel caso di altri prodotti scalare</span>         In questo esempio si è utilizzato il prodotto scalare standard per semplificare i calcoli: nel caso si utilizzasse un altro prodotto scalare cambierebbe solo la generazione del sistema lineare.     </div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Nel caso si esprimano i vettori rispetto a basi non canoniche</span>         In questo caso tutti i vettori sono espressi rispetto alla base canonica di <span class="math-tag">\( \mathbb{R}^{3}\)</span>: nel caso si utilizzasse una base diversa, cambierebbe la forma di un generico vettore <span class="math-tag">\( v\)</span>, calcolabile, considerando una base non canonica <span class="math-tag">\( B = (b_{1}, b_{2}, b_{3})\)</span> nel seguente modo:         <span class="math-tag">\[             v = x_{1} \cdot b_{1} + x_{2} \cdot b_{2} + x_{3} \cdot b_{3}         \]</span></div></div><div class="demonstration environment" id="dem8-11" ><span class="demonstration-header environment-title">Dimostrazione 8.11 - Dimensione del complemento ortogonale di un sottospazio vettoriale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La dimensione del complemento ortogonale <span class="math-tag">\( W^{\perp}\)</span> di un sottospazio vettoriale <span class="math-tag">\( W\)</span> dello spazio vettoriale euclideo <span class="math-tag">\( V\)</span> è dipendente dalle dimensioni di <span class="math-tag">\( W\)</span> e <span class="math-tag">\( V\)</span>, ovvero         <span class="math-tag">\[             dim(W^{\perp}) = dim(V) - dim(W)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando una base ortonormale <span class="math-tag">\( B_{W} = (w_{1}, \ \ldots \ , w_{k})\)</span> di <span class="math-tag">\( k\)</span> vettori per <span class="math-tag">\( W\)</span>, per il teorema di completamento ad una base ortonormale, è possibile ottenere una base <span class="math-tag">\( B_{V}\)</span> per <span class="math-tag">\( V\)</span> composta da <span class="math-tag">\( n\)</span> vettori         <span class="math-tag">\[             B_{V} = (w_{1}, \ \ldots \ , w_{k}, w_{k + 1}, \ \ldots \ , w_{n})         \]</span>         Per dimostrare la proposizione, è sufficiente dimostrare che i vettori (<span class="math-tag">\( w_{k + 1}, \ \ldots \ , w_{n}\)</span>) siano una base ortonormale per <span class="math-tag">\( W^{\perp}\)</span>.         <br ></br>         Innanzitutto, sappiamo che ogni vettore (<span class="math-tag">\( w_{k + 1}, \ \ldots \ , w_{n}\)</span>) appartiene a <span class="math-tag">\( W^{\perp}\)</span> in quanto sono ortogonali a tutti i vettori di <span class="math-tag">\( W\)</span> (grazie al fatto che abbiamo considerato una base ortonormale) e che sono linearmente indipendenti (in quanto sono ortogonali tra loro).          <br ></br>         &Egrave; quindi necessario dimostrare che i vettori  (<span class="math-tag">\( w_{k + 1}, \ \ldots \ , w_{n}\)</span>) siano un sistema di generatori per <span class="math-tag">\( W^{\perp}\)</span>. Per farlo consideriamo che <span class="math-tag">\( W^{\perp}\)</span> è un sottospazio vettoriale di <span class="math-tag">\( V\)</span>, allora è possibile esprimere un qualsiasi vettore <span class="math-tag">\( u \in W^{\perp}\)</span> come combinazione lineare di <span class="math-tag">\( B_{V}\)</span>, ovvero         <span class="math-tag">\[             u = x_{1} \cdot w_{1} + \ \ldots \ + x_{k} \cdot w_{k} + x_{k + 1} \cdot w_{k + 1} + \ \ldots \ + x_{n} \cdot w_{n}         \]</span>         Ora, è sufficiente dimostrare che le coordinate <span class="math-tag">\( (x_{1}, \ \ldots \ , x_{k})\)</span> sono nulle (perchè ciò significa che non contribuiscono alla combinazione lineare dei vettori di <span class="math-tag">\( W^{\perp}\)</span>)         Per definizione, si ha che <span class="math-tag">\( u \star w_{i} = 0\)</span> (ovvero che il prodotto tra <span class="math-tag">\( u\)</span> e i vettori <span class="math-tag">\( (w_{1}, \ \ldots \ , w_{k})\)</span> è <span class="math-tag">\( 0\)</span>) e sappiamo che il prodotto scalare tra un vettore e un vettore di una base ortonormale fornisce la coordinata associata a tale vettore (rispetto a tale base ortonormale): sappiamo quindi che le coordinate <span class="math-tag">\( x_{1}, \ \ldots \ , x_{k}\)</span> (ovvero quelle associate ai vettori di <span class="math-tag">\( B_{W}\)</span>) sono tutte nulle. Quindi, è possibile scrivere <span class="math-tag">\( u\)</span> come         <span class="math-tag">\[             u = x_{k + 1} \cdot w_{k + 1} + \ \ldots \ + x_{n} \cdot w_{n}         \]</span>         combinazione lineare dei vettori <span class="math-tag">\( w_{k + 1}, \ \ldots \ , w_{n}\)</span>.         <br ></br>         Si è dimostrato che tali vettori sono una base per <span class="math-tag">\( W^{\perp}\)</span>.     </div></div></div><div class="demonstration environment" id="dem8-12" ><span class="demonstration-header environment-title">Dimostrazione 8.12 - Complemento ortogonale del complemento ortogonale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il complemento ortogonale <span class="math-tag">\( (W^{\perp})^{\perp}\)</span> del complemento ortogonale <span class="math-tag">\( W^{\perp}\)</span> del sottospazio vettoriale <span class="math-tag">\( W\)</span> è il sottospazio vettoriale <span class="math-tag">\( W\)</span>, ovvero         <span class="math-tag">\[             (W^{\perp})^{\perp} = W         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - In altre parole</span>         Ciò significa che l'insieme dei vettori di <span class="math-tag">\( V\)</span> che sono ortogonali a tutti i vettori di <span class="math-tag">\( W^{\perp}\)</span> sono i vettori di <span class="math-tag">\( W\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Dimostrare tale proposizione significa dimostrare che <span class="math-tag">\( W\)</span> e <span class="math-tag">\( (W^{\perp})^{\perp}\)</span> sono lo stesso spazio vettoriale: per farlo è sufficiente dimostrare che una base per <span class="math-tag">\( W\)</span> è equivalente a una base per <span class="math-tag">\( (W^{\perp})^{\perp}\)</span>.         <br ></br>         Per farlo consideriamo una base ortonormale <span class="math-tag">\( B_{W} = (w_{1}, \ \ldots \ , w_{k})\)</span> e completiamola ad una base ortonormale per <span class="math-tag">\( B_{V}\)</span>, ovvero otteniamo la base         <span class="math-tag">\[             B_{V} = (w_{1}, \ \ldots \ , w_{k}, w_{k + 1}, \ \ldots \ , w_{n})         \]</span>         Per definizione, sappiamo quindi che i vettori aggiunti sono vettori linearmente indipendenti e ortogonali a tutti i vettori di <span class="math-tag">\( B_{W}\)</span>, inoltre sono esattamente <span class="math-tag">\( n - k\)</span> che è esattamente la dimensione del complemento ortogonale (ovvero <span class="math-tag">\( dim(W^{\perp}) = dim(V) - dim(W) = n - k\)</span>): per questo motivo possiamo dire che sono una base per <span class="math-tag">\( W^{\perp}\)</span>.         <br ></br>         Ora considerando <span class="math-tag">\( U = W^{\perp}\)</span>, per dimostrare la proposizione è necessario cercare <span class="math-tag">\( U^{\perp}\)</span>. Sempre considerando la relazione <span class="math-tag">\( dim(U^{\perp}) = dim(V) - dim(U)\)</span> otteniamo che <span class="math-tag">\( dim(U^{\perp}) = n - (n - k) = k\)</span>: è sufficiente quindi trovare <span class="math-tag">\( k\)</span> vettori ortogonali (in quanto l'ortogonalità implica la lineare indipendenza) tra loro. Ricordando che la base <span class="math-tag">\( B_{W}\)</span> contiene <span class="math-tag">\( k\)</span> vettori ortogonali tra loro, possiamo dire che <span class="math-tag">\( B_{W} = B_{U^{\perp}}\)</span> che dimostra la proposizione.      </div></div></div><div class="definition environment" id="def8-14" ><span class="definition-header environment-title">Definizione 8.14 - Matrice ortogonale</span>     Una matrice <span class="math-tag">\( A = M_{n \times n}(\mathbb{R})\)</span> si dice ortogonale se la sua inversa coincide con la sua trasposta, ovvero     <span class="math-tag">\[         {}^{t} \! A \odot A = I     \]</span>     Le matrici ortogonali sono anche le matrici le cui colonne (o le righe) sono le coordinate dei vettori di una base ortonormale.  </div><div class="myexample environment" id="example52" ><span class="myexample-header environment-title">Esempio 52 - Esempio di matrice ortogonale</span>     La matrice <span class="math-tag">\( 3 \times 3\)</span><span class="math-tag">\[         \left(         \begin{array}{ccc}            \frac{1}{3}  & \frac{2}{3} & \frac{2}{3} \\            \frac{2}{3}  & \frac{1}{3} & -\frac{2}{3} \\            -\frac{2}{3}  & \frac{2}{3} & -\frac{1}{3}         \end{array}         \right)     \]</span>     è ortogonale, infatti considerando la sua trasposta     <span class="math-tag">\[         \left(         \begin{array}{ccc}            \frac{1}{3}  & \frac{2}{3} & -\frac{2}{3} \\            \frac{2}{3}  & \frac{1}{3} & \frac{2}{3} \\            \frac{2}{3}  & -\frac{2}{3} & -\frac{1}{3}         \end{array}         \right)     \]</span>     si ha che     <span class="math-tag">\[         \left(         \begin{array}{ccc}            \frac{1}{3}  & \frac{2}{3} & \frac{2}{3} \\            \frac{2}{3}  & \frac{1}{3} & -\frac{2}{3} \\            -\frac{2}{3}  & \frac{2}{3} & -\frac{1}{3}         \end{array}         \right)         \odot          \left(         \begin{array}{ccc}            \frac{1}{3}  & \frac{2}{3} & -\frac{2}{3} \\            \frac{2}{3}  & \frac{1}{3} & \frac{2}{3} \\            \frac{2}{3}  & -\frac{2}{3} & -\frac{1}{3}         \end{array}         \right)         =          \left(         \begin{array}{ccc}            1 & 0 & 0 \\            0 & 1 & 0 \\            0 & 0 & 1         \end{array}         \right)     \]</span></div><div class="demonstration environment" id="dem8-13" ><span class="demonstration-header environment-title">Dimostrazione 8.13 - Determinante di una matrice ortogonale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il determinante di una matrice ortogonale <span class="math-tag">\( A\)</span> è <span class="math-tag">\( \pm 1\)</span>, ovvero         <span class="math-tag">\[             det(A) = \pm 1         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare questa proposizione, consideriamo che <span class="math-tag">\( 1\)</span> è il determinante della matrice identica, ovvero         <span class="math-tag">\begin{aligned}             & 1 = det(I)             & \iff         \end{aligned}</span>         e, dato che <span class="math-tag">\( A\)</span> è ortogonale, è possibile scrivere la matrice identica come         <span class="math-tag">\begin{aligned}             & det(A \odot {}^{t} \! A)             & \iff         \end{aligned}</span>         e per Binet         <span class="math-tag">\begin{aligned}             & det(A) \cdot det({}^{t} \! A)              & \iff         \end{aligned}</span>         Ricordandosi che il determinante di una matrice è uguale a quello della sua trasposta, si ottiene che         <span class="math-tag">\begin{aligned}            & det(A) \cdot det(A) = (det(A))^{2}            & \iff         \end{aligned}</span>         e paragonando il risultato con il valore di partenza si ottiene         <span class="math-tag">\begin{aligned}             & 1 = (det(A))^{2}             & \iff \\             & \pm 1 = det(A)         \end{aligned}</span>         ovvero che il determinante di una matrice ortogonale può essere solo <span class="math-tag">\( \pm 1\)</span>.     </div></div></div><div class="definition environment" id="def8-15" ><span class="definition-header environment-title">Definizione 8.15 - Matrici ortogonalmente diagonalizzabili</span>     Una matrice <span class="math-tag">\( A \in M_{n \times n}(\mathbb{R})\)</span> si dice ortogonalmente diagonalizzabile se esiste una matrice <span class="math-tag">\( E\)</span> tale che <span class="math-tag">\( {}^{t} E \odot A \odot E\)</span> è diagonale.  </div><div class="definition environment" id="def8-16" ><span class="definition-header environment-title">Definizione 8.16 - Teorema spettrale</span>     Una matrice <span class="math-tag">\( A \in M_{n \times n}(\mathbb{R})\)</span> è ortogonalmente diagonalizzabile se e solo se è simmetrica. </div><div class="definition environment" id="def8-17" ><span class="definition-header environment-title">Definizione 8.17 - Isometria</span>     Un'isometria è un'applicazione lineare <span class="math-tag">\( f: V \to V\)</span>, dove <span class="math-tag">\( V\)</span> è uno spazio vettoriale euclideo che conserva il prodotto scalare tra vettori, ovvero, dati <span class="math-tag">\( v, w \in V\)</span><span class="math-tag">\[         v \star w = f(v) \star f(w)     \]</span>     Ciò significa che ogni isometria conserva angoli e norme.     <div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Esempi di isometrie</span>         Le isometrie sono le applicazioni lineari che rappresentano i movimenti rigidi nello spazio.     </div></div><div class="demonstration environment" id="dem8-14" ><span class="demonstration-header environment-title">Dimostrazione 8.14 - Matrice associata ad una isometria</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La matrice associata ad un'isometria <span class="math-tag">\( f: V \to V\)</span> è ortogonale.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò, consideriamo la matrice <span class="math-tag">\( A \in M_{n \times n}(\mathbb{R})\)</span> associata all'isometria <span class="math-tag">\( f: V \to V\)</span>, le coordinate di due vettori <span class="math-tag">\( x = (x_{1}, \ \ldots \ , x_{n})\)</span> e <span class="math-tag">\( y = (y_{1}, \ \ldots \ , y_{n})\)</span>, si avrà che l'immagine dei due vettori può essere così scritta         <span class="math-tag">\begin{aligned}             & f(x) = A \odot {}^{t} \! x             & f(y) = A \odot {}^{t} \! y         \end{aligned}</span>         Dove <span class="math-tag">\( x\)</span> e <span class="math-tag">\( y\)</span> sono trasposti per poterli moltiplicare alla matrice <span class="math-tag">\( A\)</span>. Tuttavia, le immagini risultano essere vettori colonna mentre per fare il prodotto è necessario che <span class="math-tag">\( f(x)\)</span> sia un vettore riga. Ciò significa che si deve trasporre <span class="math-tag">\( f(x)\)</span> per cui si ottiene          <span class="math-tag">\begin{aligned}             & {}^{t} \! (A \odot {}^{t} \! x) = x \odot {}^{t} \! A         \end{aligned}</span>         Dunque, ricordando che ogni prodotto scalare può essere associato alla matrice identica (se si scelgono le basi giuste), si ha che la definizione di isometria può essere così scritta         <span class="math-tag">\[              x \odot {}^{t} \! A \odot A \odot {}^{t} \! y = x \odot I \odot {}^{t} \! y         \]</span>         Ciò può avvenire se e solo se <span class="math-tag">\( {}^{t} \! A \odot A = I\)</span>, ovvero se e solo se <span class="math-tag">\( A^{-1} = {}^{t} \! A\)</span> (che è la definizione di matrice ortogonale). Ciò significa che le matrici associate alle isometrie sono esattamente le matrici ortogonali.     </div></div></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Basi ortonormali di <span class="math-tag">\( \mathbb{R}^{2}\)</span></span>     Considerando un qualsiasi vettore di norma <span class="math-tag">\( 1\)</span>, le sue coordinate in funzione dell'angolo <span class="math-tag">\( \theta\)</span> sono     <span class="math-tag">\[         v_{1} = (\cos(\theta), \sin(\theta))     \]</span>     Il vettore <span class="math-tag">\( v_{2}\)</span> deve essere ortogonale a <span class="math-tag">\( v_{1}\)</span> e le possibilità sono che l'angolo di <span class="math-tag">\( v_{2}\)</span> sia <span class="math-tag">\( \theta \pm \frac{\pi}{2}\)</span>. Avremo quindi     <span class="math-tag">\begin{aligned}         \text{se } \theta + \frac{\pi}{2}         \quad \implies \quad         v_{2} = (\cos(\theta + \frac{\pi}{2}), \sin(\theta + \frac{\pi}{2}))         \\         \text{se } \theta - \frac{\pi}{2}         \quad \implies \quad         v_{2} = (\cos(\theta - \frac{\pi}{2}), \sin(\theta - \frac{\pi}{2}))     \end{aligned}</span>     che, ricordandosi gli angoli associati, è come scrivere     <span class="math-tag">\begin{aligned}         \text{se } \theta + \frac{\pi}{2}         \quad &\implies \quad         v_{2} = (-\sin(\theta), \cos(\theta))         \\         \text{se } \theta - \frac{\pi}{2}         \quad &\implies \quad         v_{2} = (\sin(\theta), -\cos(\theta))     \end{aligned}</span>     Dunque, avremo che tutte le possibili basi ortonormali di <span class="math-tag">\( \mathbb{R}^{2}\)</span> sono      <span class="math-tag">\begin{aligned}         & B_{1} = ((\cos(\theta), \sin(\theta)), (-\sin(\theta), \cos(\theta)))         \\         & B_{2} = ((\cos(\theta), \sin(\theta)), (\sin(\theta), -\cos(\theta)))     \end{aligned}</span>     e le relative matrici ortogonali sono     <span class="math-tag">\[         \left(         \begin{array}{cc}             \cos(\theta) & \sin(\theta) \\             -\sin(\theta) & \cos(\theta)         \end{array}         \right)     \]</span>     che rappresentano la rotazione di <span class="math-tag">\( \theta\)</span> radianti nel piano e le matrici      <span class="math-tag">\[         \left(         \begin{array}{cc}             \cos(\theta) & \sin(\theta) \\             \sin(\theta) & -\cos(\theta)         \end{array}         \right)     \]</span>     che rappresentano la rotazione di <span class="math-tag">\( \theta\)</span> radianti nel piano e la simmetria di una retta passante per l'origine. </div><div class="demonstration environment" id="dem8-15" ><span class="demonstration-header environment-title">Dimostrazione 8.15 - 1 autovalore di ogni matrice <span class="math-tag">\( n \times n\)</span> ortogonale, con determinante positivo e <span class="math-tag">\( n\)</span> dispari</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando una matrice <span class="math-tag">\( A \in M_{n \times n}(\mathbb{R})\)</span> ortogonale, con il determinante positivo (<span class="math-tag">\( det(A) \gt  0\)</span>) e <span class="math-tag">\( n\)</span> dispari, allora un autovalore di <span class="math-tag">\( A\)</span> è <span class="math-tag">\( 1\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Dimostrare che <span class="math-tag">\( 1\)</span> è autovalore di <span class="math-tag">\( A\)</span>, significa dire che il polinomio caratterisco di <span class="math-tag">\( A\)</span> con <span class="math-tag">\( \lambda = 1\)</span> è uguale a <span class="math-tag">\( 0\)</span>, ovvero         <span class="math-tag">\begin{aligned}             & p_{A}(1) = 0             & \iff         \end{aligned}</span>         che è equivalente a scrivere         <span class="math-tag">\begin{aligned}             & 0 = det(A - 1 \cdot I)             & \iff         \end{aligned}</span>         Allora, ricordando che la trasposta di una matrice ortogonale è anche la sua inversa, è possibile scrivere         <span class="math-tag">\begin{aligned}             & det(A - A \odot {}^{t} \! A)             & \iff         \end{aligned}</span>         e raccogliendo <span class="math-tag">\( A\)</span> si può ottenere          <span class="math-tag">\begin{aligned}             & det(A \odot (I - {}^{t} \! A)             & \iff         \end{aligned}</span>         e per Binet         <span class="math-tag">\begin{aligned}             & det(A) \cdot det(I - {}^{t} \! A)             & \iff         \end{aligned}</span>         e, dato che il determinante di una matrice ortogonale può essere solo <span class="math-tag">\( \pm 1\)</span>, e che per ipotesi il determinante è positivo, si ha che il <span class="math-tag">\( det(A) = 1\)</span>, quindi è equivalente scrivere         <span class="math-tag">\begin{aligned}             & det(I - {}^{t} \! A)             & \iff         \end{aligned}</span>         Dato che il determinante di una matrice trasposta non cambia, si può trasporre la matrice <span class="math-tag">\( A - I\)</span> (che è uguale alla differenza delle trasposte), quindi         <span class="math-tag">\begin{aligned}             & det({}^{t} \! (I - {}^{t} \! A))             & \iff \\             & det(I - A)              & \iff         \end{aligned}</span>          e raccogliendo <span class="math-tag">\( -1\)</span> si ottiene         <span class="math-tag">\begin{aligned}             & det(-(A - I))             & \iff         \end{aligned}</span>          Ricordando che la moltiplicazione di una qualsiasi matrice per <span class="math-tag">\( -1\)</span> inverte il determinante, possiamo vedere <span class="math-tag">\( -(A - I)\)</span> come la moltiplicazione di <span class="math-tag">\( n\)</span> volte per <span class="math-tag">\( -1\)</span>, per cui scrivere <span class="math-tag">\( det(-(A - I))\)</span> è come scrivere         <span class="math-tag">\begin{aligned}             & (-1)^{n} \cdot det(A - I)             & \iff         \end{aligned}</span>          Nel caso quindi <span class="math-tag">\( n\)</span> sia dispari, avremmo che         <span class="math-tag">\[             det(A - I) = -det(A - I)         \]</span>         ovvero che i due determinanti sono uguali, ovvero che il determinante è nullo. Si è quindi dimostrato che <span class="math-tag">\( 1\)</span> è autovalore.     </div></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Una visione sul mondo</span>         Dire che <span class="math-tag">\( 1\)</span> è un autovalore, significa dire che esiste un vettore la cui immagine è sè stesso per ogni isometria associata ad una matrice con <span class="math-tag">\( n\)</span> dispari.     </div></div></div></div>
        </div>
    </div>
    <footer class="footer-container">
        <span>
            Created by 
            <typewriting-text class="credits-subtitle" still-time="1000" erasing-speed="150" >
                <word>lorenzoarlo</word>
                <word>Lorenzo Arlotti</word>
            </typewriting-text>
        </span>
    </footer>
</body>

</html>