<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="icon" href="../resources/logo.png" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
    <script defer id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script defer src="../scripts/TypewritingText.js"></script>
    <link rel="stylesheet" href="../styles/style.css" />
    <script src="../scripts/proof-event.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-78NHLXDQD8"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-78NHLXDQD8');
    </script>
    <title>Geometria e algebra - Dimostrazioni</title>
</head>

<body>
    <header class="header-container">
        <div class="logo-wrapper">
        </div>
        <div class="header-title">
            <h1>Geometria e algebra</h1>
            <span>Geometria e algebra - Dimostrazioni</span>
        </div>
        <div class="material-symbols-outlined header-title settings-button">
            <!-- settings -->
        </div>
    </header>
    <div class="main-container">
        <div class="content-container">
            <div class="section part" id="sec1" ><span class="section-header part-title">1 - Operazioni e strutture algebriche</span><div class="subsection part" id="subsec1-1" ><span class="subsection-header part-title">1.1 - Gruppi, anelli e campi</span><div class="demonstration environment" id="dem1-1" ><span class="demonstration-header environment-title">Dimostrazione 1.1 - Elemento neutro di <span class="math-tag">\( +\)</span> come elemento nullo per <span class="math-tag">\( \cdot\)</span> in un anello</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un anello <span class="math-tag">\( (A, +, \cdot)\)</span> e <span class="math-tag">\( a \in A\)</span>, si ha che l'elemento neutro di <span class="math-tag">\( +\)</span> è l'elemento nullo dell'operazione <span class="math-tag">\( \cdot\)</span>, ovvero         <span class="math-tag">\[             a \cdot 0_{+} = 0_{+}         \]</span>         dove <span class="math-tag">\( 0_{+}\)</span> è l'elemento neutro dell'operazione <span class="math-tag">\( +\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostare tale proprietà consideriamo l'uguaglianza         <span class="math-tag">\begin{aligned}             & a \cdot 0 = a \cdot 0              & \iff         \end{aligned}</span>         Ricordando che <span class="math-tag">\( 0\)</span> è l'elemento neutro di <span class="math-tag">\( +\)</span> (ovvero <span class="math-tag">\( 0_{+}\)</span>), si può dire che         <span class="math-tag">\begin{aligned}             & a \cdot 0 = a \cdot (0 + 0)             & \iff         \end{aligned}</span>         e che <span class="math-tag">\( \cdot\)</span> è distributiva rispetto a <span class="math-tag">\( +\)</span>, possiamo scrivere         <span class="math-tag">\begin{aligned}             & a \cdot 0 = a \cdot 0 + a \cdot 0             & \iff         \end{aligned}</span>         Considerando quindi <span class="math-tag">\( b\)</span> l'opposto di <span class="math-tag">\( a \cdot 0\)</span> (ovvero l'elemento inverso rispetto a <span class="math-tag">\( +\)</span>), si ha quindi         <span class="math-tag">\begin{aligned}             & b + (a \cdot 0) = b + (a \cdot 0 + a \cdot 0)             & \iff         \end{aligned}</span>         e, annullando l'espressione a sinistra e applicando l'associatività di <span class="math-tag">\( +\)</span><span class="math-tag">\begin{aligned}             & 0 = (b + (a \cdot 0) + (a \cdot 0))             & \iff \\             & 0 = 0 + (a \cdot 0)              & \iff \\             & 0 = (a \cdot 0)         \end{aligned}</span>         che dimostra la proprietà.     </div></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando l'espressione          <span class="math-tag">\begin{aligned}             (1 + 0) \cdot a = (1 + 0) \cdot a          \end{aligned}</span>         (dove <span class="math-tag">\( 1\)</span> è l'elemento neutro di <span class="math-tag">\( \cdot\)</span> e <span class="math-tag">\( 0\)</span> è l'elemento neutro di <span class="math-tag">\( +\)</span>), è possibile svolgerla in due modi differenti.         <br ></br>         Concentrandosi sull'espressione a sinistra, si ha che, per la proprietà distributiva di <span class="math-tag">\( \cdot\)</span> rispetto a <span class="math-tag">\( +\)</span>, è possibile scrivere         <span class="math-tag">\begin{aligned}             & (1 \cdot a) + (0 \cdot a) = (1 + 0) \cdot a             & \iff         \end{aligned}</span>         e applicando la definizione di unità rispetto a <span class="math-tag">\( \cdot\)</span> si ha che         <span class="math-tag">\begin{aligned}             & a + (0 \cdot a) = (1 + 0) \cdot a             & \iff         \end{aligned}</span>         Concentrandosi ora sull'espressione a destra, si ha per le definizioni di unità         <span class="math-tag">\begin{aligned}             & a + (0 \cdot a) = 1 \cdot a             & \iff \\             & a + (0 \cdot a) = a             & \iff         \end{aligned}</span>         Dato che ogni elemento di un gruppo ammette inverso, si ha che è possibile aggiungere l'opposto di <span class="math-tag">\( a\)</span> (ovvero <span class="math-tag">\( b\)</span>, l'inverso di <span class="math-tag">\( a\)</span> rispetto a <span class="math-tag">\( +\)</span>) ad entrambe le espressioni         <span class="math-tag">\begin{aligned}             & b + a + (0 \cdot a) = b + a              & \iff         \end{aligned}</span>         e applicando la proprietà associativa a destra e risolvendo l'espressione a sinistra si ha che         <span class="math-tag">\begin{aligned}             & (b + a) + (0 \cdot a) = 0             & \iff         \end{aligned}</span>         Ricordando infine che <span class="math-tag">\( (b + a) = 0\)</span> è l'elemento neutro di <span class="math-tag">\( +\)</span>, si ha che         <span class="math-tag">\begin{aligned}             & 0 \cdot a = 0             &         \end{aligned}</span>         che dimostra la proprietà.     </div></div></div><div class="demonstration environment" id="dem1-2" ><span class="demonstration-header environment-title">Dimostrazione 1.2 - Campi senza divisori dello zero</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         I campi non ammettono divisori dello zero.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, consideriamo il campo campo <span class="math-tag">\( (A, +, \cdot)\)</span>, <span class="math-tag">\( a, b \in A : a \neq 0\)</span> e <span class="math-tag">\( b \neq 0\)</span>, e supponiamo per assurdo che         <span class="math-tag">\begin{aligned}             & a \cdot b = 0             & \iff         \end{aligned}</span>         Dato che <span class="math-tag">\( a\)</span> non è nullo e siamo in un campo, <span class="math-tag">\( a\)</span> ammette inverso: si può quindi "moltiplicare" per <span class="math-tag">\( a^{-1}\)</span> da entrambe le parti         <span class="math-tag">\begin{aligned}             & a^{-1} \cdot (a \cdot b) = a^{-1} \cdot 0             & \iff         \end{aligned}</span>         Grazie all'associatività di <span class="math-tag">\( \cdot\)</span> è possibile scrivere         <span class="math-tag">\begin{aligned}             &(a^{-1} \cdot a) \cdot b = a^{-1} \cdot 0             & \iff         \end{aligned}</span>         e poi sostituire <span class="math-tag">\( (a^{-1} \cdot a)\)</span> con l'unità di <span class="math-tag">\( \cdot\)</span> (<span class="math-tag">\( 1\)</span>).         <span class="math-tag">\begin{aligned}             & 1 \cdot b = a^{-1} \cdot 0             & \iff \\             & b = a^{-1} \cdot 0             & \iff         \end{aligned}</span>         grazie al fatto che <span class="math-tag">\( 0\)</span> è l'elemento neutro di <span class="math-tag">\( +\)</span> (e quindi l'elemento nullo per <span class="math-tag">\( \cdot\)</span>)          <span class="math-tag">\begin{aligned}             & b = 0             &         \end{aligned}</span>         Si ottiene che <span class="math-tag">\( b = 0\)</span>, che è un assurdo in quanto <span class="math-tag">\( b \neq 0\)</span> era una delle assunzioni iniziali.     </div></div></div><div class="demonstration environment" id="dem1-3" ><span class="demonstration-header environment-title">Dimostrazione 1.3 - Condizione necessaria e sufficiente per gli insiemi delle classi di modulo per essere un campo</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span><span class="math-tag">\( (\mathbb{Z}_{n}, +, \cdot)\)</span> è un campo se e solo se <span class="math-tag">\( n\)</span> è un numero primo.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale teorema, è necessario dimostrare che         <ul ><li >"se <span class="math-tag">\( (\mathbb{Z}_{n}, +, \cdot)\)</span> è un campo, <span class="math-tag">\( n\)</span> è un numero primo". Per dimostrare ciò supponiamo per assurdo che <span class="math-tag">\( n\)</span> non sia primo. Ciò implicherebbe però che <span class="math-tag">\( \mathbb{Z}_{n}\)</span> ha divisori dello zero: dato che i campi non ammettono divisori dello zero, si è verificata questa implicazione.             </li><li >"se <span class="math-tag">\( n\)</span> è un numero primo, <span class="math-tag">\( (\mathbb{Z}_{n}, +, \cdot)\)</span> è un campo". Per dimostrare ciò è sufficiente dimostrare che ogni elemento non nullo ammette inverso rispetto a <span class="math-tag">\( \cdot\)</span>, in quanto le altre proprietà sono facilmente verificabili).             <br ></br>             Consideriamo quindi un elemento non nullo <span class="math-tag">\( z \in \mathbb{Z}_{n} : z \neq 0\)</span>. Dato che <span class="math-tag">\( \mathbb{Z}_{n}\)</span> ha finiti elementi, è possibile dire che potenze diverse avranno lo stesso valore, ovvero             <span class="math-tag">\begin{aligned}                 & z^{n} = z^{m} \qquad \text{con } n \gt  m                 & \iff \\                 & z^{n} - z^{m} = 0                 & \iff             \end{aligned}</span>             Dato che la proprietà distributiva è valida negli anelli, è possibile "raccogliere" <span class="math-tag">\( z^{m}\)</span> e scrivere             <span class="math-tag">\begin{aligned}                 z^{m} \cdot (z^{n - m} - 1) = 0             \end{aligned}</span>             Dato che i campi non ammettono divisori dello zero, si ha che uno dei due fattori deve essere nullo. Dato che <span class="math-tag">\( z\)</span> non è nullo (per ipotesi iniziale), è impossibile che <span class="math-tag">\( z^{m}\)</span> lo sia, allora obbligatoriamente si ha che             <span class="math-tag">\begin{aligned}                 & z^{n - m} - 1 = 0                 & \iff \\                 & z^{n - m} = 1                 & \iff \\                 & z^{n - m - 1} \cdot z = 1                 &             \end{aligned}</span>             e ricordando che <span class="math-tag">\( 1\)</span> è l'elemento neutro di <span class="math-tag">\( \cdot\)</span>, si ha che ogni elemento non nullo ammette un inverso: <span class="math-tag">\( (\mathbb{Z}_{n}, +, \cdot)\)</span> è quindi un campo.         </li></ul>         Si è quindi dimostrato il teorema.     </div></div></div></div><div class="subsection part" id="subsec1-2" ><span class="subsection-header part-title">1.2 - Omomorfismi e isomorfismi</span><div class="demonstration environment" id="dem1-4" ><span class="demonstration-header environment-title">Dimostrazione 1.4 - Esistenza di un omomorfismo tra gruppi e commutatività di un gruppo come condizione sufficiente per la commutatività dell'altro</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due gruppi <span class="math-tag">\( (A, +_{A})\)</span> e <span class="math-tag">\( (B, +_{B})\)</span> e un omorfismo tra gruppi          <span class="math-tag">\[             f: A \to B         \]</span>         se il primo gruppo è commutativo, allora anche il secondo lo è.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando gli elementi <span class="math-tag">\( a_{1}, a_{2} \in A\)</span> e gli elementi <span class="math-tag">\( b_{1}, b_{2} \in B\)</span>, ipotizziamo che il gruppo <span class="math-tag">\( (A, +_{A})\)</span> sia commutativo, quindi si può scrivere         <span class="math-tag">\begin{aligned}             & f(a_{1} +_{A} a_{2}) = f(a_{2} +_{A} a_{1})             & \iff         \end{aligned}</span>         e applicando la definizione di omomorfismo         <span class="math-tag">\begin{aligned}             & b_{1} +_{B} b_{2} = b_{2} +_{B} b_{1}             &          \end{aligned}</span>         che è esattamente la definizione di gruppo commutativo: si è quindi dimostrata la proposizione.     </div></div></div></div></div><div class="section part" id="sec2" ><span class="section-header part-title">2 - Spazi vettoriali</span><div class="subsection part" id="subsec2-1" ><span class="subsection-header part-title">2.1 - Nozioni preliminari su vettori e spazi vettoriali</span></div><div class="subsection part" id="subsec2-2" ><span class="subsection-header part-title">2.2 - Combinazione lineare e lineare indipendenza</span><div class="demonstration environment" id="dem2-1" ><span class="demonstration-header environment-title">Dimostrazione 2.1 - Vettori linearmente indipendenti se e solo se nessuno è combinazione lineare degli altri</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         I vettori <span class="math-tag">\( (v_{1}, \ \ldots \ , v_{s})\)</span> sono linearmente indipendenti se e solo se nessuno di essi si può scrivere come combinazione lineare dei rimanenti.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, è necessario dimostrare che         <ul ><li >"se i vettori sono linearmente indipendenti, allora nessuno di essi si può scrivere come combinazione lineare degli altri". <br ></br>             Dunque, supponendo che i vettori <span class="math-tag">\( (v_{1}, \ \ldots \ , v_{s})\)</span> sono linearmente indipendenti, supponiamo per assurdo che uno di questi vettori è combinazione lineare degli altri, ovvero             <span class="math-tag">\begin{aligned}                 & v_{1} = a_{2} \cdot v_{2} + \ \ldots \ + a_{s} \cdot v_{s}                  & \iff             \end{aligned}</span>             Ora, "portando tutto a sinistra" si ottiene             <span class="math-tag">\begin{aligned}                 & 1 \cdot v_{1} - a_{2} \cdot v_{2} - \ \ldots \ - a_{s} \cdot v_{s} = 0_{V}             \end{aligned}</span>             che è un assurdo, in quanto abbiamo scritto il vettore nullo senza che i coefficienti siano tutti nulli;             </li><li >"se nessuno dei vettori si può scrivere come combinazione lineare degli altri, allora sono linearmente indipendenti".             <br ></br>             Dunque, ipotizzando per assurdo che <span class="math-tag">\( (v_{1}, \ \ldots \ , v_{s})\)</span> non siano linearmente indipendenti, si avrebbe una combinazione lineare             <span class="math-tag">\begin{aligned}                 & a_{1} \cdot v_{1} + a_{2} \cdot v_{2} + \ \ldots \ + a_{s} \cdot v_{s} = 0_{V}                 & \iff             \end{aligned}</span>             non composta da tutti coefficienti nulli (ipotizziamo quindi che <span class="math-tag">\( a_{1}\)</span> non sia nullo) e "portando a destra" tutti gli altri vettori si avrebbe             <span class="math-tag">\begin{aligned}                 & a_{1} \cdot v_{1} = - a_{2} \cdot v_{2} + \ \ldots \ - a_{s} \cdot v_{s}                 &             \end{aligned}</span>             che è una contraddizione, in quanto abbiamo scritto un vettore come combinazione lineare degli altri.         </li></ul>         Si è quindi dimostrata la proposizione     </div></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Definizioni lineare indipendenza ugualmente valide</span>         Questa definizione di lineare indipendenza e la precedente sono entrambe definizioni valide per descrivere la lineare indipendenza.          <br ></br>         Tuttavia la prima è più semplice (in quanto non privilegia alcun elemento) per le dimostrazioni, mentre la seconda risulta essere più utile per il calcolo.     </div></div><div class="demonstration environment" id="dem2-2" ><span class="demonstration-header environment-title">Dimostrazione 2.2 - Combinazione lineare unica rispetto ad una base</span>     Data l'affermazione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando una base         <span class="math-tag">\[             B = (v_{1}, \ \ldots \ , v_{n} )         \]</span>         per lo spazio vettoriale <span class="math-tag">\( V\)</span>, allora è possibile associare ad ogni vettore <span class="math-tag">\( v \in V\)</span>, una e una sola <span class="math-tag">\( n\)</span>-upla di coordinate rispetto a <span class="math-tag">\( B\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Supponiamo che esistano due combinazioni lineari di <span class="math-tag">\( v\)</span> rispetto a <span class="math-tag">\( B\)</span>, ovvero         <span class="math-tag">\begin{aligned}             & v = x_{1} \cdot v_{1} + \ \ldots \ + x_{n} v_{n}              \\             & v = y_{1} \cdot v_{1} + \ \ldots \ + y_{n} v_{n}          \end{aligned}</span>         allora è lecito scrivere         <span class="math-tag">\begin{aligned}             & x_{1} \cdot v_{1} + \ \ldots \ + x_{n} v_{n} = y_{1} \cdot v_{1} + \ \ldots \ + y_{n} v_{n}             & \iff         \end{aligned}</span>         e, portando prima tutto a sinistra e poi raccogliendo i vettori <span class="math-tag">\( v_{i}\)</span>, si ottiene         <span class="math-tag">\begin{aligned}             & x_{1} \cdot v_{1} + \ \ldots \ + x_{n} v_{n} - y_{1} \cdot v_{1} - \ \ldots \ - y_{n} v_{n} = 0_{V}             & \iff \\             & (x_{1} - y_{1}) \cdot v_{1} + \ \ldots \ + (x_{n} - y_{n}) \cdot v_{n} = 0_{V}         \end{aligned}</span>         Considerando la lineare indipendenza, si ha che l'unico modo per ottenere il vettore nullo è che tutti i coefficienti siano nulli, per cui <span class="math-tag">\( x_{i} - y_{i}\)</span> deve essere uguale a <span class="math-tag">\( 0\)</span>: ciò significa che <span class="math-tag">\( x_{i}\)</span> e <span class="math-tag">\( y_{i}\)</span> sono uguali e per questo esiste un'unica <span class="math-tag">\( n\)</span>-upla di coordinate.         <br ></br>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem2-3" ><span class="demonstration-header environment-title">Dimostrazione 2.3 - Esistenza di una base per ogni spazio finitamente generato</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Ogni spazio vettoriale finitamente generato ammette almeno una base.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando un qualsiasi insieme di vettori, è possibile o che tali vettori non siano un sistema di generatori o che non siano linearmente indipendenti: in caso contrario si avrebbe quindi una base.         <br ></br>         Nel caso tali vettori non siano un sistema di generatori, è sufficiente aggiungere i vettori che non sono generati dalla chiusura lineare.         <br ></br>         Nel caso invece siano un sistema di generatori ma non siano linearmente indipendenti, è sufficiente eliminare i vettori che sono combinazione lineare degli altri: si ottiene quindi una base.     </div></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Base dello spazio vettoriale nullo</span>         Alcune definizioni considerano che lo spazio vettoriale nullo non ammette base. In questo caso, si può considerare come sua base l'insieme vuoto <span class="math-tag">\( \varnothing\)</span>.     </div></div><div class="demonstration environment" id="dem2-4" ><span class="demonstration-header environment-title">Dimostrazione 2.4 - Equipotenza delle basi</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Tutte le basi di uno spazio vettoriale <span class="math-tag">\( V\)</span> finitamente generato hanno la stessa cardinalità.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando due basi per <span class="math-tag">\( V\)</span><span class="math-tag">\( B_{1}\)</span> e <span class="math-tag">\( B_{2}\)</span> si ha che, per definizione di base, entrambe sono sia un sistema di generatori sia che siano linearmente indipendenti.          <br ></br>         Quindi, dato che <span class="math-tag">\( B_{1}\)</span> è un sistema di generatori e <span class="math-tag">\( B_{2}\)</span> è linearmente indipendente, possiamo dire che la cardinalità di <span class="math-tag">\( B_{1}\)</span> è maggiore o uguale della cardinalità di <span class="math-tag">\( B_{2}\)</span>, ovvero         <span class="math-tag">\[             \left| B_{1} \right| \geq \left| B_{2} \right|         \]</span>         Tuttavia, si ha che anche <span class="math-tag">\( B_{2}\)</span> è un sistema di generatori e <span class="math-tag">\( B_{1}\)</span> è linearmente indipendente, quindi la cardinalità di <span class="math-tag">\( B_{2}\)</span> è maggiore o uguale alla cardinalità di <span class="math-tag">\( B_{1}\)</span>, ovvero         <span class="math-tag">\[             \left| B_{2} \right| \geq \left| B_{1} \right|         \]</span>         Unendo le due relazione, si ha infine che         <span class="math-tag">\[             \left| B_{1} \right| \geq \left| B_{2} \right| \geq \left| B_{1} \right|             \qquad \implies \qquad             \left| B_{1} \right| = \left| B_{2} \right|         \]</span></div></div></div><div class="demonstration environment" id="dem2-5" ><span class="demonstration-header environment-title">Dimostrazione 2.5 - Teorema del completamento a una base</span>     Dato il seguente teorema      <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Ogni insieme linearmente indipendente può essere completato ad una base per uno spazio vettoriale finitamente generato.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando          <span class="math-tag">\[             (w_{1}, \ \ldots \ , w_{k})         \]</span>         una <span class="math-tag">\( k\)</span>-upla di vettori linearmente indipendenti, nel caso essa sia già un sistema di generatori è possibile dire che è già una base.          <br ></br>         Nel caso non lo sia, allora esiste un vettore <span class="math-tag">\( w_{k + 1}\)</span> che non è combinazione lineare della <span class="math-tag">\( k\)</span>-upla. Consideriamo quindi la nuova <span class="math-tag">\( k + 1\)</span>-upla         <span class="math-tag">\[             (w_{1}, \ \ldots \ , w_{k}, w_{k + 1})         \]</span>         &Egrave; quindi necessario verificare che tali vettori siano linearmente indipendenti. Ciò non avviene in due casi:         <ul ><li >se <span class="math-tag">\( w_{k + 1}\)</span> è combinazione lineare dei rimanenti (che non avviene proprio perchè abbiamo scelto uno che non è combinazione lineare degli altri);             </li><li >se uno dei primi <span class="math-tag">\( k\)</span> vettori è combinazione lineare degli altri, ovvero ipotizziamo per assurdo che <span class="math-tag">\( w_{1}\)</span> (o uno degli altri vettori) è combinazione lineare dei rimanenti              <span class="math-tag">\begin{aligned}                 & w_{1} = a_{2} \cdot w_{2} + \ \ldots \ + a_{k} \cdot w_{k} + a_{k + 1} \cdot w_{k + 1}                 & \iff             \end{aligned}</span>             si ha che sicuramente <span class="math-tag">\( a_{k + 1}\)</span> non è nullo (per la lineare indipendenza dei primi <span class="math-tag">\( k\)</span> vettori). Detto ciò, si può quindi isolare <span class="math-tag">\( w_{k + 1}\)</span>, ottenendo quindi             <span class="math-tag">\begin{aligned}                 & a_{k + 1} \cdot w_{k + 1} = w_{1} - a_{2} \cdot w_{2} - \ \ldots \ - a_{k} \cdot w_{k}                 & \iff             \end{aligned}</span>             e moltiplicando per l'inverso di <span class="math-tag">\( a_{k + 1}\)</span> (che non è nullo)             <span class="math-tag">\begin{aligned}                 & w_{k + 1} = \frac{w_{1}}{a_{k + 1}} - \frac{a_{2}}{a_{k + 1}} \cdot w_{2} - \ \ldots \ - \frac{a_{k}}{a_{k + 1}} \cdot w_{k}                 & \iff             \end{aligned}</span>             si è ottenuto quindi un assurdo (ovvero <span class="math-tag">\( w_{k + 1}\)</span> come combinazione lineare dei rimanenti). Dunque la <span class="math-tag">\( k + 1\)</span>-upla è linearmente indipendente.         </li></ul>         A questo punto se la <span class="math-tag">\( k + 1\)</span>-upla è un sistema di generatori si è dimostrato il teorema, altrimenti è possibile ripetere il procedimento fino al raggiungimento di una base (che prima o poi terminerà in quanto lo spazio è finitamente generato).     </div></div></div><div class="demonstration environment" id="dem2-6" ><span class="demonstration-header environment-title">Dimostrazione 2.6 - Cardinalità di una base come numero massimo di vettori linearmente indipendenti</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando <span class="math-tag">\( dim(V)\)</span> la dimensione di uno spazio vettoriale <span class="math-tag">\( V\)</span>, allora il numero massimo di vettori linearmente indipendenti ammissibili in quello spazio è proprio <span class="math-tag">\( dim(V)\)</span>. Inoltre, se la cardinalità di tale insieme di vettori è proprio <span class="math-tag">\( dim(V)\)</span>, si ha che tali vettori sono una base per <span class="math-tag">\( V\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare che il numero massimo di vettori linearmente indipendenti sia proprio la dimensione di una qualsiasi base è possibile utilizzare il "teorema del completamento ad una base", che ci permette di dire che è possibile aggiungere vettori ad un insieme di vettori linearmente indipendenti fino all'ottenimento di una base. Tuttavia, per "l'equipotenza delle basi", si ha che nel momento in cui l'insieme di vettori è una base, esso ha proprio <span class="math-tag">\( dim(V)\)</span> elementi.         <br ></br>         Per lo stesso motivo, si ha che se un insieme linearmente indipendente ha <span class="math-tag">\( dim(V)\)</span> elementi, allora esso è una base.     </div></div></div></div><div class="subsection part" id="subsec2-3" ><span class="subsection-header part-title">2.3 - Sottostrutture algebriche</span></div><div class="subsection part" id="subsec2-4" ><span class="subsection-header part-title">2.4 - Operazioni tra sottospazi vettoriali</span><div class="demonstration environment" id="dem2-7" ><span class="demonstration-header environment-title">Dimostrazione 2.7 - Intersezione tra sottospazi di <span class="math-tag">\( V\)</span> come sottospazio di <span class="math-tag">\( V\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         L'intersezione tra due sottospazi vettoriali di <span class="math-tag">\( V\)</span> (<span class="math-tag">\( W \cap U\)</span>), è anch'essa un sottospazio vettoriale di <span class="math-tag">\( V\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando <span class="math-tag">\( u, w \in (W \cap U)\)</span>, si ha che:         <ul ><li >l'elemento nullo di <span class="math-tag">\( V\)</span> è presente sia in <span class="math-tag">\( W\)</span> che in <span class="math-tag">\( U\)</span>, in quanto <span class="math-tag">\( W\)</span> e <span class="math-tag">\( V\)</span> sono già sottospazi di <span class="math-tag">\( V\)</span>;             </li><li >la somma <span class="math-tag">\( u + w\)</span> la possiamo vedere come somma tra elementi di <span class="math-tag">\( W\)</span> (oppure di <span class="math-tag">\( V\)</span>): in questo modo il fatto che la somma sia interna è già verificato;             </li><li >il prodotto per uno scalare <span class="math-tag">\( \lambda \cdot u\)</span> è interno sia al sottospazio <span class="math-tag">\( W\)</span> che a <span class="math-tag">\( U\)</span>, il che significa che appartiene ancora ad un sottospazio vettoriale.         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem2-8" ><span class="demonstration-header environment-title">Dimostrazione 2.8 - Somma tra sottospazi di <span class="math-tag">\( V\)</span> come sottospazio di <span class="math-tag">\( V\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La somma tra due sottospazi vettoriali di <span class="math-tag">\( V\)</span> (<span class="math-tag">\( W + U\)</span>) è anch'essa un sottospazio vettoriale di <span class="math-tag">\( V\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando <span class="math-tag">\( u, u' \in U\)</span> e <span class="math-tag">\( w, w' \in W\)</span>, allora due vettori di <span class="math-tag">\( W + U\)</span>, si possono scrivere come <span class="math-tag">\( (u + w)\)</span> e <span class="math-tag">\( (u' + w')\)</span>: ciò è utile in quanto per dimostrare che <span class="math-tag">\( W + U\)</span> è un sottospazio vettoriale di <span class="math-tag">\( V\)</span> è necessario dimostrare che:         <ul ><li >l'elemento nullo di <span class="math-tag">\( V\)</span> è contenuto anche in <span class="math-tag">\( W + U\)</span>. Per dimostrare ciò consideriamo che <span class="math-tag">\( W\)</span> e <span class="math-tag">\( U\)</span> contengono già l'elemento nullo, quindi è possibile scrivere l'elemento nullo di <span class="math-tag">\( W + U\)</span> come la somma degli elementi nulli di <span class="math-tag">\( W\)</span> e <span class="math-tag">\( U\)</span>;             </li><li >la somma è interna, ovvero considerando             <span class="math-tag">\begin{aligned}                 & (w + u) + (w' + u')                 & =             \end{aligned}</span>             si può scrivere come             <span class="math-tag">\begin{aligned}                 & w + u + w' + u'                 & =             \end{aligned}</span>             e per la commutatività (prima) e l'associatività (poi) si ha              <span class="math-tag">\begin{aligned}                 & (w + w') + (u + u')                 &             \end{aligned}</span>             ovvero si ha che la somma tra due elementi di <span class="math-tag">\( W + U\)</span> è anch'essa un vettore di <span class="math-tag">\( W + U\)</span> (dato che <span class="math-tag">\( (w + w')\)</span> appartiene a <span class="math-tag">\( W\)</span> e <span class="math-tag">\( (u + u')\)</span> appartiene a <span class="math-tag">\( U\)</span>).             </li><li >il prodotto per uno scalare è interno, ovvero considerando             <span class="math-tag">\begin{aligned}                 & \lambda \cdot (w + u)                 & =             \end{aligned}</span>             si ha che             <span class="math-tag">\begin{aligned}                 & \lambda \cdot w + \lambda \cdot u                 &             \end{aligned}</span>             che è anch'esso interno ad <span class="math-tag">\( W + U\)</span>.         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Unione tra sottospazi non è un sottospazio vettoriale di <span class="math-tag">\( V\)</span></span>         &Egrave; da notare che l'unione tra due sottospazi vettoriali di <span class="math-tag">\( V\)</span>, a differenza della somma e l'intersezione, non è generalmente anch'essa un sottospazio vettoriale di <span class="math-tag">\( V\)</span>.     </div></div><div class="demonstration environment" id="dem2-9" ><span class="demonstration-header environment-title">Dimostrazione 2.9 - Formula di Grassmann</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando <span class="math-tag">\( W\)</span> e <span class="math-tag">\( U\)</span> due sottospazi vettoriali di <span class="math-tag">\( V\)</span>, allora si ha che la dimensione dello spazio vettoriale somma (<span class="math-tag">\( dim(W + U)\)</span>) è uguale alla dimensione di <span class="math-tag">\( W\)</span> sommata alla dimensione di <span class="math-tag">\( U\)</span> meno l'intersezione di <span class="math-tag">\( W\)</span> e <span class="math-tag">\( U\)</span>, ovvero         <span class="math-tag">\[             dim(W + U) = dim(W) + dim(U) - dim(W \cap U)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare la proposizione, consideriamo una base per <span class="math-tag">\( W \cap U\)</span>:         <span class="math-tag">\[             B_{W \cap U} = (v_{1}, \ \ldots \ , v_{s})         \]</span>         Possiamo dire che l'intersezione dei due sottospazi è contenuta in <span class="math-tag">\( W\)</span> e, per il teorema del completamento ad una base, possiamo completare <span class="math-tag">\( B_{W \cap U}\)</span> ad una base per <span class="math-tag">\( W\)</span>, ottenendo così la base         <span class="math-tag">\[             B_{W} = (v_{1}, \ \ldots \ , v_{s}, w_{s + 1}, \ \ldots \ , w_{s + r})         \]</span>         Inoltre, è possibile effettuare tali osservazioni anche per <span class="math-tag">\( U\)</span> ottenendo la base         <span class="math-tag">\[             B_{U} = (v_{1}, \ \ldots \ , v_{s}, u_{s + 1}, \ \ldots \ , u_{s + t})         \]</span>         A questo punto, è possibile definire l'unione dei vettori di <span class="math-tag">\( B_{W \cap U}\)</span>, <span class="math-tag">\( B_{W}\)</span> e <span class="math-tag">\( B_{U}\)</span><span class="math-tag">\[             B_{W + U} = (v_{1}, \ \ldots \ , v_{s}, w_{s + 1}, \ \ldots \ , w_{s + r}, u_{s + 1}, \ \ldots \ , u_{s + t})         \]</span>         e definirla una base per <span class="math-tag">\( W + U\)</span>, in quanto:         <ul ><li ><span class="math-tag">\( B_{W + U}\)</span> è un sistema di generatori per <span class="math-tag">\( W + U\)</span>, in quando considerando un vettore <span class="math-tag">\( \overline{v} \in U + W\)</span>, è possibile scriverlo come             <span class="math-tag">\begin{aligned}                 & \overline{v} = w + u                 & \iff             \end{aligned}</span>             e, dato che <span class="math-tag">\( w \in W\)</span> e <span class="math-tag">\( u \in U\)</span>, è possibile scriverli come combinazione lineare delle basi <span class="math-tag">\( B_{W}\)</span> e <span class="math-tag">\( B_{U}\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & w = \sum_{i = 1}^{s} a_{i} \cdot v_{i} + \sum_{j = 1}^{r} b_{s + j} \cdot w_{s + j} \\                 & u = \sum_{i = 1}^{s} a_{i}' \cdot v_{i} + \sum_{j = 1}^{t} c_{s + j} \cdot u_{s + j}             \end{aligned}</span>             quindi, sostituendoli nell'uguaglianza si ha che             <span class="math-tag">\begin{aligned}                 & \overline{v} = \sum_{i = 1}^{s} a_{i} \cdot v_{i} + \sum_{j = 1}^{r} b_{s + j} \cdot w_{s + j} + \sum_{i = 1}^{s} a_{i}' \cdot v_{i} + \sum_{j = 1}^{t} c_{s + j} \cdot u_{s + j}                 & \iff             \end{aligned}</span>             e raccogliendo i vettori <span class="math-tag">\( v_{i}\)</span> si ottiene             <span class="math-tag">\begin{aligned}                 & \overline{v} = \sum_{i = 1}^{s} v_{i} \cdot (a_{i} + a_{i}') + \sum_{j = 1}^{r} b_{s + j} \cdot w_{s + j} + \sum_{j = 1}^{r} c_{s + j} \cdot u_{s + j}                 &             \end{aligned}</span>             ovvero si è scritto un generico vettore di <span class="math-tag">\( W + U\)</span> come combinazione lineare di <span class="math-tag">\( B_{W + U}\)</span>.             </li><li ><span class="math-tag">\( B_{W + U}\)</span> è linearmente indipendente. Per dimostrarlo partiamo dalla combinazione lineare del vettore nullo, e proviamo che tutti i coefficienti siano nulli, ovvero             <span class="math-tag">\begin{aligned}                 & 0_{V} = \sum_{i = 1}^{s} a_{i} \cdot v_{i} + \sum_{j = 1}^{r} b_{s + j} \cdot w_{s + j} + \sum_{k = 1}^{t} c_{s + k} \cdot u_{s + k}                 & \iff             \end{aligned}</span>             Portando quindi a sinistra <span class="math-tag">\( \sum_{k = 1}^{t} c_{s + k} \cdot u_{s + k}\)</span> si ha che             <span class="math-tag">\begin{aligned}                 & (-1) \cdot (\sum_{k = 1}^{t} c_{s + k} \cdot u_{s + k}) = \sum_{i = 1}^{s} a_{i} \cdot v_{i} + \sum_{j = 1}^{r} b_{s + j} \cdot w_{s + j}                 & \iff             \end{aligned}</span>             ovvero che <span class="math-tag">\( \sum_{k = 1}^{t} c_{s + k} \cdot u_{s + k}\)</span> è combinazione lineare di <span class="math-tag">\( B_{W}\)</span> e, dato che tale vettore fa parte anche di <span class="math-tag">\( B_{U}\)</span>, si ha che appartengono a <span class="math-tag">\( W \cap U\)</span>.             <br ></br>             Detto ciò, si può quindi scrivere il vettore come combinazione lineare di <span class="math-tag">\( B_{W \cap U}\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & (-1) \cdot (\sum_{k = 1}^{t} c_{s + k} \cdot u_{s + k}) = \sum_{i = 1}^{s} d_{i} \cdot v_{i}                 & \iff             \end{aligned}</span>             e "portando nuovamente tutto da una parte", si ottiene che             <span class="math-tag">\begin{aligned}                 & 0_{V} = \sum_{i = 1}^{s} d_{i} \cdot v_{i} + \sum_{k = 1}^{t} c_{s + k} \cdot u_{s + k}                 &             \end{aligned}</span>             e dato che si è ottenuto il vettore nullo come combinazione lineare dei vettori della base <span class="math-tag">\( B_{U}\)</span> (tutti vettori linearmente indipendenti per ipotesi), si ha che tutti i coefficienti <span class="math-tag">\( c_{s + k}\)</span> sono nulli.             <br ></br>             Ritornando quindi all'uguaglianza iniziale             <span class="math-tag">\begin{aligned}                 & 0_{V} = \sum_{i = 1}^{s} a_{i} \cdot v_{i} + \sum_{j = 1}^{r} b_{s + j} \cdot w_{s + j} + \sum_{k = 1}^{t} c_{s + k} \cdot u_{s + k}                 & \iff             \end{aligned}</span>             e sostituendo a <span class="math-tag">\( c_{s + k}\)</span> il coefficiente <span class="math-tag">\( 0\)</span> si ottiene             <span class="math-tag">\begin{aligned}                 & 0_{V} = \sum_{i = 1}^{s} a_{i} \cdot v_{i} + \sum_{j = 1}^{r} b_{s + j} \cdot w_{s + j}                 &             \end{aligned}</span>             la combinazione lineare del vettore nullo della base <span class="math-tag">\( B_{W}\)</span>, di cui conosciamo la lineare indipendenza per ipotesi. Possiamo quindi dire che sia i coefficienti <span class="math-tag">\( a_{i}\)</span>, sia i coefficienti <span class="math-tag">\( b_{s + j}\)</span> sono <span class="math-tag">\( 0\)</span>.              <br ></br>             Abbiamo quindi dimostrato la lineare indipendenza.         </li></ul>         Si ha quindi che <span class="math-tag">\( B_{U + V}\)</span> è una base per <span class="math-tag">\( W + U\)</span>, ed è composta da:         <ul ><li ><span class="math-tag">\( s\)</span> vettori <span class="math-tag">\( (v_{1}, \ \ldots \ , v_{s})\)</span>;             </li><li ><span class="math-tag">\( r\)</span> vettori <span class="math-tag">\( (w_{s + 1}, \ \ldots \ , w_{s + r})\)</span>;             </li><li ><span class="math-tag">\( t\)</span> vettori <span class="math-tag">\( (u_{s + 1}, \ \ldots \ , u_{s + t})\)</span>.         </li></ul>         Riscrivendo le dimensioni dei vari spazi vettoriali si ha che         <span class="math-tag">\[             \begin{array}{lcl}                 dim(W + U) & = & s + r +t  \\                 dim(W) & = & s + r \\                 dim(U) & = & s + t \\                 dim(W \cap U) & = & s             \end{array}         \]</span>         e riscrivendo la formula di Grassman         <span class="math-tag">\[             \begin{array}{lcl}                 dim(W + U) & = & dim(W) + dim(U) - dim(W \cap U)  \\                 s + r + t & = & (s + r) + (s + t) - (s) \\                 s + r + t & = & s + r + t             \end{array}         \]</span>         è evidente la dimostrazione.     </div></div></div></div></div><div class="section part" id="sec3" ><span class="section-header part-title">3 - Matrici</span><div class="subsection part" id="subsec3-1" ><span class="subsection-header part-title">3.1 - Lo spazio vettoriale delle matrici <span class="math-tag">\( m \times n\)</span></span></div><div class="subsection part" id="subsec3-2" ><span class="subsection-header part-title">3.2 - L'anello delle matrici quadrate <span class="math-tag">\( n \times n\)</span></span><div class="demonstration environment" id="dem3-1" ><span class="demonstration-header environment-title">Dimostrazione 3.1 - Somma di matrici simmetriche come matrice simmetrica</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due matrici simmetriche <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span>, allora la matrice somma <span class="math-tag">\( A + B\)</span> è anch'essa una matrice simmetrica.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che (considerando <span class="math-tag">\( A\)</span> e <span class="math-tag">\( B\)</span> due matrici simmetriche) <span class="math-tag">\( A + B = {}^{t} \! (A + B)\)</span>.         <br ></br>         Considerando quindi che per definizione di matrice simmetrica, le matrici <span class="math-tag">\( A\)</span> e <span class="math-tag">\( B\)</span> sono uguali alle loro trasposte, si ha che         <span class="math-tag">\begin{aligned}             & A + B = {}^{t} \! A + {}^{t} \! B             & \iff         \end{aligned}</span>         e che la somma delle trasposte è uguale alla trasposta della somma         <span class="math-tag">\begin{aligned}             & A + B = {}^{t} \! (A + B)             &         \end{aligned}</span>         si è dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem3-2" ><span class="demonstration-header environment-title">Dimostrazione 3.2 - Traccia della matrice prodotto <span class="math-tag">\( A \odot B\)</span> equivalente alla traccia della matrice prodotto <span class="math-tag">\( B \odot A\)</span></span>     Dato la seguente proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due matrici <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span>, la traccia del prodotto <span class="math-tag">\( A \odot B\)</span> è equivalente alla traccia del prodotto <span class="math-tag">\( B \odot A\)</span>, ovvero         <span class="math-tag">\[             tr(A \odot B) = tr(B \odot A)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare la proposizione, consideriamo la matrice <span class="math-tag">\( C\)</span> come la matrice prodotto <span class="math-tag">\( A \odot B\)</span> e <span class="math-tag">\( D\)</span> la matrice prodotto <span class="math-tag">\( B \odot A\)</span>, ovvero         <span class="math-tag">\[             \begin{array}{lcl}                 A                  & = &                  \left(                 \begin{array}{c}                     a_{i, j}                 \end{array}                 \right)                 \\                 B                  & = &                  \left(                 \begin{array}{c}                     b_{i, j}                 \end{array}                 \right)                 \\                 C = A \odot B                 & = &                  \left(                 \begin{array}{c}                     c_{i, j}                 \end{array}                 \right)                 \\                 D = B \odot A                 & = &                  \left(                 \begin{array}{c}                     d_{i, j}                 \end{array}                 \right)             \end{array}         \]</span>         Allora è possibile scrivere un qualsiasi elemento <span class="math-tag">\( c_{i, j}\)</span> come         <span class="math-tag">\[             c_{i, j} = \sum_{r = 1}^{n}(a_{i, r} \cdot b_{r, j})         \]</span>         e un qualsiasi elemento <span class="math-tag">\( d_{i, j}\)</span> come          <span class="math-tag">\[             d_{i, j} = \sum_{r = 1}^{n}(b_{i, r} \cdot a_{r, j})         \]</span>         Scrivendo quindi in questo modo le tracce di <span class="math-tag">\( C\)</span> e <span class="math-tag">\( D\)</span> si avrà         <span class="math-tag">\begin{aligned}             & tr(C) = \sum_{i = 1}^{n}(c_{i, i}) = \sum_{i = 1}^{n}(\sum_{r = 1}^{n}(a_{i, r} \cdot b_{r, i}))             \\             & tr(D) = \sum_{i = 1}^{n}(d_{i, i}) = \sum_{i = 1}^{n}(\sum_{r = 1}^{n}(b_{i, r} \cdot a_{r, i}))         \end{aligned}</span>         Commutando prima il prodotto interno alla sommatoria della traccia di <span class="math-tag">\( D\)</span> e poi commutando l'ordine delle sommatorie (ovvero invertendo gli indici <span class="math-tag">\( i\)</span> e <span class="math-tag">\( r\)</span> grazie alla proprietà delle sommatorie), si ottiene         <span class="math-tag">\begin{aligned}             \sum_{i = 1}^{n}(\sum_{r = 1}^{n}(a_{i, r} \cdot b_{r, i})) = \sum_{i = 1}^{n}(\sum_{r = 1}^{n}(a_{i, r} \cdot b_{r, i}))         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem3-3" ><span class="demonstration-header environment-title">Dimostrazione 3.3 - Divisori dello zero nell'anello delle matrici quadrate</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         L'anello delle matrici quadrate ammette divisori dello zero.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando le matrici quadrate non nulle <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span><span class="math-tag">\[             A = B =              \left(             \begin{array}{cc}                 0 & 0 \\                 1 & 0             \end{array}             \right)         \]</span>         ed effettuando il prodotto <span class="math-tag">\( A \odot B\)</span> si          <span class="math-tag">\[             \left(             \begin{array}{cc}                 0 & 0 \\                 1 & 0              \end{array}             \right)             \odot             \left(             \begin{array}{cc}                 0 & 0 \\                 1 & 0             \end{array}             \right)             =             \left(             \begin{array}{cc}                 0 & 0 \\                 0 & 0             \end{array}             \right)         \]</span>         si ottiene la matrice nulla, dimostrando quindi la proposizione.     </div></div></div><div class="demonstration environment" id="dem3-4" ><span class="demonstration-header environment-title">Dimostrazione 3.4 - Unicità della matrice inversa</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Se la matrice inversa esiste, essa è unica.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare questa proposizione consideriamo una matrice quadrata <span class="math-tag">\( A\)</span> e ipotizziamo per assurdo che esistano due matrici inverse (<span class="math-tag">\( B\)</span>, <span class="math-tag">\( C\)</span>), allora l'espressione         <span class="math-tag">\[             B \odot A \odot C         \]</span>         per la proprietà associativa, può essere svolta in due modi, ovvero         <span class="math-tag">\begin{aligned}             & (B \odot A) \odot C = B \odot (A \odot C)             & \iff         \end{aligned}</span>         e, per definizione di matrice inversa, si ha che <span class="math-tag">\( B \odot A = I\)</span> e che <span class="math-tag">\( A \odot C = I\)</span>, quindi         <span class="math-tag">\begin{aligned}             & I \odot C = B \odot I             & \iff \\             & C = B             &         \end{aligned}</span>         da cui si è dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem3-5" ><span class="demonstration-header environment-title">Dimostrazione 3.5 - Inversa del prodotto <span class="math-tag">\( A \odot B\)</span> come prodotto di <span class="math-tag">\( B^{-1} \odot A^{-1}\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due matrici invertibili <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span> allora l'inverso del prodotto delle due matrici è il prodotto delle inverse commutate, ovvero         <span class="math-tag">\[             (A \odot B)^{-1} = B^{-1} \odot A^{-1}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare la proposizione è necessario dimostrare che l'inversa della matrice <span class="math-tag">\( A \odot B\)</span> (che sarebbe <span class="math-tag">\( (A \odot B)^{-1}\)</span>) è la matrice <span class="math-tag">\( B^{-1} \odot A^{-1}\)</span>.          <br ></br>         Per farlo dimostriamo la seguente espressione         <span class="math-tag">\begin{aligned}             & (A \odot B) \odot (B^{-1} \odot A^{-1}) = I             & \iff         \end{aligned}</span>         Per la proprietà associativa (e per definizione di matrice inversa), è quindi possibile scrivere         <span class="math-tag">\begin{aligned}             & A \odot (B \odot B^{-1}) \odot A^{-1} = I             & \iff \\             & A \odot I \odot A^{-1} = I             & \iff \\             & I = I         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem3-6" ><span class="demonstration-header environment-title">Dimostrazione 3.6 - Trasposta del prodotto <span class="math-tag">\( A \odot B\)</span> come prodotto di <span class="math-tag">\( {}^{t} \! B \odot {}^{t} \! A\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando una matrice <span class="math-tag">\( A \in M_{m \times k}(\mathbb{K})\)</span> e <span class="math-tag">\( B \in M_{k \times n}(\mathbb{K})\)</span> allora         <span class="math-tag">\[             {}^t \! (A \odot B) = {}^t \! B \odot {}^t \! A         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare la proposizione, consideriamo le seguenti matrici:         <ul ><li >la matrice <span class="math-tag">\( A\)</span>, di dimensione <span class="math-tag">\( m \times k\)</span><span class="math-tag">\[                 A                  =                 \left(                 \begin{array}{c}                     a_{i, j}                 \end{array}                 \right)                 =                 \left(                 \begin{array}{cccc}                     a_{1, 1} & a_{1, 2} & \cdots & a_{1, k} \\                     a_{2, 1} & a_{2, 2} & \cdots & a_{2, k} \\                     \vdots & \vdots & \ddots & \vdots \\                     a_{m, 1} & a_{m, 2} & \cdots & a_{m, k}                 \end{array}                 \right)             \]</span></li><li >la matrice <span class="math-tag">\( {}^{t} \! A\)</span>, di dimensione <span class="math-tag">\( k \times m\)</span><span class="math-tag">\[                 {}^{t} \! A                 =                 \left(                 \begin{array}{c}                     a_{j, i}                 \end{array}                 \right)                 =                 \left(                 \begin{array}{cccc}                     a_{1, 1} & a_{1, 2} & \cdots & a_{1, m} \\                     a_{2, 1} & a_{2, 2} & \cdots & a_{2, m} \\                     \vdots & \vdots & \ddots & \vdots \\                     a_{k, 1} & a_{k, 2} & \cdots & a_{k, m}                 \end{array}                 \right)             \]</span></li><li >la matrice <span class="math-tag">\( B\)</span>, di dimensione <span class="math-tag">\( k \times n\)</span><span class="math-tag">\[                 B                 =                 \left(                 \begin{array}{c}                     b_{i, j}                 \end{array}                 \right)                 =                 \left(                 \begin{array}{cccc}                     b_{1, 1} & b_{1, 2} & \cdots & b_{1, n} \\                     b_{2, 1} & b_{2, 2} & \cdots & b_{2, n} \\                     \vdots & \vdots & \ddots & \vdots \\                     b_{k, 1} & b_{k, 2} & \cdots & b_{k, n}                 \end{array}                 \right)             \]</span></li><li >la matrice <span class="math-tag">\( {}^{t} \! B\)</span>, di dimensione <span class="math-tag">\( n \times k\)</span><span class="math-tag">\[                 {}^{t} \! B                 =                 \left(                 \begin{array}{c}                     b_{j, i}                 \end{array}                 \right)                 =                 \left(                 \begin{array}{cccc}                     b_{1, 1} & b_{1, 2} & \cdots & b_{1, k} \\                     b_{2, 1} & b_{2, 2} & \cdots & b_{2, k} \\                     \vdots & \vdots & \ddots & \vdots \\                     b_{n, 1} & b_{n, 2} & \cdots & b_{n, k}                 \end{array}                 \right)             \]</span></li><li >la matrice <span class="math-tag">\( C = A \odot B\)</span>, di dimensione <span class="math-tag">\( m \times n\)</span><span class="math-tag">\[                 C                 =                 \left(                 \begin{array}{c}                     c_{i, j}                 \end{array}                 \right)                 =                 \left(                 \begin{array}{cccc}                     c_{1, 1} & c_{1, 2} & \cdots & c_{1, n} \\                     c_{2, 1} & c_{2, 2} & \cdots & c_{2, n} \\                     \vdots & \vdots & \ddots & \vdots \\                     c_{m, 1} & c_{m, 2} & \cdots & c_{m, n}                 \end{array}                 \right)             \]</span></li><li >la matrice <span class="math-tag">\( {}^{t} \! C = {}^{t} \! (A \odot B)\)</span>, di dimensione <span class="math-tag">\( n \times m\)</span><span class="math-tag">\[                 {}^{t} \! C                 =                 \left(                 \begin{array}{c}                     c_{j, i}                 \end{array}                 \right)                 =                 \left(                 \begin{array}{cccc}                     c_{1, 1} & c_{1, 2} & \cdots & c_{1, m} \\                     c_{2, 1} & c_{2, 2} & \cdots & c_{2, m} \\                     \vdots & \vdots & \ddots & \vdots \\                     c_{n, 1} & c_{n, 2} & \cdots & c_{n, m}                 \end{array}                 \right)             \]</span></li><li >la matrice <span class="math-tag">\( D = {}^{t} \! B \odot {}^{t} \! A\)</span>, di dimensione <span class="math-tag">\( n \times m\)</span><span class="math-tag">\[                 D                 =                 \left(                 \begin{array}{c}                     d_{i, j}                 \end{array}                 \right)                 =                 \left(                 \begin{array}{cccc}                     d_{1, 1} & d_{1, 2} & \cdots & d_{1, m} \\                     d_{2, 1} & d_{2, 2} & \cdots & d_{2, m} \\                     \vdots & \vdots & \ddots & \vdots \\                     d_{n, 1} & d_{n, 2} & \cdots & d_{n, m}                 \end{array}                 \right)             \]</span></li></ul>         si vuole quindi dimostrare che         <span class="math-tag">\[             \left(                 \begin{array}{c}                     c_{j, i}                 \end{array}                 \right)             =             \left(                 \begin{array}{c}                     d_{i, j}                 \end{array}             \right)         \]</span>         ovvero che ogni elemento <span class="math-tag">\( c_{j, i}\)</span> è uguale ad ogni elemento <span class="math-tag">\( d_{i, j}\)</span>.         <br ></br>         Detto ciò, possiamo scivere un generico elemento <span class="math-tag">\( c_{i, j}\)</span> come la seguente sommatoria         <span class="math-tag">\[             c_{i, j} = \sum_{r = 1}^{k} a_{i, r} \cdot b_{r, j}         \]</span>         mentre un generico elemento di <span class="math-tag">\( c_{j, i}\)</span> può essere scritto come (invertendo l'indice <span class="math-tag">\( i\)</span> con <span class="math-tag">\( j\)</span>)         <span class="math-tag">\[             c_{j, i} = \sum_{r = 1}^{k} a_{j, r} \cdot b_{r, i}         \]</span>         Scriviamo quindi un generico elemento di <span class="math-tag">\( D\)</span>, ovvero         <span class="math-tag">\[             d_{i, j} = \sum_{r = 1}^{k} b_{r, i} \cdot a_{j, r}         \]</span>         in quanto stiamo considerando le trasposte. Utilizzando quindi la proprietà commutativa interna alla sommatoria si ottiene         <span class="math-tag">\[             d_{i, j} = \sum_{r = 1}^{k} a_{j, r} \cdot b_{r, i} = c_{j, i}         \]</span>         che dimostra la proposizione.     </div></div></div></div><div class="subsection part" id="subsec3-3" ><span class="subsection-header part-title">3.3 - Le magiche operazioni riga</span><div class="demonstration environment" id="dem3-7" ><span class="demonstration-header environment-title">Dimostrazione 3.7 - Calcolo dell'inversa (se esiste) di una matrice con il metodo delle matrici affiancate</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando una matrice <span class="math-tag">\( A \in M_{m \times m}(\mathbb{K})\)</span>, per trovare la sua inversa (se esiste) è possibile affiancarla alla matrice identica <span class="math-tag">\( m \times m\)</span><span class="math-tag">\[             \begin{pmatrix}                 a & b & \left|\right. & 1 & 0 \\                 c & d & \left|\right. & 0 & 1             \end{pmatrix}         \]</span>         e effettuare le operazioni riga al fine di portare la matrice identica a sinistra: una volta fatto ciò, a destra si troverà l'inversa.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando due matrici quadrate <span class="math-tag">\( A, I \in M_{m \times m}(\mathbb{K})\)</span> è possibile considerare ogni operazione riga come la moltiplicazione per matrici identiche modificate (<span class="math-tag">\( E_{n-operazioni}\)</span>), ovvero         <span class="math-tag">\[             \left(             \begin{array}{ccc}                 A & \left|\right. & I              \end{array}             \right)             \quad             \xrightarrow[]{operazione-riga}             \quad             \left(             \begin{array}{ccc}                 E_{1} \odot A                  & \left|\right. &                  E_{1} \odot I              \end{array}             \right)         \]</span>         Dopo <span class="math-tag">\( n\)</span> operazioni (nel caso esista l'inversa) si avrà         <span class="math-tag">\[             \left(             \begin{array}{lcl}                 E_{n} \odot E_{n - 1} \odot \ldots \odot E_{1} \odot A                 & \left|\right. &                 E_{n} \odot E_{n - 1} \odot \ldots \odot E_{1} \odot I             \end{array}             \right)         \]</span>         e a sinistra la matrice identica         <span class="math-tag">\[             \left(             \begin{array}{lcl}                 I                 & \left|\right. &                 E_{n} \odot E_{n - 1} \odot \ldots \odot E_{1}             \end{array}             \right)         \]</span>         Stiamo dicendo che <span class="math-tag">\( (E_{n} \odot E_{n - 1} \odot \ldots \odot E_{1}) \odot (A) = I\)</span>, che è equivalente a dire che <span class="math-tag">\( (E_{n} \odot E_{n - 1} \odot \ldots \odot E_{1})\)</span> è l'inversa di <span class="math-tag">\( A\)</span>.           <br ></br>         Dato che si sono effettuate le medesime operazioni riga anche sulla matrice identica, si ha che la matrice <span class="math-tag">\( (E_{n} \odot E_{n - 1} \odot \ldots \odot E_{1})\)</span> è la matrice che si trova a destra.         <br ></br>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem3-8" ><span class="demonstration-header environment-title">Dimostrazione 3.8 - Immutabilità dello spazio delle righe tramite operazioni riga</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Le operazioni riga non cambiano lo spazio delle righe di una matrice.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario esaminare l'effetto di ogni operazione riga sullo spazio delle righe.         <br ></br>         Per farlo consideriamo quindi la matrice <span class="math-tag">\( A \in M_{m \times n}(\mathbb{K})\)</span><span class="math-tag">\[             A =              \left(             \begin{array}{c}                 R_{1} \\                 R_{2} \\                 \vdots \\                 R_{m}             \end{array}             \right)         \]</span>         e:         <ul ><li >aggiungendo ad una riga il multiplo di un'altra (ipotizziamo che sia <span class="math-tag">\( R_{2})\)</span>, ovvero             <span class="math-tag">\[                 \xrightarrow[]{R_{1} \leftarrow R_{1} + k \cdot R_{2}}                 \left(                 \begin{array}{c}                     R_{1} + k \cdot R_{2} \\                     R_{2} \\                     \vdots \\                     R_{m}                 \end{array}                 \right)             \]</span>             abbiamo che lo spazio delle righe è calcolato come             <span class="math-tag">\begin{aligned}                 & a_{1} \cdot (R_{1} + k \cdot R_{2}) + a_{2} \cdot R_{2} + \ \ldots \ + a_{m} \cdot R_{m}                  & = \\                 & a_{1} \cdot R_{1} + a_{1} \cdot k \cdot R_{2} + a_{2} \cdot R_{2} + \ \ldots \ + a_{m} \cdot R_{m}                 & = \\                 & a_{1} \cdot R_{1} + (a_{1} \cdot k + a_{2}) \cdot R_{2} + \ \ldots \ + a_{m} \cdot R_{m}             \end{aligned}</span>             quindi, lo spazio delle righe non cambia;             </li><li >scambiando una riga con un'altra lo spazio delle righe non cambia grazie alla proprietà commutativa della somma;             </li><li >moltiplicando una riga per uno scalare diverso da <span class="math-tag">\( 0\)</span>, ovvero             <span class="math-tag">\[                 \xrightarrow[]{R_{1} \leftarrow \alpha \cdot R_{1}}                 \left(                 \begin{array}{c}                     \alpha \cdot R_{1} \\                     R_{2} \\                     \vdots \\                     R_{m}                 \end{array}                 \right)                 \]</span>                 abbiamo che lo spazio delle righe è calcolato come                 <span class="math-tag">\begin{aligned}                     & a_{1} \cdot (\alpha \cdot R_{1}) + a_{2} \cdot R_{2} + \ \ldots \ + a_{m} \cdot R_{m}                      & = \\                     & (a_{1} \cdot \alpha) \cdot R_{1} + a_{2} \cdot R_{2} + \ \ldots \ + a_{m} \cdot R_{m}                      & = \\                 \end{aligned}</span>                 quindi, lo spazio delle righe non cambia.         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem3-9" ><span class="demonstration-header environment-title">Dimostrazione 3.9 - Rango per righe uguale al rango per colonne</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il rango per righe di una matrice è uguale al rango per colonne della stessa matrice, ovvero         <span class="math-tag">\[             r_{\mathcal{C}}(A) = r_{\mathcal{R}}(A)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando una matrice <span class="math-tag">\( A \in M_{m \times n}(\mathbb{K})\)</span>, è possibile ottenere la sua forma completamente ridotta tramite delle operazioni riga (che non modificano il rango per righe della matrice)         <span class="math-tag">\[             \left(             \begin{array}{cccc}                 1 & x & 0 & y \\                 0 & 0 & 1 & z \\                 0 & 0 & 0 & 0             \end{array}             \right)         \]</span>         Considerando quindi che <span class="math-tag">\( x, y\)</span> e <span class="math-tag">\( z\)</span> sono valori qualunque, questa è una generica matrice completamente ridotta: è possibile eliminare tali elementi sottraendo la colonna a sè stessa per <span class="math-tag">\( x\)</span> (<span class="math-tag">\( y\)</span> e <span class="math-tag">\( z\)</span>) volte.         <br ></br>         Applicando le giuste operazioni colonna è quindi possibile ottenere la seguente matrice         <span class="math-tag">\[             \left(             \begin{array}{cccc}                 1 & 0 & 0 & 0 \\                 0 & 0 & 1 & 0 \\                 0 & 0 & 0 & 0             \end{array}             \right)         \]</span>         e, invertendo le colonne in maniera adeguata, possiamo ottenere all'interno della matrice, una matrice identica <span class="math-tag">\( n \times n\)</span>.         <span class="math-tag">\[             \left(             \begin{array}{cccc}                 1 & 0 & 0 & 0 \\                 0 & 1 & 0 & 0 \\                 0 & 0 & 0 & 0             \end{array}             \right)         \]</span>         Ciò rende evidente che il rango per righe sia uguale al rango per colonne (ovvero <span class="math-tag">\( n\)</span>), in quanto le righe linearmente indipendenti sono esattamente il numero di colonne linearmente indipendenti.          <br ></br>         Si è quindi verificata la proposizione.     </div></div></div></div></div><div class="section part" id="sec4" ><span class="section-header part-title">4 - Sistemi lineari</span><div class="subsection part" id="subsec4-1" ><span class="subsection-header part-title">4.1 - Sistemi lineari come matrici</span><div class="demonstration environment" id="dem4-1" ><span class="demonstration-header environment-title">Dimostrazione 4.1 - Teorema di Rouchè-Capelli</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Un sistema lineare         <span class="math-tag">\[             A \odot X = B         \]</span>         è risolubile se, considerando la matrice completa <span class="math-tag">\( C\)</span><span class="math-tag">\[             C =              \left(             \begin{array}{ccc}                 A & \left|\right. & B              \end{array}             \right)         \]</span>         il rango della matrice <span class="math-tag">\( A\)</span> è uguale al rango della matrice completa <span class="math-tag">\( C\)</span><span class="math-tag">\[             r(A) = r(C)         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Siamo poi così diversi, io e te?</span>         Dire ciò equivale a dire che un sistema lineare è risolubile se e solo se la forma completamente ridotta della sua matrice associata non contiene pivot nell'ultima colonna.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò è sufficiente spiegare il contributo del pivot al rango della matrice. Infatti, i pivot sono i primi elementi non nulli che compaiono sulla riga. Nel caso comparisse un pivot nell'ultima colonna, significherebbe che l'ultima colonna non è combinazione lineare delle precedenti, ovvero che è linearmente indipendente (e quindi il rango della matrice <span class="math-tag">\( A\)</span> sarebbe minore del rango della matrice completa <span class="math-tag">\( C\)</span>).     </div></div></div><div class="demonstration environment" id="dem4-2" ><span class="demonstration-header environment-title">Dimostrazione 4.2 - Soluzioni del sistema come somma della soluzione del sistema lineare omogeneo associato e una soluzione particolare del sistema</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Le soluzioni (se esistono) di un generico sistema lineare          <span class="math-tag">\[             A \odot X = B         \]</span>         sono tutte e sole le <span class="math-tag">\( n\)</span>-uple che si possono scrivere nella forma         <span class="math-tag">\[             Y + \bar{X}         \]</span>         dove <span class="math-tag">\( Y\)</span> è una soluzione del sistema lineare omogeneo associato e <span class="math-tag">\( \bar{X}\)</span> è una soluzione particolare del sistema lineare.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò, consideriamo quindi <span class="math-tag">\( \bar{X}\)</span> come una delle soluzioni del sistema         <span class="math-tag">\[             A \odot X = B         \]</span>         , mentre con <span class="math-tag">\( H\)</span> una qualunque soluzione del sistema lineare.         <br ></br>         &Egrave; lecito quindi scrivere         <span class="math-tag">\begin{aligned}             & H = H - \bar{X} + \bar{X}             & \iff \\             & H = (H - \bar{X}) + \bar{X}         \end{aligned}</span>         Concentriamoci ora su <span class="math-tag">\( H - \bar{X}\)</span> e dimostriamo che sia una soluzione qualsiasi del sistema lineare omogeneo associato, ovvero si deve avere che         <span class="math-tag">\begin{aligned}             & A \odot (H - \bar{X}) = 0             & \iff         \end{aligned}</span>         Per la proprietà distributiva si ottiene         <span class="math-tag">\begin{aligned}             & A \odot H - A \odot \bar{X} = 0             & \iff         \end{aligned}</span>         e, dato che <span class="math-tag">\( H\)</span> è una qualunque soluzione del sistema lineare si ha che <span class="math-tag">\( A \odot H = B\)</span>. Inoltre anche <span class="math-tag">\( \bar{X}\)</span> è una soluzione del sistema lineare, per cui <span class="math-tag">\( A \odot \bar{X} = B\)</span>. Risolvendo le moltiplicazioni tra matrici si ottiene quindi che         <span class="math-tag">\begin{aligned}             & B - B = 0             & \iff \\             & 0 = 0             &         \end{aligned}</span>         Per cui, dato che <span class="math-tag">\( (H - \bar{X})\)</span> è una soluzione del sistema lineare omogeneo associato, è possibile denominarla con <span class="math-tag">\( Y\)</span> e riguardando l'espressione iniziale         <span class="math-tag">\begin{aligned}             & H = (H - \bar{X}) + \bar{X}             & \iff \\             & H = Y + \bar{X}         \end{aligned}</span>         ovvero si è scritta una qualsiasi soluzione del sistema lineare (<span class="math-tag">\( H\)</span>) come somma tra una soluzione del sistema lineare omogeneo associato (<span class="math-tag">\( Y\)</span>) e una soluzione particolare (<span class="math-tag">\( \bar{X}\)</span>).         <br ></br>         Si è quindi dimostrata la proposizione.     </div></div></div></div></div><div class="section part" id="sec5" ><span class="section-header part-title">5 - Applicazioni lineari</span><div class="subsection part" id="subsec5-1" ><span class="subsection-header part-title">5.1 - Applicazioni lineari come funzioni</span><div class="demonstration environment" id="dem5-1" ><span class="demonstration-header environment-title">Dimostrazione 5.1 - Immagine del vettore nullo rispetto ad un'applicazione lineare uguale al vettore nullo</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Se         <span class="math-tag">\[             f : V \to W         \]</span>         è un'applicazione lineare, allora l'immagine del vettore nullo di <span class="math-tag">\( V\)</span> è il vettore nullo di <span class="math-tag">\( W\)</span>, ovvero         <span class="math-tag">\[             f(0_{V}) = 0_{W}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare la proposizione consideriamo l'uguaglianza          <span class="math-tag">\begin{aligned}             & f(0_{V} + 0_{V}) = f(0_{V}) + f(0_{V})             & \iff         \end{aligned}</span>         che è valida per la linearità.         <br ></br>         Considerando quindi <span class="math-tag">\( x \in W\)</span> come l'opposto di <span class="math-tag">\( f(0_{V})\)</span>, aggiungiamolo ad entrambe le parti         <span class="math-tag">\begin{aligned}             & x + f(0_{V} + 0_{V}) = f(0_{V}) + f(0_{V}) + x             & \iff         \end{aligned}</span>         Ora, dato che <span class="math-tag">\( 0_{V} + 0_{V} = 0_{V}\)</span>, è possibile scrivere         <span class="math-tag">\begin{aligned}             & x + f(0_{V}) = f(0_{V}) + f(0_{V}) + x             & \iff         \end{aligned}</span>         e per l'associatività è lecito scrivere         <span class="math-tag">\begin{aligned}             & x + f(0_{V}) = f(0_{V}) + (f(0_{V}) + x)             & \iff         \end{aligned}</span>         Applicando quindi la definizione di opposto, si ha che <span class="math-tag">\( x + f(0_{V}) = 0_{W}\)</span> e quindi         <span class="math-tag">\begin{aligned}             & 0_{W} = f(0_{V}) + 0_{W}             & \iff         \end{aligned}</span>         e dato che qualsiasi elemento sommato all'elemento nullo è l'elemento stesso, si ha che         <span class="math-tag">\[             0_{W} = f(0_{V})         \]</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-2" ><span class="demonstration-header environment-title">Dimostrazione 5.2 - Teorema fondamentale delle applicazioni lineari</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due spazi vettoriali <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span>, una base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> per <span class="math-tag">\( V\)</span> e <span class="math-tag">\( n\)</span> vettori <span class="math-tag">\( w_{1}, \ \ldots \ , w_{n}\)</span> di <span class="math-tag">\( W\)</span>, allora esiste ed è unica un'applicazione lineare         <span class="math-tag">\[             f: V \to W         \]</span>         tale che          <span class="math-tag">\[             f(v_{1}) = w_{1}             \quad , \quad               \ldots              \quad , \quad             f(v_{n}) = w_{n}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, è necessario dimostrare:         <ul ><li >esiste tale applicazione lineare. Per dimostrarlo, consideriamo quindi un vettore <span class="math-tag">\( v \in V\)</span> che è combinazione lineare dei vettori di <span class="math-tag">\( B_{V}\)</span><span class="math-tag">\[                 v = a_{1} \cdot v_{1} + \ \ldots \ + a_{n} \cdot v_{n}             \]</span>             Significa quindi che ogni vettore è associato ad una <span class="math-tag">\( n\)</span>-upla di coordinate. Consideriamo quindi la seguente funzione             <span class="math-tag">\[                 f(v) = a_{1} \cdot w_{1} + \ \ldots \ + a_{n} \cdot w_{n}             \]</span>             dove <span class="math-tag">\( (a_{1}, \ \ldots \ , a_{n})\)</span> sono le coordinate di <span class="math-tag">\( v\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>. Inoltre, dato che <span class="math-tag">\( (a_{1}, \ \ldots \ , a_{n})\)</span> sono proprio le coordinate rispetto ai vettori della base, si ha che saranno del tipo             <span class="math-tag">\[                 f(v_{1}) = f(1 \cdot v_{1} + 0 \cdot v_{2} + \ \ldots \ + 0 \cdot v_{n}) = 1 \cdot w_{1}             \]</span>             che dimostra quindi l'esistenza della funzione richiesta.              <br ></br>             &Egrave; ora necessario che tale funzione sia un'applicazione lineare, ovvero che             <ul ><li ><span class="math-tag">\( f(v + u) = f(v) + f(u)\)</span>: per farlo consideriamo le immagini dei vettori, ovvero                 <span class="math-tag">\begin{aligned}                     & f(v) = \sum_{i = 1}^{n} a_{i} \cdot v_{i} \\                     & f(u) = \sum_{i = 1}^{n} b_{i} \cdot v_{i} \\                     & f(v + u) = \sum_{i = 1}^{n} a_{n} \cdot v_{1} + b_{n} \cdot v_{i}                 \end{aligned}</span>                 da cui, grazie alle proprietà delle sommatorie, è facile dimostrare la linearità della somma                 <span class="math-tag">\[                     \sum_{i = 1}^{n} a_{n} \cdot v_{1} + b_{n} \cdot v_{i} = \sum_{i = 1}^{n} a_{i} \cdot v_{i} + \sum_{i = 1}^{n} b_{i} \cdot v_{i}                 \]</span></li><li ><span class="math-tag">\( f(\alpha \cdot v) = \alpha \cdot f(v)\)</span>: per farlo consideriamo le immagini dei vettori, ovvero                 <span class="math-tag">\begin{aligned}                     & f(v) = \sum_{i = 1}^{n} a_{i} \cdot v_{i} \\                     & \alpha \cdot f(v) = \alpha \cdot \sum_{i = 1}^{n} a_{i} \cdot v_{i} \\                     & f(\alpha \cdot v) = \sum_{i = 1}^{n} \alpha \cdot a_{i} \cdot v_{i}                 \end{aligned}</span>                 da cui, grazie alla proprietà distributiva delle sommatorie, è facile dimostrare la linearità del prodotto per uno scalare                 <span class="math-tag">\[                     \sum_{i = 1}^{n} \alpha \cdot a_{i} \cdot v_{i} = \alpha \cdot \sum_{i = 1}^{n} a_{i} \cdot v_{i}                  \]</span></li></ul>             Si è quindi dimostrata la linearità di <span class="math-tag">\( f\)</span>;             </li><li >tale applicazione lineare è unica. Per dimostrarlo consideriamo quindi due applicazioni lineari <span class="math-tag">\( f_{1}\)</span> e <span class="math-tag">\( f_{2}\)</span> tali che             <span class="math-tag">\[                 f_{1}(v_{i}) = w_{i} = f_{2}(v_{i})             \]</span>             Dato che un qualsiasi vettore <span class="math-tag">\( v \in V\)</span> può essere scritto come combinazione lineare di <span class="math-tag">\( B_{V}\)</span> si ha che             <span class="math-tag">\begin{aligned}                 & f_{1}(v) = f_{1}(\sum_{i = 1}^{n} a_{i} \cdot v_{i})                 & \iff             \end{aligned}</span>             e per la linearità di <span class="math-tag">\( f_{1}\)</span> si ha che              <span class="math-tag">\begin{aligned}                 & f_{1}(\sum_{i = 1}^{n} a_{i} \cdot v_{i}) = \sum_{i = 1}^{n} a_{i} \cdot f_{1}(v_{i}) = \sum_{i = 1}^{n} a_{i} \cdot w_{i}                 &             \end{aligned}</span>             Ora, dato che la combinazione lineare di un vettore rispetto ad una base è unica, ripetendo l'analogo ragionamento si otterrebbero sempre i coefficienti <span class="math-tag">\( a_{i}\)</span>, e per questo motivo l'applicazione lineare è unica.         </li></ul>         Si è quindi dimostrato il teorema.     </div></div></div><div class="demonstration environment" id="dem5-3" ><span class="demonstration-header environment-title">Dimostrazione 5.3 - Nucleo di <span class="math-tag">\( f: V \to W\)</span> come sottospazio vettoriale di <span class="math-tag">\( V\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>, allora il nucleo di <span class="math-tag">\( f\)</span> (ovvero <span class="math-tag">\( ker(f)\)</span>) è un sottospazio vettoriale di <span class="math-tag">\( V\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che         <ul ><li >la somma di due vettori <span class="math-tag">\( v_{1}, v_{2} \in ker(f)\)</span> è sempre interna al nucleo di <span class="math-tag">\( f\)</span>, ovvero             <span class="math-tag">\[                 f(v_{1}) = 0_{W}                  \text{ e }                 f(v_{2}) = 0_{W}                  \quad                 \implies                 \quad                 f(v_{1} + v_{2}) = 0_{W}             \]</span>             Per dimostrarlo consideriamo quindi la linearità di <span class="math-tag">\( f\)</span><span class="math-tag">\begin{aligned}                 & f(v_{1} + v_{2}) = f(v_{1}) + f(v_{2})                  & \iff             \end{aligned}</span>             e sostituendoli con la loro immagine si ha che             <span class="math-tag">\begin{aligned}                 & f(v_{1}) + f(v_{2}) = 0_{W} + 0_{W} = 0_{W}                 &             \end{aligned}</span>             ovvero che la somma è interna;             </li><li >il prodotto per uno scalare è interno al nucleo di <span class="math-tag">\( f\)</span>, ovvero considerando un vettore <span class="math-tag">\( v \in V\)</span><span class="math-tag">\[                 f(v) = 0_{W}                 \qquad                 \implies                 \qquad                 f(\alpha \cdot v) = 0_{W}             \]</span>             Per dimostrarlo consideriamo quindi la linearità di <span class="math-tag">\( f\)</span><span class="math-tag">\begin{aligned}                 & f(\alpha \cdot v) = \alpha \cdot f(v)                 & \iff             \end{aligned}</span>             e sostituendo <span class="math-tag">\( v\)</span> con la sua immagine si ha che             <span class="math-tag">\begin{aligned}                 & \alpha \cdot f(v)= \alpha \cdot 0_{W} = 0_{W}                 &             \end{aligned}</span>             ovvero che il prodotto per uno scalare è interno.         </li></ul>         Si è quindi dimostrato che il nucleo di <span class="math-tag">\( f\)</span> è un sottospazio vettoriale di <span class="math-tag">\( V\)</span>.     </div></div></div><div class="demonstration environment" id="dem5-4" ><span class="demonstration-header environment-title">Dimostrazione 5.4 - Immagine di <span class="math-tag">\( f: V \to W\)</span> come sottospazio vettoriale di <span class="math-tag">\( W\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Data un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>, allora l'immagine di <span class="math-tag">\( f\)</span> (ovvero <span class="math-tag">\( Im(f)\)</span>) è un sottospazio vettoriale di <span class="math-tag">\( W\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che         <ul ><li >la somma di due vettori <span class="math-tag">\( w_{1}, w_{2} \in Im(f)\)</span> è sempre interna all'immagine di <span class="math-tag">\( f\)</span>.             <br ></br>             Per dimostrarlo consideriamo che da definizione <span class="math-tag">\( w_{1} = f(v_{1})\)</span> e che <span class="math-tag">\( w_{2} = f(v_{2})\)</span>, quindi             <span class="math-tag">\begin{aligned}                 & w_{1} + w_{2} = f(v_{1}) + f(v_{2})                 & \iff             \end{aligned}</span>             quindi, per linearità di <span class="math-tag">\( f\)</span> si ha che             <span class="math-tag">\begin{aligned}                 & f(v_{1}) + f(v_{2}) = f(v_{1} + v_{2})                 &             \end{aligned}</span>             ovvero che il vettore somma è ancora un vettore appartenente all'immagine;             </li><li >il prodotto per uno scalare è interno all'immagine di <span class="math-tag">\( f\)</span>.             <br ></br>             Per dimostrarlo consideriamo quindi un vettore <span class="math-tag">\( w \in Im(f)\)</span> e consideriamo che tale vettore è per definizione <span class="math-tag">\( w = f(v)\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & \alpha \cdot w = \alpha \cdot f(v)                 & \iff             \end{aligned}</span>             e per linearità di <span class="math-tag">\( f\)</span> si ha che             <span class="math-tag">\begin{aligned}                 & \alpha \cdot f(v) = f(\alpha \cdot v)                 &             \end{aligned}</span>             ovvero che il prodotto per uno scalare è interno.         </li></ul>         Si è quindi dimostrato che l'immagine di <span class="math-tag">\( f\)</span> è un sottospazio vettoriale di <span class="math-tag">\( W\)</span>.     </div></div></div><div class="demonstration environment" id="dem5-5" ><span class="demonstration-header environment-title">Dimostrazione 5.5 - Nucleo di un'applicazione lineare iniettiva composto solo dal vettore nullo</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f : V \to W\)</span>, <span class="math-tag">\( f\)</span>, essa è iniettiva se e solo se il suo nucleo è composto solamente dal vettore nullo, ovvero         <span class="math-tag">\[             ker(f) = \{ 0_{V} \}         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Presenza del vettore nullo</span>         &Egrave; necessario notare che il vettore nullo è sempre presente nel nucleo, in quanto l'immagine del vettore nullo attraverso un'applicazione lineare è sempre il vettore nullo.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che:         <ul ><li >"Se <span class="math-tag">\( f\)</span> è iniettiva, allora il nucleo di <span class="math-tag">\( f\)</span> sarà composto solo dal vettore nullo", infatti considerando per assurdo un vettore <span class="math-tag">\( v \in ker(f)\)</span> non nullo (<span class="math-tag">\( v \neq 0_{V}\)</span>), otterremmo             <span class="math-tag">\[                 f(v) = 0_{W} = f(0_{V})             \]</span>             che renderebbe <span class="math-tag">\( f\)</span> non iniettiva in quanto il vettore nullo <span class="math-tag">\( 0_{W}\)</span> sarebbe immagine di due vettori distinti.             </li><li >"Se nel nucleo di <span class="math-tag">\( f\)</span> è presente solo il vettore nullo, allora <span class="math-tag">\( f\)</span> è iniettiva". Consideriamo per assurdo che due vettori <span class="math-tag">\( v_{1}, v_{2} \in V\)</span> abbiano la stessa immagine, ovvero             <span class="math-tag">\begin{aligned}                 f(v_{1}) = f(v_{2})             \end{aligned}</span>             &Egrave; quindi possibile aggiungere l'opposto di <span class="math-tag">\( f(v_{2})\)</span> da entrambi i lati e, utilizzando la linearità di <span class="math-tag">\( f\)</span>, scrivere              <span class="math-tag">\begin{aligned}                 & f(v_{1}) - f(v_{2}) = 0_{W}                 & \iff \\                 & f(v_{1} - v_{2}) = 0_{W}                 &             \end{aligned}</span>             Tale uguaglianza ci permette di dire che il vettore <span class="math-tag">\( v_{1} - v_{2}\)</span> è un vettore del nucleo e, dato che per ipotesi nel nucleo è presente solo il vettore nullo, possiamo dire che quel vettore è proprio il vettore nullo <span class="math-tag">\( 0_{V}\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & v_{1} - v_{2} = 0_{V}                  & \iff             \end{aligned}</span>             Ora, "portando a destra" <span class="math-tag">\( v_{2}\)</span><span class="math-tag">\begin{aligned}                 & v_{1} = v_{2}                 &             \end{aligned}</span>             si ha che se due elementi hanno la stessa immagine (e nel nucleo è presente il solo vettore nullo), allora tali elementi sono uguali (che è la definizione di iniettività).          </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-6" ><span class="demonstration-header environment-title">Dimostrazione 5.6 - Equazione dimensionale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f : V \to W\)</span>, si ha che la dimensione di <span class="math-tag">\( V\)</span> è uguale alla somma delle dimensioni dell'immagine e del nucleo di <span class="math-tag">\( f\)</span>, ovvero         <span class="math-tag">\[             dim(V) = dim(Im(f)) + dim(ker(f))         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - "Non posso né scendere né salire"</span>         Ciò significa che fissata la dimensione di <span class="math-tag">\( V\)</span>, nel caso cresca la dimensione dell'immagine di <span class="math-tag">\( f\)</span>, la dimensione del nucleo dovrà diminuire (e viceversa). Tutto ciò deve avvenire ricordandosi che tali valori devono essere positivi o nulli.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, consideriamo <span class="math-tag">\( n = dim(V)\)</span> e una base per il nucleo di <span class="math-tag">\( f\)</span><span class="math-tag">\( B_{ker}\)</span><span class="math-tag">\[             B_{ker} = (v_{1}, \ \ldots \ , v_{r})         \]</span>         composta da <span class="math-tag">\( r\)</span> vettori.         <br ></br>         Applichiamo il teorema del completamento ad una base per ottenere una base <span class="math-tag">\( B_{V}\)</span> per <span class="math-tag">\( V\)</span>, ovvero         <span class="math-tag">\[             B_{V} = (v_{1}, \ \ldots \ , v_{r}, v_{r + 1}, \ \ldots \ , v_{n})         \]</span>         composta da <span class="math-tag">\( n\)</span> vettori.         <br ></br>         Consideriamo ora le immagini dei vettori aggiunti, ovvero          <span class="math-tag">\[             f(v_{r + 1})             \quad , \quad             \ldots              \quad , \quad              f(v_{n})         \]</span>         di cui è possibile dire che sono una base dell'immagine di <span class="math-tag">\( f\)</span>, in quanto:         <ul ><li >sono un sistema di generatori per <span class="math-tag">\( Im(f)\)</span>. Infatti, considerando un qualsiasi vettore <span class="math-tag">\( f(v) \in Im(f)\)</span>, è possibile scrivere <span class="math-tag">\( v\)</span> come combinazione lineare di <span class="math-tag">\( B_{V}\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & f(v) = f(x_{1} \cdot v_{1} + \ \ldots \ + x_{r} \cdot v_{r} + x_{r + 1} \cdot v_{r + 1} + \ \ldots \ + x_{n} \cdot v_{n})                 & \iff             \end{aligned}</span>              Ora, per la linearità di <span class="math-tag">\( f\)</span> è possibile scrivere l'immagine di un generico vettore <span class="math-tag">\( v\)</span> come              <span class="math-tag">\begin{aligned}                 & x_{1} \cdot f(v_{1}) + \ \ldots \ + x_{r} \cdot f(v_{r}) + x_{r + 1} \cdot f(v_{r + 1}) + \ \ldots \ + x_{n} \cdot f(v_{n})                 & \iff             \end{aligned}</span>             Ricordando che i vettori <span class="math-tag">\( (v_{1}, \ \ldots \ , v_{r})\)</span> sono elementi del nucleo (e quindi la loro immagine è nulla), è possibile scrivere             <span class="math-tag">\begin{aligned}                 & f(v) = x_{r + 1} \cdot f(v_{r + 1}) + \ \ldots \ + x_{n} \cdot f(v_{n})                 &             \end{aligned}</span>             che dimostra il fatto tali vettori siano un sistema di generatori per <span class="math-tag">\( Im(f)\)</span>.             </li><li >sono vettori linearmente indipendenti. Per dimostrare ciò supponiamo             <span class="math-tag">\begin{aligned}                 & a_{r+1} \cdot f(v_{r + 1}) + \ \ldots \ + a_{n} \cdot f(v_{n}) = 0_{W}                 & \iff             \end{aligned}</span>             Grazie alla linearità di <span class="math-tag">\( f\)</span>, è possibile scrivere             <span class="math-tag">\begin{aligned}                 & f(a_{r + 1} \cdot v_{r + 1} + \ \ldots \ + a_{n} \cdot v_{n}) = 0_{W}                 &              \end{aligned}</span>             che ci permette di dire che il vettore <span class="math-tag">\( a_{r + 1} \cdot v_{r + 1} + \ \ldots \ + a_{n} \cdot v_{n}\)</span> appartiene al nucleo di <span class="math-tag">\( f\)</span> (in quanto la sua immagine è il vettore nullo). Tale vettore può quindi essere rappresentato come combinazione lineare di <span class="math-tag">\( B_{ker}\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & a_{r + 1} \cdot v_{r + 1} + \ \ldots \ + a_{n} \cdot v_{n} = a_{1} \cdot v_{1} + \ \ldots \ + a_{r} \cdot v_{r}                 & \iff             \end{aligned}</span>             e aggiungendo da entrambi lati l'opposto della combinazione lineare di <span class="math-tag">\( B_{ker}\)</span> si ottiene             <span class="math-tag">\begin{aligned}                - a_{1} \cdot v_{1} - \ \ldots \ - a_{r} \cdot v_{r} + a_{r + 1} \cdot v_{r + 1} + \ \ldots \ + a_{n} \cdot v_{n} = 0_{V}                 & \iff             \end{aligned}</span>             i cui coefficienti possono essere solo nulli, in quanto l'ipotesi iniziale era che tali vettori fossero linearmente indipendenti (in quanto sono i vettori della base <span class="math-tag">\( B_{V}\)</span>).              <br ></br>             Si è dimostrata quindi la lineare indipendenza.         </li></ul>        Dunque si è dimostrato che i vettori <span class="math-tag">\( (f(v_{r + 1}), \ \ldots \ , f(v_{n}))\)</span> sono una base per <span class="math-tag">\( Im(f)\)</span>: ciò implica che la sua dimensione sia        <span class="math-tag">\[             dim(Im(f)) = n - r        \]</span>        Ricordando che la dimensione di <span class="math-tag">\( B_{ker} = r\)</span>, è ora semplice dimostrare che la dimensione di <span class="math-tag">\( V\)</span> sia <span class="math-tag">\( n\)</span> (come da ipotesi):        <span class="math-tag">\[             \begin{array}{ccccc}                 dim(V) & = & dim(Im(f)) & + & dim(ker(f)) \\                 n & = & (n - r) & + & r             \end{array}        \]</span>        ovvero,        <span class="math-tag">\[             n = n        \]</span>        che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-7" ><span class="demonstration-header environment-title">Dimostrazione 5.7 - Iniettività e suriettività di un'applicazione lineare nel caso la dimensione degli spazi vettoriali sia uguale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due spazi vettoriali <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span> della stessa dimensione, ovvero         <span class="math-tag">\[             dim(V) = dim(W)         \]</span>         e un'applicazione lineare <span class="math-tag">\( f : V \to W\)</span> allora <span class="math-tag">\( f\)</span> è iniettiva se e solo se è suriettiva.      </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che, considerando la dimensione di <span class="math-tag">\( V\)</span> uguale alla dimensione di <span class="math-tag">\( W\)</span>":         <ul ><li >"se <span class="math-tag">\( f\)</span> è iniettiva allora è anche suriettiva".              <br ></br>             Per dimsotrare ciò si consideri l'equazione dimensionale, ovvero             <span class="math-tag">\[                 dim(V) = dim(Im(f)) + dim(ker(f))             \]</span>             e, ricordando che il nucleo delle applicazioni lineari iniettive è composto solo dal vettore nullo (ovvero che la dimensione del nucleo è <span class="math-tag">\( 0\)</span>)             <span class="math-tag">\[                 dim(ker(f)) = 0             \]</span>             è semplice ottenere che la dimensione di <span class="math-tag">\( V\)</span> è uguale alla dimensione dell'immagine di <span class="math-tag">\( f\)</span><span class="math-tag">\[                 dim(V) = dim(Im(f)) + 0             \]</span>             e, dato che la dimensione di <span class="math-tag">\( V\)</span> è uguale alla dimensione di <span class="math-tag">\( W\)</span>, si ottiene la definizione di suriettività, ovvero             <span class="math-tag">\[                 dim(Im(f)) = dim(W)             \]</span>             che dimostra la prima parte della proposizione.             </li><li >"se <span class="math-tag">\( f\)</span> è suriettiva allora è anche iniettiva". Per dimostrare consideriamo l'equazione dimensionale, ovvero             <span class="math-tag">\[                 dim(V) = dim(Im(f)) + dim(ker(f))             \]</span>             e, ricordando la definizione di suriettività, ovvero             <span class="math-tag">\[                 Im(f) = W             \]</span>             è semplice ottenere (per il fatto che la dimensione di <span class="math-tag">\( V\)</span> sia uguale alla dimensione di <span class="math-tag">\( W\)</span>) che la dimensione del nucleo è <span class="math-tag">\( 0\)</span>, ovvero             <span class="math-tag">\[                 dim(ker(f)) = 0             \]</span>             Provando ciò, si è dimostrata l'iniettività grazie al fatto che se il nucleo di <span class="math-tag">\( f\)</span> è nullo, allora <span class="math-tag">\( f\)</span> è iniettiva. Ciò dimostra la seconda implicazione.         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-8" ><span class="demonstration-header environment-title">Dimostrazione 5.8 - Suriettività di <span class="math-tag">\( f: V \to W\)</span> e relazione tra le dimensioni di <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f : V \to W\)</span> suriettiva, allora si ha che la dimensione di <span class="math-tag">\( V\)</span> è maggiore o uguale alla dimensione di <span class="math-tag">\( W\)</span>, ovvero         <span class="math-tag">\[             dim(V) \geq dim(W)         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Altra prospettiva</span>         Questa proposizione implica che un'applicazione lineare non può essere suriettiva se la dimensione di <span class="math-tag">\( V\)</span> è minore della dimensione di <span class="math-tag">\( W\)</span>,     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando la definizione di suriettività         <span class="math-tag">\[             Im(f) = W         \]</span>         e l'equazione dimensionale, si ha che         <span class="math-tag">\[             \begin{array}{ccccc}                 dim(V) & = & dim(Im(f)) & + & dim(ker(f)) \\                 dim(V) & = & dim(W) & + & dim(ker(f))             \end{array}         \]</span>         che dimostra la proposizione, in quanto la dimensione del nucleo di <span class="math-tag">\( f\)</span> è al minimo nulla.     </div></div></div><div class="demonstration environment" id="dem5-9" ><span class="demonstration-header environment-title">Dimostrazione 5.9 - Iniettività di <span class="math-tag">\( f: V \to W\)</span> e relazione tra le dimensioni di <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f : V \to W\)</span>, iniettiva, allora si ha che la dimensione di <span class="math-tag">\( V\)</span> è minore o uguale alla dimensione di <span class="math-tag">\( W\)</span>, ovvero         <span class="math-tag">\[             dim(V) \leq dim(W)         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Altra prospettiva</span>         Questa proposizione implica che un'applicazione lineare non può essere iniettiva se la dimensione di <span class="math-tag">\( V\)</span> è maggiore della dimensione di <span class="math-tag">\( W\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando che il nucleo di un'applicazione lineare iniettiva contiene solo il vettore nullo, si ha che         <span class="math-tag">\[             dim(ker(f)) = 0         \]</span>         e considerando l'equazione dimensionale si ha che         <span class="math-tag">\[             \begin{array}{ccccc}                 dim(V) & = & dim(Im(f)) & + & dim(ker(f)) \\                 dim(V) & = & dim(Im(f)) & + & 0             \end{array}         \]</span>         Dato che l'immagine di <span class="math-tag">\( f\)</span> è inclusa in <span class="math-tag">\( W\)</span>, ovvero         <span class="math-tag">\[             dim(Im(f)) \leq dim(W)         \]</span>         si ha che          <span class="math-tag">\[             dim(V) \leq dim(W)         \]</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-10" ><span class="demonstration-header environment-title">Dimostrazione 5.10 - Funzione inversa di un isomorfismo come isomorfismo</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un isomorfismo <span class="math-tag">\( f: V \to W\)</span>, allora anche l'applicazione lineare          <span class="math-tag">\[             f^{-1} : W \rightarrow V         \]</span>         è un isomorfismo.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare che <span class="math-tag">\( f^{-1}\)</span> è un isomorfismo è sufficiente dimostrare la sua linearità (infatti la suriettività e l'iniettività sono già dimostrate in quanto è una funzione inversa). Per dimostrarlo è necessario dimostrare che         <ul ><li >la somma è lineare, ovvero che             <span class="math-tag">\[                 f^{-1}(w_{1} + w_{2}) = f^{-1}(w_{1}) + f^{-1}(w_{2})             \]</span>             Consideriamo quindi che ogni vettore <span class="math-tag">\( w_{1}, w_{2} \in W\)</span> è immagine un distinto vettore <span class="math-tag">\( v_{1}, v_{2} \in V\)</span> attraverso <span class="math-tag">\( f\)</span>, ovvero             <span class="math-tag">\begin{aligned}                & f^{-1}(w_{1} + w_{2}) = f^{-1}(f(v_{1}) + f(v_{2}))                & \iff             \end{aligned}</span>             e per la linearità di <span class="math-tag">\( f\)</span>, si ha che             <span class="math-tag">\begin{aligned}                & f^{-1}(w_{1} + w_{2}) = f^{-1}(f(v_{1} + v_{2}))                & \iff             \end{aligned}</span>             e dato che la composizione con l'inversa annulla la funzione si ha che             <span class="math-tag">\begin{aligned}                & f^{-1}(w_{1} + w_{2}) = v_{1} + v_{2}                & \iff             \end{aligned}</span>             Infine, ricordando che <span class="math-tag">\( v_{1} = f^{-1}(w_{1})\)</span> si ottiene             <span class="math-tag">\begin{aligned}                & f^{-1}(w_{1} + w_{2}) = f^{-1}(w_{1}) + f^{-1}(w_{2})                &             \end{aligned}</span>             che dimostra la proprietà;             </li><li >il prodotto per uno scalare è lineare, ovvero che             <span class="math-tag">\[                 f^{-1}(\alpha \cdot w) = \alpha \cdot f^{-1}(w)              \]</span>             Consideriamo quindi che ogni vettore <span class="math-tag">\( w \in W\)</span> è immagine un distinto vettore <span class="math-tag">\( v \in V\)</span> attraverso <span class="math-tag">\( f\)</span>, ovvero             <span class="math-tag">\begin{aligned}                & f^{-1}(\alpha \cdot w) = f^{-1}(\alpha \cdot f(v))                 & \iff             \end{aligned}</span>             e per la linearità di <span class="math-tag">\( f\)</span>, si ha che             <span class="math-tag">\begin{aligned}                & f^{-1}(\alpha \cdot w) = f^{-1}(f(\alpha \cdot v))                & \iff             \end{aligned}</span>             e dato che la composizione con l'inversa annulla la funzione si ha che             <span class="math-tag">\begin{aligned}                & f^{-1}(\alpha \cdot w) = \alpha \cdot v                & \iff             \end{aligned}</span>             Infine, ricordando che <span class="math-tag">\( v = f^{-1}(w)\)</span> si ottiene             <span class="math-tag">\begin{aligned}                & f^{-1}(\alpha \cdot w) = \alpha \cdot f^{-1}(w)                &             \end{aligned}</span>             che dimostra la proprietà;         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-11" ><span class="demonstration-header environment-title">Dimostrazione 5.11 - Composizione di applicazioni lineari come applicazione lineare</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due applicazioni lineari <span class="math-tag">\( f: V \to U\)</span> e <span class="math-tag">\( g: U \to W\)</span>  allora anche la loro composizione, ovvero         <span class="math-tag">\[             g \circ f: V \to W         \]</span>         è lineare.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione considerando <span class="math-tag">\( v_{1}, v_{2} \in V\)</span> e la funzione <span class="math-tag">\( h = g \circ f\)</span>, è necessario dimostrare la seguente proprietà         <span class="math-tag">\[             h(\alpha \cdot v_{1} + \beta \cdot v_{2}) = \alpha \cdot h(v_{1}) + \beta \cdot h(v_{2}         \]</span>         Innanzitutto è possibile riscrivere la funzione composta <span class="math-tag">\( h\)</span> come         <span class="math-tag">\begin{aligned}            & h(\alpha \cdot v_{1} + \beta \cdot v_{2}) = g(f(\alpha \cdot v_{1} + \beta \cdot v_{2}))            & \iff         \end{aligned}</span>         e per la linearità di <span class="math-tag">\( f\)</span>, si può scrivere         <span class="math-tag">\begin{aligned}             & g(f(\alpha \cdot v_{1} + \beta \cdot v_{2})) = g(\alpha \cdot f(v_{1}) + \beta \cdot f(v_{2}))             & \iff         \end{aligned}</span>         mentre per la linearità di <span class="math-tag">\( g\)</span>, si può scrivere         <span class="math-tag">\begin{aligned}             & g(\alpha \cdot f(v_{1}) + \beta \cdot f(v_{2})) = \alpha \cdot g(f(v_{1}) + \beta \cdot g(f(v_{2}))             & \iff         \end{aligned}</span>         che è equivalente a         <span class="math-tag">\begin{aligned}             & \alpha \cdot h(v_{1}) + \beta \cdot h(v_{2})         \end{aligned}</span>         che dimostra la proprietà.     </div></div></div><div class="demonstration environment" id="dem5-12" ><span class="demonstration-header environment-title">Dimostrazione 5.12 - Corollario - Composizioni di isomorfismi</span>     Dato il corollario     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due isomorfismi <span class="math-tag">\( f: V \to U\)</span> e <span class="math-tag">\( g: U \to W\)</span>  allora anche la loro composizione, ovvero         <span class="math-tag">\[             g \circ f: V \to W         \]</span>         è un isomorfismo.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò è sufficiente ricordare che la composizione di applicazioni lineari è anch'essa una funzione lineare e che la composizione di due funzioni biunivoche è anch'essa una funzione biunivoca.     </div></div></div><div class="demonstration environment" id="dem5-13" ><span class="demonstration-header environment-title">Dimostrazione 5.13 - Isomorfismo associato alla <span class="math-tag">\( n\)</span>-upla delle coordinate</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando uno spazio vettoriale <span class="math-tag">\( V\)</span> di dimensione <span class="math-tag">\( n\)</span> e una base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> per <span class="math-tag">\( V\)</span>, allora è sempre possibile costruire un isomorfismo         <span class="math-tag">\[             f : V \to \mathbb{R}^{n}         \]</span>         tale che         <span class="math-tag">\[             f(v) = (x_{1}, \ \ldots \ , x_{n})         \]</span>         dove <span class="math-tag">\( x_{i}\)</span> sono le coordinate di <span class="math-tag">\( v\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>.     </div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Spazi vettoriali di dimensione <span class="math-tag">\( n\)</span> isomorfi a <span class="math-tag">\( \mathbb{R}^{n}\)</span></span>         Dato che tale isomorfismo esiste, significa che ogni spazio vettoriale <span class="math-tag">\( V\)</span> di dimensione <span class="math-tag">\( n\)</span> è isomorfo a <span class="math-tag">\( \mathbb{R}^{n}\)</span>, ovvero         <span class="math-tag">\[             n = dim(V)              \qquad             \implies             \qquad             V \cong \mathbb{R}^{n}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario provare che         <ul ><li >tale funzione è lineare, ovvero considerando <span class="math-tag">\( u, w \in V\)</span><span class="math-tag">\[                 f(\alpha \cdot u + \beta \cdot w) = \alpha \cdot f(u) + \beta \cdot f(w)             \]</span>             Per farlo, si parterà dalle due espressioni separatamente per poi raggiungere lo stesso risultato.              <br ></br>             Consideriamo quindi             <span class="math-tag">\begin{aligned}                 & f(\alpha \cdot u + \beta \cdot w)                  & \iff             \end{aligned}</span>             e che <span class="math-tag">\( u, w\)</span> possono essere scritti come combinazione lineare di <span class="math-tag">\( B_{V}\)</span>, ovvero             <span class="math-tag">\begin{aligned}                 & f(\alpha \cdot \sum_{i = 1}^{n} a_{i} \cdot v_{i} + \beta \cdot \sum_{i = 1}^{n} b_{i} \cdot v_{i})                 & \iff             \end{aligned}</span>             e, per le proprietà delle sommatorie,             <span class="math-tag">\begin{aligned}                 & f(\sum_{i = 1}^{n} (\alpha \cdot a_{i} + \beta \cdot b_{i}) \cdot v_{i})                 & \iff             \end{aligned}</span>             da cui si ottiene la seguente <span class="math-tag">\( n\)</span>-upla di coordinate             <span class="math-tag">\[                 (\alpha \cdot a_{1} + \beta \cdot b_{1}, \ \ldots \ , \alpha \cdot a_{n} + \beta \cdot b_{n})             \]</span>             Ora, considerando l'espressione a destra             <span class="math-tag">\begin{aligned}                 & \alpha \cdot f(u) + \beta \cdot f(w)                 & \iff             \end{aligned}</span>             è possibile applicare l'immagine dei vettori             <span class="math-tag">\begin{aligned}                 & = \alpha \cdot (a_{1}, \ \ldots \ , a_{n}) + \beta \cdot (b_{1}, \ \ldots \ , b_{n})                 & \iff             \end{aligned}</span>             da cui si ottiene la seguente <span class="math-tag">\( n\)</span>-upla             <span class="math-tag">\[                 (\alpha \cdot a_{1} + \beta \cdot b_{1}, \ \ldots \ , \alpha \cdot a_{n} + \beta \cdot b_{n})             \]</span>             Si è quindi dimostrata la linearità.             </li><li >tale funzione è biunivoca. Per dimostrarlo è sufficiente considerare per la suriettività che <span class="math-tag">\( B_{V}\)</span> è un sistema di generatori (ovvero che ogni vettori di <span class="math-tag">\( V\)</span> è combinazione lineare di <span class="math-tag">\( B_{V}\)</span>) e per l'iniettività che ogni vettore corrisponde ad un'unica <span class="math-tag">\( n\)</span>-upla di coordinate rispetto ad una base;         </li></ul>         Si è  quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-14" ><span class="demonstration-header environment-title">Dimostrazione 5.14 - Spazi vettoriali della stessa dimensione isomorfi</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due spazi vettoriali <span class="math-tag">\( V\)</span> e <span class="math-tag">\( W\)</span>, questi hanno la stessa dimensione se e solo se sono isomorfi.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario dimostrare che:          <ul ><li >"se due spazi vettoriali hanno la stessa dimensione allora sono isomorfi". Considerando <span class="math-tag">\( n\)</span> la dimensione di tali spazi, si ha che <span class="math-tag">\( V\)</span> è isomorfo a <span class="math-tag">\( \mathbb{R}^{n}\)</span> e che anche <span class="math-tag">\( W\)</span> è isomorfo a <span class="math-tag">\( \mathbb{R}^{n}\)</span>, per cui esisteranno le seguenti applicazioni lineari             <span class="math-tag">\begin{aligned}                 & V \cong \mathbb{R}^{n}                 \qquad                 \implies                 \qquad                 f: V \to \mathbb{R}^{n}                 & \\                 & W \cong \mathbb{R}^{n}                 \qquad                 \implies                 \qquad                 g: W \to \mathbb{R}^{n}                 &             \end{aligned}</span>             da cui è possibile dire che esiste l'isomorfismo <span class="math-tag">\( g^{-1}: \mathbb{R}^{n} \to W\)</span> e che esisterà l'isomorfismo <span class="math-tag">\( h = g^{-1} \circ f\)</span><span class="math-tag">\[                 h: V \to W             \]</span>             per cui è possibile dire che <span class="math-tag">\( V\)</span> è isomorfo a <span class="math-tag">\( W\)</span>.             <br ></br>             Si è quindi dimostrata l'implicazione.             </li><li >"se due spazi vettoriali sono isomorfi allora hanno la stessa dimensione". Per farlo, consideriamo l'equazioni dimensionale di <span class="math-tag">\( f: V \to W\)</span> e, consci del fatto che <span class="math-tag">\( f\)</span> è biettiva (ovvero suriettiva e iniettiva) si ha che             <span class="math-tag">\[                 \begin{array}{ccccc}                     dim(V) & = & dim(ker(f)) & + & dim(Im(f))  \\                     dim(V) & = & 0 & + & dim(W)                 \end{array}             \]</span>             in quanto <span class="math-tag">\( dim(ker(f)) = 0\)</span> perchè <span class="math-tag">\( f\)</span> è iniettiva e <span class="math-tag">\( dim(Im(f)) = dim(W)\)</span> perchè <span class="math-tag">\( f\)</span> è suriettiva.             <br ></br>             Si è quindi dimostrata l'implicazione.         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div></div><div class="subsection part" id="subsec5-2" ><span class="subsection-header part-title">5.2 - Applicazioni lineari come matrici</span><div class="demonstration environment" id="dem5-15" ><span class="demonstration-header environment-title">Dimostrazione 5.15 - Matrice associata ad un'applicazione lineare</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando         <ul ><li >un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>;             </li><li >la base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> di <span class="math-tag">\( V\)</span>;             </li><li >la base <span class="math-tag">\( B_{W} = (w_{1}, \ \ldots \ , w_{m})\)</span> di <span class="math-tag">\( W\)</span>;             </li><li >il vettore <span class="math-tag">\( (x_{1}, \ \ldots \ , x_{n})\)</span> delle coordinate di un qualsiasi vettore <span class="math-tag">\( v \in V\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>;             </li><li >il vettore <span class="math-tag">\( (y_{1}, \ \ldots \ , y_{m})\)</span> delle coordinate di <span class="math-tag">\( f(v)\)</span> rispetto a <span class="math-tag">\( B_{W}\)</span></li></ul>         allora esiste una matrice <span class="math-tag">\( A\)</span> associata ad <span class="math-tag">\( f\)</span> rispetto alle basi <span class="math-tag">\( B_{V}\)</span> e <span class="math-tag">\( B_{W}\)</span>, tale che         <span class="math-tag">\[             \left(             \begin{array}{c}                 y_{1} \\                 \vdots \\                 y_{m}             \end{array}             \right)             =              \left(             \begin{array}{ccc}                 a_{1, 1} & \cdots & a_{1, n} \\                 \vdots & \ddots & \vdots \\                 a_{m, 1} & \cdots & a_{m,n}             \end{array}             \right)             \odot              \left(             \begin{array}{c}                 x_{1} \\                 \vdots \\                 x_{n}             \end{array}             \right)         \]</span>         ovvero         <span class="math-tag">\[             A = M_{B_{V}, B_{W}}(f)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò è necessario provare che le coordinate di un qualsiasi vettore <span class="math-tag">\( f(v)\)</span> rispetto a <span class="math-tag">\( B_{W}\)</span> possono essere calcolate utilizzando le coordinate di <span class="math-tag">\( v\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>.          <br ></br>         Consideriamo la combinazione lineare di un generico vettore <span class="math-tag">\( v \in V\)</span> rispetto a <span class="math-tag">\( B_{V}\)</span>, ovvero         <span class="math-tag">\[             v = \sum_{j = 1}^{n} x_{j} \cdot v_{j}         \]</span>         e la combinazione lineare dell'immagine di un vettore della base <span class="math-tag">\( B_{V}\)</span>, ovvero         <span class="math-tag">\[             f(v_{j}) = \sum_{i = 1}^{m} a_{i, j} \cdot w_{i}         \]</span>         dove <span class="math-tag">\( a_{i, j}\)</span> indica il valore della coordinata <span class="math-tag">\( i\)</span> nella combinazione lineare rispetto a <span class="math-tag">\( B_{W}\)</span> del vettore <span class="math-tag">\( v_{j}\)</span>.         <br ></br>         Allora si ha che <span class="math-tag">\( f(v)\)</span> può essere scritto come         <span class="math-tag">\begin{aligned}             & f(v) = f(\sum_{j = 1}^{n} x_{j} \cdot v_{j})             & \iff         \end{aligned}</span>         e, per la linearità di <span class="math-tag">\( f\)</span> si può scrivere         <span class="math-tag">\begin{aligned}             & f(v) = \sum_{j = 1}^{n} x_{j} \cdot f(v_{j})             & \iff         \end{aligned}</span>         Ora, è possibile sostituire <span class="math-tag">\( f(v_{j})\)</span> con la sua combinazione lineare rispetto a <span class="math-tag">\( B_{W}\)</span>, ovvero         <span class="math-tag">\begin{aligned}             & f(v) = \sum_{j = 1}^{n} x_{j} \cdot \sum_{i = 1}^{m} a_{i, j} \cdot w_{i}             & \iff         \end{aligned}</span>         Grazie alla proprietà distributiva delle sommatorie è possibile "portare all'interno della sommatoria più interna" <span class="math-tag">\( x_{j}\)</span>, ottenendo         <span class="math-tag">\begin{aligned}             & f(v) = \sum_{j = 1}^{n} \sum_{i = 1}^{m} a_{i, j} \cdot  x_{j} \cdot w_{i}             & \iff         \end{aligned}</span>         ed è ora possibile, sempre per le proprietà delle sommatorie, scambiarle         <span class="math-tag">\begin{aligned}             & f(v) = \sum_{i = 1}^{m} \sum_{j = 1}^{n} a_{i, j} \cdot  x_{j} \cdot w_{i}             & \iff         \end{aligned}</span>         e grazie alla proprietà associativa         <span class="math-tag">\begin{aligned}             & f(v) = \sum_{i = 1}^{m}              \left( \sum_{j = 1}^{n} a_{i, j} \cdot x_{j} \right) \cdot w_{i}             & \iff         \end{aligned}</span>         Infine si può esplicitare la sommatoria ottenendo         <span class="math-tag">\begin{aligned}             & f(v) = \left( \sum_{j = 1}^{n} a_{1, j} \cdot x_{j} \right) \cdot w_{1}             , \ \ldots \ ,             \left( \sum_{j = 1}^{n} a_{m, j} \cdot x_{j} \right) \cdot w_{m}             &         \end{aligned}</span>         ovvero <span class="math-tag">\( f(v)\)</span> come combinazione lineare di <span class="math-tag">\( B_{W}\)</span>. Tali coordinate, sono esattamente la formula del prodotto tra matrici, e ciò dimostra il teorema.      </div></div></div><div class="demonstration environment" id="dem5-16" ><span class="demonstration-header environment-title">Dimostrazione 5.16 - Dimensione dell'immagine di un'applicazione lineare come rango della matrice associata</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f: V \to W\)</span>, una base <span class="math-tag">\( B_{V}\)</span> per lo spazio vettoriale <span class="math-tag">\( V\)</span>, una base <span class="math-tag">\( B_{W}\)</span> per lo spazio vettoriale <span class="math-tag">\( W\)</span>, la matrice associata ad <span class="math-tag">\( f\)</span> rispetto <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span>, allora la dimensione dell'immagine di <span class="math-tag">\( f\)</span> è uguale al rango di <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span>, ovvero         <span class="math-tag">\[             dim(Im(f)) = r(M_{B_{V}, B_{W}}(f))         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò consideriamo la matrice <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span> scritta per colonne, ovvero         <span class="math-tag">\begin{aligned}             M_{B_{V}, B_{W}}(f) =              \left(              \begin{array}{ccccc}                 C_{1} & \left|\right. & \cdots & \left|\right. & C_{n}             \end{array}             \right)         \end{aligned}</span>         Consideriamo l'applicazione lineare          <span class="math-tag">\[             L_{V}: v \to (x_{1}, \ \ldots \ , x_{n})         \]</span>         che trasforma un qualsiasi vettore <span class="math-tag">\( v \in V\)</span> nella <span class="math-tag">\( n\)</span>-upla delle sue coordinate rispetto a <span class="math-tag">\( B_{V}\)</span> e l'applicazione lineare         <span class="math-tag">\[             L_{W}: w \rightarrow (y_{1}, \ \ldots \ , y_{m})         \]</span>         che trasforma un qualsiasi vettore <span class="math-tag">\( w \in W\)</span> nella <span class="math-tag">\( m\)</span>-upla delle sue coordinate rispetto a <span class="math-tag">\( B_{W}\)</span>: entrambe queste applicazioni lineari sono isomorfismi.         <br ></br>         Il rango è quindi la dimensione delle colonne di <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span> mentre la dimensione dell'immagine è la dimensione dello spazio generato dalla chiusura lineare <span class="math-tag">\( \lt f(v_{1}), \ \ldots \ , f(v_{n})\gt \)</span>. Questi due spazi, ovvero quello delle colonne di <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span> e quello della chiusura lineare di <span class="math-tag">\( \lt f(v_{1}), \ \ldots \ , f(v_{n})\gt \)</span>, sono isomorfi perchè è possibile vedere che l'applicazione <span class="math-tag">\( L_{W}\)</span> non fa altro che restituire una colonna della matrice <span class="math-tag">\( M_{B_{V}, B_{W}}(f)\)</span>, ovvero         <span class="math-tag">\[            L_{W}(f(v_{1})) = C_{1}             \qquad            \ldots            \qquad            L_{W}(f(v_{n})) = C_{n}          \]</span>         Dato che due isomorfismi hanno sempre la stessa dimensione, possiamo dire che la proposizione è dimostrata.     </div></div></div><div class="demonstration environment" id="dem5-17" ><span class="demonstration-header environment-title">Dimostrazione 5.17 - Matrice associata all'applicazione lineare identità</span>     Considerando la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando l'applicazione lineare identità <span class="math-tag">\( \text{id}_{V}: V \to V\)</span> tale che <span class="math-tag">\( f(v) = v\)</span>, considerando per <span class="math-tag">\( V\)</span> una qualsiasi base <span class="math-tag">\( B_{V}\)</span>, allora si ha che la matrice <span class="math-tag">\( M_{B_{V}, B_{V}(\text{id}_{V}})\)</span> è la matrice identica.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione consideriamo il metodo con cui si costruisce la matrice associata ad un'applicazione lineare. Considerando la generica base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> e il fatto che stiamo trattando l'applicazione lineare identità, si ha che le coordinate di tali vettori rispetto a <span class="math-tag">\( B_{V}\)</span> sono         <span class="math-tag">\[             ((1, 0, \ \ldots \ , 0), (0, 1, \ \ldots \ , 0), \ \ldots \ , (0, 0, \ \ldots \ , 1)         \]</span>         e ponendoli sulle colonne della matrice associata si ottiene         <span class="math-tag">\[             \left(             \begin{array}{cccc}                 1 & 0 & \cdots & 0 \\                 0 & 1 & \cdots & 0 \\                 \vdots & \vdots & \ddots & 0 \\                 0 & 0 & \cdots & 1             \end{array}             \right)         \]</span>         che è esattamente la matrice identica.     </div></div></div><div class="demonstration environment" id="dem5-18" ><span class="demonstration-header environment-title">Dimostrazione 5.18 - Matrice invertibile se e solo se il rango è massimo</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La matrice <span class="math-tag">\( A \in M_{n \times n}(\mathbb{K})\)</span> è invertibile se e solo se il suo rango è massimo, ovvero         <span class="math-tag">\[             r(A) = n         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Rango come lineare indipendenza</span>         Ciò significa che è invertibile se e solo se tutte le sue righe (e colonne) sono linearmente indipendenti tra loro.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare questa proposizione è necessario dimostrare che:         <ul ><li >"se <span class="math-tag">\( A\)</span> è invertibile, il suo rango è massimo", per farlo consideriamo l'applicazione lineare <span class="math-tag">\( f_{A}\)</span> che moltiplica una matrice <span class="math-tag">\( X\)</span> qualsiasi <span class="math-tag">\( n \times n\)</span> per la matrice <span class="math-tag">\( A\)</span>, ovvero             <span class="math-tag">\[                 f_{A}(X) = A \odot X             \]</span>             Ciò ci permette di dire che la matrice <span class="math-tag">\( A\)</span> è associata all'applicazione lineare <span class="math-tag">\( f_{A}\)</span> (in quanto il suo compito è moltiplicare per una matrice, e ciò è possibile solo se la matrice associata è proprio <span class="math-tag">\( A\)</span>. Ora, dato che <span class="math-tag">\( A\)</span> è una matrice associata ad un'applicazione lineare, si ha che la sua inversa (che esiste per ipotesi) è associata all'applicazione lineare inversa di <span class="math-tag">\( f_{A}\)</span>, e se <span class="math-tag">\( f_{A}\)</span> è invertibile significa che è biunivoca, in particolare suriettiva, ovvero che l'immagine coincide con il codominio e quindi la dimensione dell'immagine deve essere uguale alla dimensione del codominio             <span class="math-tag">\[                 r(A) = dim(Im(f))                 \quad                 \text{e}                  \quad                 dim(Im(f)) = n                 \quad                 \implies                  \quad                 r(A) = n             \]</span>             che dimostra la prima parte della proposizione;             </li><li >"se il rango di <span class="math-tag">\( A\)</span> è massimo, allora essa è invertibile", per farlo si consideri che la matrice <span class="math-tag">\( A\)</span> è composta da <span class="math-tag">\( n\)</span> colonne linearmente indipendenti: ciò implica che siano una base <span class="math-tag">\( B_{C}\)</span> per <span class="math-tag">\( \mathbb{K}^{n}\)</span>.             <br ></br>             Consideriamo ora che <span class="math-tag">\( A\)</span> può essere considerata come la matrice cambiamento di base <span class="math-tag">\( M_{B_{C}, \varepsilon}(\text{id}_{\mathbb{R}})\)</span> e dato che tale matrice è associata all'applicazione lineare identità che è biettiva, possiamo dire che <span class="math-tag">\( A\)</span> ammette inversa.         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div></div><div class="subsection part" id="subsec5-3" ><span class="subsection-header part-title">5.3 - Matrici simili</span><div class="demonstration environment" id="dem5-19" ><span class="demonstration-header environment-title">Dimostrazione 5.19 - Riflessività di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Ogni matrice è simile a se stessa, ovvero         <span class="math-tag">\[             A \sim A         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare  tale proprietà verifichiamo la definizione di similarità, ovvero         <span class="math-tag">\begin{aligned}             & A = E^{-1} \odot A \odot E             & \iff          \end{aligned}</span>         Consideriamo infatti che esiste la matrice <span class="math-tag">\( I\)</span> che è invertibile e verifichiamo che         <span class="math-tag">\begin{aligned}             & A = I^{-1} \odot A \odot I         \end{aligned}</span>         che dimostra la proprietà.     </div></div></div><div class="demonstration environment" id="dem5-20" ><span class="demonstration-header environment-title">Dimostrazione 5.20 - Simmetria di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando le matrici <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span>, se <span class="math-tag">\( A\)</span> è simile a <span class="math-tag">\( B\)</span>, allora <span class="math-tag">\( B\)</span> è simile ad <span class="math-tag">\( A\)</span>, ovvero         <span class="math-tag">\[             A \sim B             \qquad             \implies             \qquad             B \sim A         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">        Per dimostrare tale proprietà è necessario provare l'esistenza di una matrice <span class="math-tag">\( F \in M_{n \times n}(\mathbb{K})\)</span> invertibile tale che        <span class="math-tag">\[            A = F^{-1} \odot B \odot F        \]</span>        Per farlo consideriamo la definizione di similarità, ovvero        <span class="math-tag">\begin{aligned}            & B = E^{-1} \odot A \odot E            & \iff        \end{aligned}</span>        e moltiplichiamo a sinistra "da entrambe le parti" per la matrice <span class="math-tag">\( E\)</span> e moltiplichiamo a destra per la matrice <span class="math-tag">\( E^{-1}\)</span>, ovvero        <span class="math-tag">\begin{aligned}            & E \odot B \odot E^{-1} = E \odot (E^{-1} \odot A \odot E) \odot E^{-1}            & \iff        \end{aligned}</span>        Ora, per l'associatività del prodotto tra matrici        <span class="math-tag">\begin{aligned}            & E \odot B \odot E^{-1} = (E \odot E^{-1}) \odot A \odot (E \odot E^{-1})            & \iff         \end{aligned}</span>         ed eseguendo i calcoli nelle parentesi         <span class="math-tag">\begin{aligned}             & E \odot B \odot E^{-1} = I \odot A \odot I             & \iff \\             & A = E \odot B \odot E^{-1}             &        \end{aligned}</span>        A questo punto è possibile porre <span class="math-tag">\( F = E^{-1}\)</span> e scrivendo le matrici come l'inversa dell'inversa, si ha che        <span class="math-tag">\begin{aligned}            & A = (E^{-1})^{-1} \odot B \odot ((E^{-1})^{-1})^{-1}            & \iff \\            & A = F^{-1} \odot B \odot F            &        \end{aligned}</span>        si dimostra la proprietà.     </div></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Inversa della matrice che verifica <span class="math-tag">\( A \sim B\)</span> come matrice che verifica <span class="math-tag">\( B \sim A\)</span></span>         Da tale dimostrazione si può dedurre che la matrice <span class="math-tag">\( F\)</span> che verifica la similarità tra <span class="math-tag">\( B \sim A\)</span> non è altro che la matrice <span class="math-tag">\( E^{-1}\)</span> che verifica la similarità <span class="math-tag">\( A \sim B\)</span>.     </div></div><div class="demonstration environment" id="dem5-21" ><span class="demonstration-header environment-title">Dimostrazione 5.21 - Transitività di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando le matrici <span class="math-tag">\( A, B, C \in M_{n \times n}(\mathbb{K})\)</span>, nel caso <span class="math-tag">\( A\)</span> sia simile a <span class="math-tag">\( B\)</span> e <span class="math-tag">\( B\)</span> sia simile a <span class="math-tag">\( C\)</span>, si ha che <span class="math-tag">\( A\)</span> è simile a <span class="math-tag">\( C\)</span><span class="math-tag">\[             A \sim B             \qquad             \text{e}              \qquad              B \sim C             \qquad             A \sim C         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando le definizioni di similarità e la simmetria tra matrici simili è possibile scrivere le matrici <span class="math-tag">\( A, B, C\)</span> come         <span class="math-tag">\[             \begin{array}{ccc}                 A \sim B & \implies & A = E^{-1} \odot B \odot E \\                 B \sim A & \implies & B = E \odot A \odot E^{-1} \\                 B \sim C & \implies & B = F^{-1} \odot C \odot F \\                 C \sim B & \implies & C = F \odot B \odot F^{-1}             \end{array}         \]</span>         Per dimostrare tale proprietà è necessario dimostrare l'esistenza di una matrice <span class="math-tag">\( H\)</span> tale che         <span class="math-tag">\begin{aligned}             A = H^{-1} \odot C \odot H             \qquad             \text{oppure}             \qquad             C = H \odot A \odot H^{-1}         \end{aligned}</span>         Considerando quindi la definizione         <span class="math-tag">\begin{aligned}             & B = E \odot A \odot E^{-1}             & \iff         \end{aligned}</span>         è possibile sostituire <span class="math-tag">\( B\)</span> con <span class="math-tag">\( (F^{-1} \odot C \odot F)\)</span><span class="math-tag">\begin{aligned}             & F^{-1} \odot C \odot F = E \odot A \odot E^{-1}             & \iff          \end{aligned}</span>         e moltiplicare per <span class="math-tag">\( F\)</span> a sinistra da "entrambe le parti"         <span class="math-tag">\begin{aligned}             & F \odot F^{-1} \odot C \odot F = F \odot E \odot A \odot E^{-1}             & \iff          \end{aligned}</span>         e poi moltiplicare per <span class="math-tag">\( F^{-1}\)</span> a destra da "entrambe le parti"         <span class="math-tag">\begin{aligned}             & F \odot F^{-1} \odot C \odot F \odot F^{-1} = F \odot E \odot A \odot E^{-1} \odot F^{-1}             & \iff          \end{aligned}</span>         &Egrave; quindi possibile per la proprietà associativa         <span class="math-tag">\begin{aligned}             & (F \odot F^{-1}) \odot C \odot (F \odot F^{-1}) = F \odot E \odot A \odot E^{-1} \odot F^{-1}             & \iff          \end{aligned}</span>         risolvere le moltiplicazioni fra matrici inverse e ottenere         <span class="math-tag">\begin{aligned}             & I \odot C \odot I = F \odot E \odot A \odot E^{-1} \odot F^{-1}             & \iff          \end{aligned}</span>         &Egrave; poi possibile riscrivere la moltiplicazione <span class="math-tag">\( (E^{-1} \odot F^{-1})\)</span> come <span class="math-tag">\( (F \odot E)^{-1}\)</span> ed ottenere         <span class="math-tag">\begin{aligned}             & C = (F \odot E) \odot A \odot (F \odot E)^{-1}             & \iff          \end{aligned}</span>         e riscrivendo <span class="math-tag">\( (F \odot E)\)</span> come <span class="math-tag">\( H\)</span> si ottiene         <span class="math-tag">\begin{aligned}             & C = H \odot A \odot (H)^{-1}             &         \end{aligned}</span>         che dimostra la proprietà.     </div></div></div><div class="demonstration environment" id="dem5-22" ><span class="demonstration-header environment-title">Dimostrazione 5.22 - Similarità tra matrici legate allo stesso endomorfismo rispetto a basi diverse</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un'applicazione lineare <span class="math-tag">\( f: V \to V\)</span> e le matrici ad essa associate <span class="math-tag">\( M_{B_{V}, B_{V}}(f)\)</span> e <span class="math-tag">\( M_{B_{V}', B_{V}'}(f)\)</span> (ovvero rispetto a basi diverse), si ha che         <span class="math-tag">\[             M_{B_{V}, B_{V}}(f) \sim M_{B_{V}', B_{V}'}(f)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò è necessario provare l'esistenza di una matrice <span class="math-tag">\( E\)</span> tale che         <span class="math-tag">\[             M_{B_{V}', B_{V}'}(f) = E^{-1} \odot M_{B_{V}, B_{V}}(f) \odot E           \]</span>         Riscrivendo il procedimento per il "cambio delle basi legate alla matrice associata ad un'applicazione lineare" si ha che         <span class="math-tag">\begin{aligned}             & M_{B_{V}', B_{V}'}(f) =              M_{B_{V}, B_{V}'}(\text{id}_{V})             \odot              M_{B_{V}, B_{V}}(f)             \odot             M_{B_{V}', B_{V}}(\text{id}_{V})             & \iff         \end{aligned}</span>         e dato che <span class="math-tag">\( M_{B_{V}, B_{V}'}(\text{id}_{W})\)</span> può essere scritta anche come <span class="math-tag">\( (M_{B_{V}', B_{V}}(\text{id}_{V}))^{-1}\)</span>, si ha che         <span class="math-tag">\begin{aligned}             & M_{B_{V}', B_{V}'}(f) =              (M_{B_{V}', B_{V}}(\text{id}_{V}))^{-1}             \odot              M_{B_{V}, B_{V}}(f)             \odot             M_{B_{V}', B_{V}}(\text{id}_{V})             &         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-23" ><span class="demonstration-header environment-title">Dimostrazione 5.23 - Traccia di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La traccia di due matrici <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span> simili è la stessa, ovvero         <span class="math-tag">\[             A \sim B \qquad \implies \qquad tr(A) = tr(B)          \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando la definizione di similarità, si può scrivere          <span class="math-tag">\begin{aligned}             & tr(B) = tr(E^{-1} \odot A \odot E)             & \iff         \end{aligned}</span>         e, considerando "l'invarianza della traccia rispetto alla permutazione ciclica del prodotto tra matrici" si ha che         <span class="math-tag">\begin{aligned}             & tr(E^{-1} \odot A \odot E) = tr(A \odot E \odot E^{-1})             & \iff         \end{aligned}</span>         e grazie alla proprietà associativa         <span class="math-tag">\begin{aligned}             & tr(E^{-1} \odot A \odot E) = tr(A \odot (E \odot E^{-1}))             & \iff         \end{aligned}</span>         e sostituendo <span class="math-tag">\( E \odot E^{-1}\)</span> con <span class="math-tag">\( I\)</span> e <span class="math-tag">\( E^{-1} \odot A \odot E\)</span> con <span class="math-tag">\( B\)</span>, si ottiene         <span class="math-tag">\begin{aligned}             & tr(B) = tr(A)             &         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem5-24" ><span class="demonstration-header environment-title">Dimostrazione 5.24 - Rango di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il rango di due matrici <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span> simili è lo stesso, ovvero         <span class="math-tag">\[             A \sim B \qquad \implies \qquad r(A) = r(B)          \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando la matrice <span class="math-tag">\( A\)</span> come la matrice associata ad un endomorfismo <span class="math-tag">\( f: V \to V\)</span>, sappiamo che il rango di <span class="math-tag">\( A\)</span> è la dimensione dell'immagine di <span class="math-tag">\( f\)</span>. Dato che due matrici simili rappresentano lo stesso endomorfismo <span class="math-tag">\( f\)</span>, si ha che anche il rango di <span class="math-tag">\( B\)</span> è uguale alla dimension dell'immagine di <span class="math-tag">\( f\)</span>, ovvero         <span class="math-tag">\[             r(A) = dim(Im(f))             \quad             \text{e}             \quad             r(B) = dim(Im(f))             \qquad             \implies             \qquad             r(A) = r(B)         \]</span>         Si è quindi dimostrata la proprietà.     </div></div></div></div></div><div class="section part" id="sec6" ><span class="section-header part-title">6 - Determinante</span><div class="subsection part" id="subsec6-1" ><span class="subsection-header part-title">6.1 - Definizioni di determinante</span><div class="demonstration environment" id="dem6-1" ><span class="demonstration-header environment-title">Dimostrazione 6.1 - Multilinearità del determinante</span>     Considerando la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando le matrici formate da <span class="math-tag">\( n\)</span> righe (o colonne), ovvero         <span class="math-tag">\begin{aligned}             & A =              \left(             \begin{array}{c}                 R_{1} \\                 R_{x} \\                 \vdots \\                 R_{n}             \end{array}               \right)             & B =             \left(             \begin{array}{c}                 R_{1} \\                 R_{y} \\                 \vdots \\                 R_{n}             \end{array}               \right)         \end{aligned}</span>         (ovvero due matrici con tutte le righe uguali fra loro eccetto una) e la matrice         <span class="math-tag">\[             C =             \left(             \begin{array}{c}                 R_{1} \\                 \alpha \cdot R_{x} + \beta \cdot R_{y} \\                 \vdots \\                 R_{n}             \end{array}             \right)         \]</span>         si ha che         <span class="math-tag">\[             det(C) = \alpha \cdot det(A) + \beta \cdot det(B)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, occorre provare che fissate tutte le righe della matrice tranne una (consideriamo la prima), si ha che il determinante è lineare rispetto a quella riga.         <br ></br>         Consideriamo quindi la definizione di determinante di una matrice <span class="math-tag">\( A\)</span>, ovvero         <span class="math-tag">\begin{aligned}             & det(A) = \sum_{p \in S_{n}} sign(p) \cdot a_{1, p(1)} \cdot \ldots \cdot a_{n, p(n)}              & \iff         \end{aligned}</span>         &Egrave; possibile vedere <span class="math-tag">\( S_{n}\)</span> come diviso in parti (in base al valore di <span class="math-tag">\( p(1)\)</span>), per esempio nel caso di <span class="math-tag">\( S_{3}\)</span><span class="math-tag">\[             \begin{array}{ccc}                 (p \in S_{3} \ : \ p(1) = 1)                  & = &                 \left\{                     \left(                     \begin{array}{ccc}                         1 & 2 & 3 \\                         1 & 2 & 3                     \end{array}                     \right),                     \left(                     \begin{array}{ccc}                         1 & 2 & 3 \\                         1 & 3 & 2                     \end{array}                     \right)                 \right\}                 \\                 \\                 (p \in S_{3} \ : \ p(1) = 2)                  & = &                 \left\{                     \left(                     \begin{array}{ccc}                         1 & 2 & 3 \\                         2 & 1 & 3                     \end{array}                     \right),                     \left(                     \begin{array}{ccc}                         1 & 2 & 3 \\                         2 & 3 & 1                     \end{array}                     \right)                 \right\}                 \\                 \\                 (p \in S_{3} \ : \ p(1) = 3)                  & = &                 \left\{                     \left(                     \begin{array}{ccc}                         1 & 2 & 3 \\                         3 & 1 & 2                     \end{array}                     \right),                     \left(                     \begin{array}{ccc}                         1 & 2 & 3 \\                         3 & 2 & 1                     \end{array}                     \right)                 \right\}                 \end{array}         \]</span>         Allora è possibile riscrivere la definizione del determinante dividendo la sommatoria         <span class="math-tag">\[             \begin{array}{cclc}                det(A)                 & = &                 \sum_{p \in S_{n} \ : \ p(1) = 1} sign(p) \cdot a_{1, 1} \cdot \ldots \cdot a_{n, p(n)} & +                 \\                & + & \sum_{p \in S_{n} \ : \ p(1) = 2} sign(p) \cdot a_{1, 2} \cdot \ldots \cdot a_{n, p(n)} & +                \\                & + & \ldots & + \\                & + & \sum_{p \in S_{n} \ : \ p(1) = n} sign(p) \cdot a_{1, n} \cdot \ldots \cdot a_{n, p(n)} &             \end{array}         \]</span>         Dato che è una sommatoria, è possibile raccogliere <span class="math-tag">\( a_{1, p(1)}\)</span> e denominare le sommatorie nel seguente modo         <span class="math-tag">\[             \begin{array}{lcl}                a_{1, 1} \cdot C_{1} & = &                \sum_{p \in S_{n} \ : \ p(1) = 1} sign(p) \cdot a_{2, p(2)} \cdot \ldots \cdot a_{n, p(n)}                 \\                \vdots & = & \cdots                \\                a_{1, n} \cdot C_{n} & = &                \sum_{p \in S_{n} \ : \ p(1) = n} sign(p) \cdot a_{2, p(2)} \cdot \ldots \cdot a_{n, p(n)}              \end{array}         \]</span></div></div>     dove <span class="math-tag">\( C_{i}\)</span> sono valori costante (in quanto dipendenti da solo elementi non appartenenti alla prima riga) mentre i valori <span class="math-tag">\( a_{1, i}\)</span> non lo sono. Si ottiene quindi che il determinante di <span class="math-tag">\( A\)</span> può essere scritto come     <span class="math-tag">\begin{aligned}         det(A) = a_{1, 1} \cdot C_{1} + a_{1, 2} \cdot C_{2} + \ \ldots \ + a_{1, n} \cdot C_{n}      \end{aligned}</span>     Dato che è possibile ripetere il ragionamento per ogni riga, ciò verifica la multilinearità. </div><div class="demonstration environment" id="dem6-2" ><span class="demonstration-header environment-title">Dimostrazione 6.2 - Determinante di una matrice trasposta</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando il determinante di una matrice <span class="math-tag">\( A\)</span>, esso è uguale al determinante della sua trasposta, ovvero         <span class="math-tag">\[             det(A) = det({}^t \! A)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, consideriamo le matrici         <span class="math-tag">\[             A =              \left(             \begin{array}{c}                 a_{i, j}             \end{array}             \right)         \]</span>         e la sua trasposta         <span class="math-tag">\[             {}^t \! A =             \left(             \begin{array}{c}                 a_{j, i}             \end{array}             \right)             =             B             =              \left(             \begin{array}{c}                 b_{i, j}             \end{array}             \right)         \]</span>         dalla definizione è possibile calcolare il determinante di <span class="math-tag">\( B\)</span> come          <span class="math-tag">\begin{aligned}             & det(B) = \sum_{p \in S_{n}} sign(p) \cdot b_{1, p(1)} \cdot \ \ldots \ \cdot b_{n, p(n)}             & \iff         \end{aligned}</span>         che è possibile riscrivere, utilizzando la produttoria, come         <span class="math-tag">\begin{aligned}             & det(B) = \sum_{p \in S_{n}} sign(p) \cdot \prod_{i = 1}^{n} b_{i, p(i)}             & \iff         \end{aligned}</span>         ricordando che <span class="math-tag">\( b_{i, p(i)}\)</span> è uguale ad <span class="math-tag">\( a_{p(i), i}\)</span> e che il segno di una produttoria è uguale al segno della sua inversa, allora possiamo scrivere         <span class="math-tag">\begin{aligned}             & det(B) = \sum_{p \in S_{n}} sign(p^{-1}) \cdot \prod_{i = 1}^{n} a_{p(i), i}             & \iff         \end{aligned}</span>         possiamo dire che <span class="math-tag">\( j = p(i)\)</span> e che <span class="math-tag">\( i = p^{-1}(j)\)</span> e quindi riscrivere la produttoria come         <span class="math-tag">\begin{aligned}             & det(B) = \sum_{p \in S_{n}} sign(p^{-1}) \cdot \prod_{j = 1}^{n} a_{j, p^{-1}(j)}             & \iff         \end{aligned}</span>         Dato che è possibile dire che ogni permutazione ammette inversa, allora la "navigazione" di <span class="math-tag">\( S_{n}\)</span> è possibile (ed equivalente) anche tramite le inverse, ovvero (considerando <span class="math-tag">\( p^{-1} = q\)</span>)         <span class="math-tag">\begin{aligned}             & det(B) = \sum_{q \in S_{n}} sign(q) \cdot \prod_{j = 1}^{n} a_{j, q(j)}             &         \end{aligned}</span>         che è esattamente la definizione del determinante di <span class="math-tag">\( A\)</span>.         <br ></br>         Si è quindi dimostrata la proposizione.      </div></div></div><div class="demonstration environment" id="dem6-3" ><span class="demonstration-header environment-title">Dimostrazione 6.3 - Determinante della matrice inversa</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando una matrice invertibile <span class="math-tag">\( A\)</span>, si ha che il determinante dell'inversa di <span class="math-tag">\( A\)</span> è il reciproco del determinante di <span class="math-tag">\( A\)</span>, ovvero         <span class="math-tag">\[             det(A^{-1}) = \frac{1}{det(A)}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò, consideriamo la moltiplicazione tra il determinante di una matrice e il determinante della sua inversa, che, per il teorema di Binet, è uguale a         <span class="math-tag">\begin{aligned}             & det(A) \cdot det(A^{-1}) = det(A \odot A^{-1})             & \iff         \end{aligned}</span>         Si ha però che <span class="math-tag">\( A \odot A^{-1}\)</span> è la matrice identica, il cui determinante è <span class="math-tag">\( 1\)</span><span class="math-tag">\begin{aligned}             & det(A \odot A^{-1}) = det(I) = 1             &         \end{aligned}</span>         Si ha quindi che il determinante dell'inversa di <span class="math-tag">\( A\)</span> è uguale al reciproco del determinante di <span class="math-tag">\( A\)</span>.         <br ></br>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem6-4" ><span class="demonstration-header environment-title">Dimostrazione 6.4 - Effetti delle operazioni riga sul determinante</span>     Date le proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Gli effetti di un'operazione riga sul determinante sono i seguenti:         <ul ><li >se si aggiunge ad una riga della matrice un multiplo di un'altra riga, il determinante non cambia;             </li><li >se si scambiano tra loro due righe della matrice, il determinante si inverte (diventa l'opposto);             </li><li >se si moltiplica una riga per uno scalare diverso da <span class="math-tag">\( 0\)</span>, il determinante è da moltiplicare per tale scalare.         </li></ul></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Aggiungere ad una riga della matrice un multiplo di un'altra riga significa concretamente utilizzare la multilinearità, infatti considerando         <span class="math-tag">\[             A =              \left(             \begin{array}{c}                 R_{1} \\                 R_{x} \\                 R_{y} \\                 \vdots \\                 R_{n}             \end{array}             \right)         \]</span>         e un'altra matrice (<span class="math-tag">\( B\)</span>) contenente al posto della riga <span class="math-tag">\( R_{y}\)</span> un multiplo della riga <span class="math-tag">\( R_{x}\)</span> (ovvero <span class="math-tag">\( \alpha \cdot R_{x}\)</span>),          <span class="math-tag">\[             B =              \left(             \begin{array}{c}                 R_{1} \\                 R_{x} \\                 \alpha \cdot R_{x} \\                 \vdots \\                 R_{n}             \end{array}               \right)         \]</span>         Si ha quindi che il determinante della matrice <span class="math-tag">\( C\)</span> sarà uguale a         <span class="math-tag">\[             \begin{array}{ccccc}                 det(C) & = & det(A) & + & det(B) \\                 det                 \left(                 \begin{array}{c}                     R_{1} \\                     R_{x} \\                     R_{y} + \alpha \cdot R_{x} \\                     \vdots \\                     R_{n}                 \end{array}                   \right)                 & = &                 det                 \left(                 \begin{array}{c}                     R_{1} \\                     R_{x} \\                     R_{y} \\                     \vdots \\                     R_{n}                 \end{array}                 \right)                 & + &                  \left(                 \begin{array}{c}                     R_{1} \\                     R_{x} \\                     \alpha \cdot R_{x} \\                     \vdots \\                     R_{n}                 \end{array}                   \right)                         \end{array}         \]</span>         Dato che <span class="math-tag">\( B\)</span> contiene una riga che è combinazione lineare delle altre, il suo determinante è <span class="math-tag">\( 0\)</span>.          Si ha quindi che il determinante rimane invariato, provando la proposizione.     </div></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Scambiare tra loro due righe della matrice significa concretamente applicare la proprietà dell'alternatività.     </div></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Moltiplicare una riga per uno scalare diverso da <span class="math-tag">\( 0\)</span> significa concretamente utilizzare la multilinearità, infatti considerando         <span class="math-tag">\[             A =              \left(             \begin{array}{c}                 R_{1} \\                 R_{x} \\                 \vdots \\                 R_{n}             \end{array}             \right)         \]</span>         e un'altra matrice (<span class="math-tag">\( B\)</span>) contenente al posto della riga <span class="math-tag">\( R_{x}\)</span> la riga <span class="math-tag">\( (\alpha - 1) \cdot R_{x}\)</span>),          <span class="math-tag">\[             B =              \left(             \begin{array}{c}                 R_{1} \\                 (\alpha - 1) \cdot R_{x} \\                 \vdots \\                 R_{n}             \end{array}               \right)         \]</span>         Si ha quindi che il determinante della matrice <span class="math-tag">\( C\)</span> sarà uguale a         <span class="math-tag">\[             \begin{array}{ccccc}                 det(C) & = & det(A) & + & det(B) \\                 det                 \left(                 \begin{array}{c}                     R_{1} \\                     R_{x} \\                     R_{y} + \alpha \cdot R_{x} \\                     \vdots \\                     R_{n}                 \end{array}                   \right)                 & = &                 det                 \left(                 \begin{array}{c}                     R_{1} \\                     R_{x} \\                     R_{y} \\                     \vdots \\                     R_{n}                 \end{array}                 \right)                 & + &                  \left(                 \begin{array}{c}                     R_{1} \\                     R_{x} \\                     \alpha \cdot R_{x} \\                     \vdots \\                     R_{n}                 \end{array}                   \right)                         \end{array}         \]</span>         Che significa aggiungere <span class="math-tag">\( \alpha - 1\)</span> volte il determinante di <span class="math-tag">\( A\)</span> ad <span class="math-tag">\( A\)</span>, che è equivalente a moltiplicare per <span class="math-tag">\( \alpha\)</span> volte il determinante. In questo modo si è dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem6-5" ><span class="demonstration-header environment-title">Dimostrazione 6.5 - Determinante in una matrice con rango non massimo</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il determinante di una matrice <span class="math-tag">\( A \in M_{n \times n}(\mathbb{K})\)</span> è diverso da <span class="math-tag">\( 0\)</span> se e solo se il suo rango è uguale a <span class="math-tag">\( n\)</span>, ovvero         <span class="math-tag">\[             det(A) \neq 0             \qquad             \iff              \qquad             r(A) = n         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Determinante di una matrice contenente una riga (o colonna) multiplo di un'altra</span>         Ciò è equivalente a dire che il determinante di una matrice contenente una riga (o colonna) che è combinazione lineare delle altre è <span class="math-tag">\( 0\)</span>.         </div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Determinante di una matrice contenente una riga (o colonna) nulla</span>         Ciò è equivalente a dire che il determinante di una matrice contenente una riga (o colonna) nulla è <span class="math-tag">\( 0\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, si consideri che effettuare operazioni riga non cambia il rango della matrice e nemmeno la "nullità" del determinante: per questo motivo è sufficiente  dimostrare la proposizione per le matrici completamente ridotte. &Egrave; quindi necessario dimostrare che:         <ul ><li >"se una matrice <span class="math-tag">\( n \times n\)</span> ha determinante diverso da <span class="math-tag">\( 0\)</span>, allora il suo rango è <span class="math-tag">\( n\)</span>". &Egrave; possibile considerare una matrice quadrata completamente ridotta come una matrice triangolare e, quindi, il suo determinante come il prodotto dei termini sulla diagonale principale: se il determinante non è nullo, allora nessuno degli elementi sulla diagonale è nullo, che equivale a dire che sono presenti <span class="math-tag">\( n\)</span> pivot. Ciò equivale a dire che il rango della matrice è <span class="math-tag">\( n\)</span>;             </li><li >"se il rango di una matrice <span class="math-tag">\( n \times n\)</span> è <span class="math-tag">\( n\)</span>, allora il suo determinante non è nullo". Considerando una matrice completamente ridotta, avere <span class="math-tag">\( n\)</span> pivot è equivalente a dire che non è presente alcuna riga nulla. Se così non fosse, considerando il calcolo del determinante             <span class="math-tag">\[                 det(A) = \sum_{p \in S_{n}} sign(p) \cdot a_{1, p(1)} \cdot \ \ldots \ \cdot a_{n, p(n)}              \]</span>             si avrebbe una sommatoria di addendi dove almeno un elemento di ogni prodotto è nullo (in quanto una riga sarebbe nulla), che porta alla sommatoria di tutti elementi nulli (ovvero il determinante uguale a <span class="math-tag">\( 0\)</span>).             <br ></br>             Invece con <span class="math-tag">\( n\)</span> pivot si avrebbe una matrice diagonale con tutti elementi non nulli (i pivot), e sappiamo quindi che il determinante non è <span class="math-tag">\( 0\)</span>.         </li></ul>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem6-6" ><span class="demonstration-header environment-title">Dimostrazione 6.6 - Determinante di matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il determinante di due matrici <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span> simili è lo stesso, ovvero         <span class="math-tag">\[             A \sim B \qquad \implies \qquad              det(A) = det(B)          \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò, consideriamo la definizione di similarità, ovvero l'esistenza di una matrice <span class="math-tag">\( E\)</span> tale che         <span class="math-tag">\begin{aligned}             & B = E^{-1} \odot A \odot E              & \iff         \end{aligned}</span>         Si vuole quindi dimostrare che         <span class="math-tag">\begin{aligned}             & det(B) = det(E^{-1} \odot A \odot E)              & \iff         \end{aligned}</span>         e, per il teorema di Binet         <span class="math-tag">\begin{aligned}             & det(B) = det(E^{-1}) \cdot det(A) \cdot det(E)              & \iff         \end{aligned}</span>         A questo punto, ricordando che il prodotto tra scalari commuta e che il determinante di una matrice è il reciproco del determinante della sua inversa, allora si può scrivere         <span class="math-tag">\begin{aligned}             & det(B) = \frac{1}{det(E)} \cdot \det(A) \cdot det(E)              & \iff \\             & det(B) = \frac{det(E)}{det(E)} \cdot det(A)             & \iff \\             & det(B) = det(A) &         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div></div></div><div class="section part" id="sec7" ><span class="section-header part-title">7 - Autovalori e autovettori</span><div class="subsection part" id="subsec7-1" ><span class="subsection-header part-title">7.1 - Autovalori, autovettori e autospazi</span><div class="demonstration environment" id="dem7-1" ><span class="demonstration-header environment-title">Dimostrazione 7.1 - <span class="math-tag">\( U_{\lambda}\)</span> come sottospazio vettoriale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         L'insieme <span class="math-tag">\( U_{\lambda}\)</span> associato ad un endomorfismo <span class="math-tag">\( f: V \to V\)</span> è un sottospazio vettoriale di <span class="math-tag">\( V\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione è necessario verificare che il vettore somma di due vettori e il vettore prodotto per uno scalare appartengono sempre ad <span class="math-tag">\( U_{\lambda}\)</span>.         <br ></br>         Consideriamo quindi <span class="math-tag">\( v, w \in U_{\lambda}\)</span> e <span class="math-tag">\( \alpha, \beta \in \mathbb{R}\)</span>, e scriviamo per linearità che         <span class="math-tag">\begin{aligned}             & f(\alpha \cdot v + \beta \cdot w) = \alpha \cdot f(v) + \beta \cdot f(w)              & \iff         \end{aligned}</span>         Ora, dato che <span class="math-tag">\( v, w\)</span> appartengono ad <span class="math-tag">\( U_{\lambda}\)</span>, sappiamo che la loro immagine è         <span class="math-tag">\begin{aligned}             & f(\alpha \cdot v + \beta \cdot w) = \alpha \cdot (\lambda \cdot v) + \beta \cdot (\lambda \cdot w)             & \iff         \end{aligned}</span>         allora per la proprietà distributiva si può scrivere         <span class="math-tag">\begin{aligned}             & f(\alpha \cdot v + \beta \cdot w) = \lambda \cdot (\alpha \cdot v + \beta \cdot w)             &          \end{aligned}</span>         che verifica la proposizione.     </div></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - <span class="math-tag">\( U_{\lambda}\)</span> e il vettore nullo</span>         Come tutti i sottospazi vettoriali, <span class="math-tag">\( U_{\lambda}\)</span> non è mai vuoto perchè contiene sempre almeno il vettore nullo.     </div></div><div class="demonstration environment" id="dem7-2" ><span class="demonstration-header environment-title">Dimostrazione 7.2 - Polinomio caratteristico di due matrici simili</span>     Data la proprietà     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due matrici <span class="math-tag">\( A, B \in M_{n \times n}(\mathbb{K})\)</span> simili allora il polinomio di tali matrici è lo stesso, ovvero         <span class="math-tag">\[             A \sim B              \qquad \implies \qquad              p_{A}(\lambda) = p_{B}(\lambda)          \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per definizione, si ha che         <span class="math-tag">\begin{aligned}             & p_{B}(\lambda) = p_{E^{-1} \odot A \odot E}(\lambda) = det((E^{-1} \odot A \odot E) - \lambda I)             & \iff         \end{aligned}</span>         Dato che <span class="math-tag">\( E^{-1} \odot E = I\)</span> e che il prodotto per uno scalare è commutativo, si può scrivere che         <span class="math-tag">\begin{aligned}             & det(E^{-1} \odot A \odot E - (E^{-1} \odot (\lambda I) \odot E))             & \iff         \end{aligned}</span>         e per la distributività si può raccogliere <span class="math-tag">\( E^{-1}\)</span> a sinistra ed <span class="math-tag">\( E\)</span> a destra, ottenendo così         <span class="math-tag">\begin{aligned}             & det(E^{-1} \odot (A - (\lambda I)) \odot E)             & \iff         \end{aligned}</span>         Ora, grazie al teorema di Binet, si può scrivere         <span class="math-tag">\begin{aligned}             & det(E^{-1}) \cdot det(A - \lambda I) \cdot det(E)             & \iff         \end{aligned}</span>         ed ora, grazie alla commutatività del prodotto tra scalari si può ottenere         <span class="math-tag">\begin{aligned}             & det(E^{-1}) \cdot det(E) \cdot det(A - \lambda I)             & \iff         \end{aligned}</span>         che è esattamente il polinomio caratteristico di <span class="math-tag">\( A\)</span>, in quanto         <span class="math-tag">\begin{aligned}             & det(A - \lambda I) = p_{A}(\lambda)             &         \end{aligned}</span>         Si è quindi dimostrata la proposizione.     </div></div></div><div class="demonstration environment" id="dem7-3" ><span class="demonstration-header environment-title">Dimostrazione 7.3 - Teorema - Determinante nullo della matrice caratteristica come condizione necessaria e sufficiente affinchè <span class="math-tag">\( \lambda\)</span> sia un autovalore</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il numero <span class="math-tag">\( \lambda \in \mathbb{R}\)</span> è un autovalore di <span class="math-tag">\( f\)</span> se e solo se il determinante della matrice caratteristica <span class="math-tag">\( (A - \lambda I)\)</span> è <span class="math-tag">\( 0\)</span>.      </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale teorema è necessario dimostrare che         <ul ><li >"se <span class="math-tag">\( \lambda\)</span> è un autovalore di <span class="math-tag">\( f\)</span>, allora il determinante della matrice caratteristica <span class="math-tag">\( (A - \lambda I)\)</span> è <span class="math-tag">\( 0\)</span>". Considerando che <span class="math-tag">\( A\)</span> è la matrice associata ad <span class="math-tag">\( f\)</span> e <span class="math-tag">\( X\)</span> le coordinate di un qualsiasi vettore <span class="math-tag">\( v \in V\)</span>, si ha che             <span class="math-tag">\begin{aligned}                 & A \odot X = \lambda \cdot X                 & \iff             \end{aligned}</span>             Affinchè <span class="math-tag">\( \lambda\)</span> sia un autovalore è necessario che <span class="math-tag">\( U_{\lambda}\)</span> contenga altri vettori oltre a quello nullo. Per provarlo, possiamo scrivere l'espressione precedente come             <span class="math-tag">\begin{aligned}                 & A \odot X = \lambda I \odot X                 & \iff             \end{aligned}</span>             e aggiungere da "entrambe le parti" l'opposto di <span class="math-tag">\( \lambda I \odot X\)</span> ottenendo             <span class="math-tag">\begin{aligned}                 & A \odot X - \lambda I \odot X = 0                 & \iff             \end{aligned}</span>             che per l'associatività è possibile scrivere come             <span class="math-tag">\begin{aligned}                 & (A - \lambda I) \odot X = 0                 &              \end{aligned}</span>             Considerando quindi questa espressione come un sistema lineare, si ha che <span class="math-tag">\( (A - \lambda I)\)</span> non deve avere rango massimo (altrimenti si avrebbe un'unica soluzione) ovvero il suo determinante deve essere nullo;             </li><li >"se il determinante della matrice caratteristica (<span class="math-tag">\( A - \lambda I\)</span>) è <span class="math-tag">\( 0\)</span>, allora <span class="math-tag">\( \lambda\)</span> è un autovalore di <span class="math-tag">\( f\)</span>". Supponiamo per assurdo che <span class="math-tag">\( \lambda\)</span> non sia autovalore di <span class="math-tag">\( f\)</span> : ciò implica che <span class="math-tag">\( U_{\lambda}\)</span> sia composto dal solo vettore nullo, ovvero che non esiste vettore non nullo <span class="math-tag">\( X\)</span> tale che             <span class="math-tag">\begin{aligned}                 & \nexists X \neq 0 \ : \ A \odot X = \lambda \cdot X                 & \iff             \end{aligned}</span>             Continuando su questa via, si può aggiungere "da entrambe le parti" l'opposto di <span class="math-tag">\( \lambda \cdot X\)</span>, ottenendo             <span class="math-tag">\begin{aligned}                 & A \odot X - \lambda \cdot X = 0                 & \iff             \end{aligned}</span>             e raccogliendo <span class="math-tag">\( X\)</span> per la proprietà distributiva si ottiene             <span class="math-tag">\begin{aligned}                 & (A \odot - \lambda \cdot I) \odot X = 0                 & \iff             \end{aligned}</span>             che rappresenta un sistema lineare. Dato che sappiamo che il determinante di <span class="math-tag">\( A \odot - \lambda \cdot I\)</span> è <span class="math-tag">\( 0\)</span>, sappiamo che il suo rango non è massimo (ovvero <span class="math-tag">\( \lt  n\)</span> come minimo). Dato che la dimensione dello spazio delle soluzioni è dato da             <span class="math-tag">\[                 dim(Sol(S) = n - (\lt  n) = (\gt  0)              \]</span>             si otterrebbe che la dimensione dello spazio delle soluzioni è maggiore di <span class="math-tag">\( 0\)</span> che è un assurdo, in quanto sappiamo che la dimensione dello spazio delle soluzioni deve essere <span class="math-tag">\( 0\)</span> (ovvero deve essere composto solo dal vettore nullo).         </li></ul>         Si è quindi dimostrato il teorema.     </div></div></div><div class="demonstration environment" id="dem7-4" ><span class="demonstration-header environment-title">Dimostrazione 7.4 - Autovalori per le composizioni di <span class="math-tag">\( f^{n}\)</span></span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando <span class="math-tag">\( v\)</span> un autovettore associato ad un autovalore <span class="math-tag">\( \lambda\)</span>, si ha che all'<span class="math-tag">\( n\)</span>-esima composizione di <span class="math-tag">\( f\)</span> sarà associato l'autovettore <span class="math-tag">\( \lambda^{n}\)</span>, ovvero         <span class="math-tag">\[             f(v) = \lambda \cdot v             \qquad             \implies             \qquad             f^{n}(v) = \lambda^{n} \cdot v         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando la composizione <span class="math-tag">\( f^{n}\)</span> dell'endomorfismo <span class="math-tag">\( f\)</span>, si ha che         <span class="math-tag">\[            f_{1} \circ \ldots \circ f_{n} = \lambda \cdot f^{n-1}(v) = \lambda^{n} \cdot v         \]</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem7-5" ><span class="demonstration-header environment-title">Dimostrazione 7.5 - Autovalori e autovettori dell'inverso di un endomorfismo</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un endomorfismo <span class="math-tag">\( f\)</span> invertibile, se <span class="math-tag">\( v\)</span> è un autovettore per <span class="math-tag">\( f\)</span>, esso lo è anche per <span class="math-tag">\( f^{-1}\)</span> e l'autovalore ad esso associato è <span class="math-tag">\( \frac{1}{\lambda}\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, consideriamo che         <span class="math-tag">\[             f(v) = \lambda \cdot v              \qquad \implies \qquad             v = f^{-1}(\lambda v)          \]</span>         allora è possibile scrivere         <span class="math-tag">\begin{aligned}             & v = f^{-1}(\lambda v)             & \iff         \end{aligned}</span>         e per la linearità di <span class="math-tag">\( f^{-1}\)</span><span class="math-tag">\begin{aligned}             & v = \lambda \cdot f^{-1}(v)             & \iff         \end{aligned}</span>         Moltiplicando da "entrambe le parti" per il reciproco di <span class="math-tag">\( \lambda\)</span> (che non è nullo, in quanto <span class="math-tag">\( f\)</span> non sarebbe invertibile)         si ha che         <span class="math-tag">\[             \frac{1}{\lambda} v = f^{-1}(v)             \qquad \implies \qquad             f^{-1}(v) = \frac{1}{\lambda} v          \]</span>         che dimostra la proposizione.     </div></div></div></div><div class="subsection part" id="subsec7-2" ><span class="subsection-header part-title">7.2 - Diagonalizzazione di una matrice</span><div class="demonstration environment" id="dem7-6" ><span class="demonstration-header environment-title">Dimostrazione 7.6 - Lineare indipendenza di vettori appartenenti a diversi autospazi</span>     Dato il lemma     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando un endomorfismo <span class="math-tag">\( f: V \to V\)</span> e i suoi autospazi <span class="math-tag">\( U_{\lambda_{1}}, \ \ldots \ , U_{\lambda_{k}}\)</span>, allora i vettori appartenenti agli autospazi sono linearmente indipendenti tra loro.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò, ipotizziamo per assurdo che la <span class="math-tag">\( k\)</span>-upla di vettori <span class="math-tag">\( (v_{1}, \ \ldots \ , v_{k})\)</span> provenienti da autospazi distinti  siano linearmente dipendenti. Possiamo quindi dire che esiste almeno una <span class="math-tag">\( k\)</span>-upla di coordinate non nulla <span class="math-tag">\( (\alpha_{1}, \ \ldots \ , \alpha_{k})\)</span> che soddisfa tale equazione         <span class="math-tag">\[             \alpha_{1} \cdot v_{1} + \ \ldots \ + \alpha_{k} \cdot v_{k} = 0         \]</span>         Tra una di queste <span class="math-tag">\( k\)</span>-uple non nulle, scegliamo la <span class="math-tag">\( k\)</span>-upla il cui numero di elementi non nulli è minimizzato, ovvero il numero di valori non nulli è il più piccolo possibile, ma non è <span class="math-tag">\( 0\)</span> (ovvero non si deve considerare la <span class="math-tag">\( k\)</span>-upla nulla). Chiamiamo tale <span class="math-tag">\( k\)</span>-upla <span class="math-tag">\( (\bar{\alpha}_{1}, \ \ldots \ , \bar{\alpha}_{k})\)</span>.         <br ></br>         Riprendendo la relazione enunciata in precedenza, è lecito scrivere         <span class="math-tag">\begin{aligned}             & \bar{\alpha}_{1} \cdot v_{1} +  \ \ldots \ + \bar{\alpha}_{k} \cdot v_{k} = 0             & \iff         \end{aligned}</span>         Applicando a ciò quindi l'endomorfismo <span class="math-tag">\( f\)</span><span class="math-tag">\begin{aligned}             & f(\bar{\alpha}_{1} \cdot v_{1} +  \ \ldots \ + \bar{\alpha}_{k} \cdot v_{k}) = f(0)             & \iff         \end{aligned}</span>         è possibile utilizzare la linearità per ottenere         <span class="math-tag">\begin{aligned}             & \bar{\alpha}_{1} \cdot f(v_{1}) +  \ \ldots \ +  \bar{\alpha}_{k} \cdot f(v_{k}) = 0             & \iff         \end{aligned}</span>         Ora, scrivendo l'immagine dei vettori si ottiene         <span class="math-tag">\begin{aligned}             & \bar{\alpha}_{1} \cdot \lambda_{1} \cdot v_{1} +  \ \ldots \ +  \bar{\alpha}_{k} \cdot \lambda_{k} \cdot v_{k} = 0             & \iff         \end{aligned}</span>         Ora, ritornando all'uguaglianza          <span class="math-tag">\begin{aligned}             & \bar{\alpha}_{1} \cdot v_{1} +  \ \ldots \ + \bar{\alpha}_{k} \cdot v_{k} = 0             & \iff         \end{aligned}</span>         invece di applicare l'endomorfismo, consideriamo che <span class="math-tag">\( \lambda_{1} \neq 0\)</span> (o un qualsiasi altro autovalore non nullo) e moltiplichiamolo all'equazione da "entrambe le parti", ovvero         <span class="math-tag">\begin{aligned}             & \bar{\alpha}_{1} \cdot \lambda_{1} \cdot v_{1} +  \ \ldots \ + \bar{\alpha}_{k} \cdot \lambda_{1} \cdot v_{k} = 0             & \iff         \end{aligned}</span>         Ora, ricordando che stiamo lavorando con delle uguaglianze, è lecito scrivere         <span class="math-tag">\begin{aligned}             & \bar{\alpha}_{1} \cdot \lambda_{1} \cdot v_{1} +  \ \ldots \ +  \bar{\alpha}_{k} \cdot \lambda_{k} \cdot v_{k} = \bar{\alpha}_{1} \cdot \lambda_{1} \cdot v_{1} +  \ \ldots \ + \bar{\alpha}_{k} \cdot \lambda_{1} \cdot v_{k}             & \iff         \end{aligned}</span>         Aggiungendo da entrambe le parti l'opposto del termine di destra, si ottiene         <span class="math-tag">\begin{aligned}             & \bar{\alpha}_{2} \cdot \lambda_{2} \cdot v_{2} +  \ \ldots \ +  \bar{\alpha}_{k} \cdot \lambda_{k} \cdot v_{k} - \bar{\alpha}_{2} \cdot \lambda_{1} \cdot v_{2} -  \ \ldots \ - \bar{\alpha}_{k} \cdot \lambda_{1} \cdot v_{k} = 0             & \iff         \end{aligned}</span>         Facendo notare che il termine <span class="math-tag">\( \bar{\alpha}_{1} \cdot \lambda_{1} \cdot v_{1}\)</span> si è annullato, possiamo raccogliere i coefficienti <span class="math-tag">\( \bar{\alpha}_{i}\)</span> e scrivere         <span class="math-tag">\begin{aligned}            & \bar{\alpha}_{2} \cdot (\lambda_{2} - \lambda_{1}) \cdot v_{2} + \ \ldots \ + \bar{\alpha}_{k} \cdot (\lambda_{k} - \lambda{1}) \cdot v_{2} = 0            &         \end{aligned}</span>         Dato che tutti i valori <span class="math-tag">\( \lambda\)</span> sono distinti (in quanto provenienti da autospazi differenti), allora si ha che le differenze <span class="math-tag">\( (\lambda_{i} - \lambda_{1})\)</span> non saranno mai nulle. Abbiamo quindi raggiunto un assurdo, in quanto avremmo trovato una <span class="math-tag">\( k\)</span>-upla in cui il numero di elementi non nulli è minore della <span class="math-tag">\( k\)</span>-upla <span class="math-tag">\( (\bar{\alpha}_{1}, \ \ldots \ , \bar{\alpha}_{k})\)</span> in quanto <span class="math-tag">\( \bar{\alpha}_{1}\)</span> è ora nullo.          <br ></br>         Si è dimostrata quindi la proposizione.     </div></div></div><div class="demonstration environment" id="dem7-7" ><span class="demonstration-header environment-title">Dimostrazione 7.7 - Condizione necessaria e sufficiente che garantisce la semplicità di un endomorfismo</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Un endomorfismo <span class="math-tag">\( f: V \to V\)</span>, (con <span class="math-tag">\( dim(V) = n\)</span>) è semplice se e solo se la somma delle dimensioni di tutti i suoi autospazi è uguale a <span class="math-tag">\( n\)</span>, ovvero         <span class="math-tag">\[             \sum dim(U_{\lambda_{i}}) = n         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - In altre parole</span>         Dire che la somma delle dimensioni degli autospazi deve essere <span class="math-tag">\( n\)</span> è equivalente a dire che la somma delle molteplicità geometriche degli autovalori deve essere <span class="math-tag">\( n\)</span>, ovvero         <span class="math-tag">\[             \sum mg(\lambda_{i}) = n         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale teorema è necessario dimostrare che         <ul ><li >"se la somma delle dimensioni di tutti gli autospazi di un endomorfismo <span class="math-tag">\( f\)</span> è <span class="math-tag">\( n\)</span>, allora <span class="math-tag">\( f\)</span> è semplice".              <br ></br>             Per dimostrarlo è necessario provare che esiste una base spettrale per <span class="math-tag">\( f\)</span>. Consideriamo quindi di costruirla unendo le basi degli autospazi:             <span class="math-tag">\begin{aligned}                 & B_{U_{\lambda_{1}}} = (v_{1}^{1}, \ \ldots \ , v_{r_{1}}^{1}) \\                 & B_{U_{\lambda_{2}}} = (v_{1}^{2}, \ \ldots \ , v_{r_{2}}^{2}) \\                 & \vdots \\                 & B_{U_{\lambda_{k}}} = (v_{1}^{k}, \ \ldots \ , v_{r_{k}}^{k})             \end{aligned}</span>             dove l'apice indica l'autospazio di appartenenza, ed il pedice la posizione del vettore in ogni base (<span class="math-tag">\( r_{k}\)</span> indica la dimensione dell'autospazio). Otteniamo quindi una base <span class="math-tag">\( B_{V}\)</span> per <span class="math-tag">\( V\)</span> contenente <span class="math-tag">\( n\)</span> vettori. Per provare che sia una base, è sufficiente provare la lineare indipendenza, ovvero che tutti i coefficienti possono essere solo nulli.             <span class="math-tag">\begin{aligned}                 &                 \begin{array}{lclc}                     0 & = &                      \alpha_{1}^{1} \cdot v_{1}^{1} + \ \ldots \ + \alpha_{r_{1}}^{1} \cdot v_{r_{1}}^{1} & + \\                     & + & \alpha_{1}^{2} \cdot v_{1}^{2} + \ \ldots \ + \alpha_{r_{1}}^{2} \cdot v_{r_{2}}^{2}                      & + \\                     & + & \ldots & + \\                     & + & \alpha_{1}^{k} \cdot v_{1}^{k} + \ \ldots \ + \alpha_{r_{k}}^{k} \cdot v_{r_{k}}^{k} &                 \end{array}                 & \iff             \end{aligned}</span>             Dato che ogni riga è combinazione lineare di un distinto autospazio, è possibile sostituirla con un vettore qualsiasi appartenente a quell'autospazio, ovvero             <span class="math-tag">\begin{aligned}                 & 0 = v^{1} + v^{2} + \ \ldots \ + v^{k}                 & \iff             \end{aligned}</span>             Sappiamo tuttavia che, per il lemma sulla "lineare indipendenza di vettori appartenenti a diversi autospazi", che tali vettori sono linearmente indipendenti tra loro. Abbiamo quindi la combinazione lineare del vettore nullo di vettori linearmente indipendenti tra loro: tali vettori devono quindi essere tutti nulli. Se ogni vettore <span class="math-tag">\( v^{i}\)</span> è nullo, otteniamo             <span class="math-tag">\begin{aligned}                 & 0 = \alpha_{1}^{i} \cdot v_{1}^{i} + \ \ldots \ + \alpha_{r_{1}}^{i} \cdot v_{r_{1}}^{i}                 &              \end{aligned}</span>             ovvero la combinazione lineare del vettore nullo di vettori linearmente indipendenti: ogni coordinata deve essere quindi nulla. Dato che si è verificata la lineare indipendenza, si è dimostrata l'implicazione.             </li><li >"se un endomorfismo <span class="math-tag">\( f\)</span> è semplice, allora la somma delle dimensioni di tutti i suoi autospazi è <span class="math-tag">\( n\)</span>".              <br ></br>             Per dimostrare ciò consideriamo che <span class="math-tag">\( f\)</span> semplice implica l'esistenza di una base <span class="math-tag">\( B_{V}\)</span> per <span class="math-tag">\( V\)</span> composta da <span class="math-tag">\( n\)</span> autovettori: esistono quindi almeno <span class="math-tag">\( n\)</span> autovettori linearmente indipendenti tra loro. Inoltre sappiamo che non possono esistere più di <span class="math-tag">\( n\)</span> autovettori linearmente indipendenti, in quanto il massimo numero di vettori linearmente indipendenti è proprio <span class="math-tag">\( n\)</span> (altrimenti due basi avrebbero dimensione diversa). Si ha quindi che la somma delle dimensioni di autospazi è proprio <span class="math-tag">\( n\)</span>. Si è quindi dimostrata l'implicazione.         </li></ul>         Si è quindi verificato il teorema.     </div></div></div><div class="demonstration environment" id="dem7-8" ><span class="demonstration-header environment-title">Dimostrazione 7.8 - Diagonalizzabilità di una matrice data l'esistenza di <span class="math-tag">\( n\)</span> autovalori distinti</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Se una matrice <span class="math-tag">\( A \in M_{n \times n}(\mathbb{R})\)</span> associata ad un endomorfismo <span class="math-tag">\( f\)</span> ha <span class="math-tag">\( n\)</span> autovalori distinti, allora è diagonalizzabile.      </div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - In altre parole</span>         Ciò è equivalente a dire che un endomorfismo <span class="math-tag">\( f: V \to V\)</span>  che ammette <span class="math-tag">\( n\)</span> (ovvero un numero pari alla dimensione di <span class="math-tag">\( V\)</span>) autovalori distinti è semplice.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò consideriamo che la molteplicità geometrica di ogni autovalore non può essere inferiore a <span class="math-tag">\( 1\)</span>: nel caso avessimo <span class="math-tag">\( n\)</span> autovalori distinti otterremo che la somma delle dimensioni degli autospazi è almeno pari a <span class="math-tag">\( n\)</span> e, dato che sono linearmente indipendenti, si ha che non possono essere più di <span class="math-tag">\( n\)</span> (altrimenti sarebbero una base con una dimensione diversa da altre basi). Otteniamo quindi che la somma delle dimensioni è esattamente <span class="math-tag">\( n\)</span>, implicando quindi l'esistenza di una base spettrale (e la semplicità dell'endomorfismo).     </div></div></div></div></div><div class="section part" id="sec8" ><span class="section-header part-title">8 - Spazi vettoriali euclidei</span><div class="subsection part" id="subsec8-1" ><span class="subsection-header part-title">8.1 - Forme bilineari e prodotto scalare</span><div class="demonstration environment" id="dem8-1" ><span class="demonstration-header environment-title">Dimostrazione 8.1 - Rappresentazione di una forma bilineare</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Scegliendo una base <span class="math-tag">\( B_{V} = (v_{1}, \ \ldots \ , v_{n})\)</span> per <span class="math-tag">\( V\)</span>, è possibile rappresentare ogni forma bilineare su <span class="math-tag">\( V\)</span> con una matrice <span class="math-tag">\( n \times n\)</span>.       </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando due vettori <span class="math-tag">\( u, w \in V\)</span> questi potranno essere scritti come combinazione lineare di <span class="math-tag">\( B_{V}\)</span>, ovvero         <span class="math-tag">\begin{aligned}             & u = \sum_{i = 1}^{n} a_{i} \cdot v_{i}             & w = \sum_{j = 1}^{n} b_{j} \cdot v_{j}         \end{aligned}</span>         &Egrave; quindi possibile scrivere l'immagine di due vettori come         <span class="math-tag">\begin{aligned}             f(u, w) = f             \left(             \sum_{i = 1}^{n}a_{i} \cdot v_{i}, \sum_{j = 1}^{n} b_{j} \cdot v_{j}             \right)         \end{aligned}</span>         e grazie alla linearità <span class="math-tag">\( f\)</span> (fissando prima <span class="math-tag">\( a_{i})\)</span> e poi <span class="math-tag">\( b_{j}\)</span> si può scrivere         <span class="math-tag">\begin{aligned}             f(u, w) =              \sum_{i = 1}^{n} a_{i}             \cdot              \sum_{j = 1}^{n} b_{j}             \cdot f(v_{i}, v_{j})         \end{aligned}</span>         che è esattamente il calcolo da fare per risolvere tale prodotto matriciale         <span class="math-tag">\[             \left(             \begin{array}{ccc}                 a_{1} & \ldots & a_{n}             \end{array}             \right)             \odot             \left(             \begin{array}{ccc}                 f(v_{1}, v_{1}) & \ldots & f(v_{1}, v_{n}) \\                 \vdots & \ddots & \vdots \\                  f(v_{n}, v_{1}) & \ldots & f(v_{n}, v_{n})             \end{array}             \right)             \odot             \left(             \begin{array}{c}                 b_{1} \\                 \vdots \\                 b_{n}             \end{array}             \right)         \]</span>         Si è quindi dimostrata la proposizione.     </div></div></div></div><div class="subsection part" id="subsec8-2" ><span class="subsection-header part-title">8.2 - Spazi vettoriali euclidei</span><div class="demonstration environment" id="dem8-2" ><span class="demonstration-header environment-title">Dimostrazione 8.2 - Norma del multiplo di un vettore</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La norma del multiplo di un vettore <span class="math-tag">\( v\)</span> (<span class="math-tag">\( \alpha \cdot v\)</span>), è il valore assoluto di <span class="math-tag">\( \alpha\)</span> moltiplicata per la norma di <span class="math-tag">\( v\)</span>, ovvero         <span class="math-tag">\[             ||\alpha \cdot v|| = \left| \alpha \right| \cdot ||v||         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale proposizione, consideriamo la definizione di norma, ovvero         <span class="math-tag">\begin{aligned}             & ||\alpha \cdot v|| = \sqrt{(\alpha \cdot v) \star (\alpha \cdot v)}             & \iff         \end{aligned}</span>         e per la bilinearità del prodotto scalare si può scrivere         <span class="math-tag">\begin{aligned}             & \sqrt{\alpha^{2} \cdot (v \star v)}             & \iff \\             & \left| \alpha \right| \cdot \sqrt{v \star v}              & \iff \\             & \left| \alpha \right| \cdot ||v||             &         \end{aligned}</span>         Che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem8-3" ><span class="demonstration-header environment-title">Dimostrazione 8.3 - Quadrato della norma della somma di due vettori</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando due vettori <span class="math-tag">\( v, w\)</span>, il quadrato della norma del vettore somma <span class="math-tag">\( v + w\)</span> è          <span class="math-tag">\[             ||v + w||^{2} = ||v||^{2} + 2 \cdot (v \star w) + ||w||^{2}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando il quadrato della norma del vettore somma <span class="math-tag">\( v + w\)</span>, si ha che         <span class="math-tag">\begin{aligned}             & ||v + w||^{2} = (v + w) \star (v + w)             & \iff         \end{aligned}</span>         e per linearità si ha che         <span class="math-tag">\begin{aligned}             & (v \star v) + (v \star w) + (w \star v) + (w \star w)             & \iff \\             & ||v||^{2} + 2 \cdot (v \star w) + ||w||^{2}             &         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem8-4" ><span class="demonstration-header environment-title">Dimostrazione 8.4 - Disuguaglianza di Cauchy-Schwarz</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il valore assoluto del prodotto scalare di due vettori <span class="math-tag">\( v, w\)</span> è minore o uguale al prodotto tra le due norme, ovvero         <span class="math-tag">\[             \left| v \star w \right| \leq ||v|| \cdot ||w||         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare tale disuguaglianza, consideriamo la norma elevata al quadrato del vettore somma <span class="math-tag">\( v + tw\)</span> (dove <span class="math-tag">\( t\)</span> è una qualsiasi variabile reale) che per definizione è         <span class="math-tag">\begin{aligned}             & ||v + tw||^{2} = (v + tw) \star (v + tw)             & \iff         \end{aligned}</span>         Per la bilinearità si può scrivere         <span class="math-tag">\begin{aligned}             & v \star v + t \cdot (v \star w) + t \cdot (v \star w) + t^{2} \cdot (w \star w)             & \iff \\             & t^{2} \cdot (w \star w) + 2t \cdot (v \star w) + (v \star v)             & \iff \\             & t^{2} \cdot ||w||^{2} + 2t \cdot (v \star w) + ||v||^{2}             & \iff         \end{aligned}</span>         che possiamo vedere come un polinomio di secondo grado nella variabile <span class="math-tag">\( t\)</span>. Dato che siamo partiti dalla norma <span class="math-tag">\( ||v + tw||^{2}\)</span>, un valore sempre positivo o nullo         <span class="math-tag">\[             t^{2} \cdot ||w||^{2} + 2t \cdot (v \star w) + ||v||^{2} \geq 0         \]</span>         possiamo dire che il discriminante di tale polinomio è minore o uguale a <span class="math-tag">\( 0\)</span> (infatti si avrebbe una parabola che può essere solo tangente all'asse delle ascisse, ma mai secante dato che avrebbe una zona in cui è minore di <span class="math-tag">\( 0\)</span>), ovvero         <span class="math-tag">\begin{aligned}             & (2 \cdot (v \star w))^{2} - 4 \cdot ||w||^{2} \cdot ||v||^{2}) \leq 0             & \iff         \end{aligned}</span>         e risolvendo la disequazione         <span class="math-tag">\begin{aligned}             & 4 \cdot (v \star w)^{2} \leq 4 \cdot ||w||^{2} \cdot ||v||^{2}             & \iff \\         \end{aligned}</span>         e eliminando l'esponente con la radice da entrambe le parti         <span class="math-tag">\begin{aligned}             & \left| v \star w \right| \leq ||v|| \cdot ||w||              &         \end{aligned}</span>         si dimostra la proposizione.     </div></div></div><div class="demonstration environment" id="dem8-5" ><span class="demonstration-header environment-title">Dimostrazione 8.5 - Coordinate di un vettore rispetto ad una base ortonormale</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando <span class="math-tag">\( B = (v_{1}, \ \ldots \ , v_{n})\)</span> una base ortonormale, per ottenere la <span class="math-tag">\( i\)</span>-esima coordinata di un qualsiasi vettore <span class="math-tag">\( v\)</span> rispetto a tale base         <span class="math-tag">\[             v = x_{1} \cdot v_{1} + \ \ldots \ + x_{i} \cdot v_{i}  + \ \ldots \ + x_{n} \cdot v_{n}         \]</span>         è sufficiente fare il prodotto scalare tra <span class="math-tag">\( v\)</span> e l'<span class="math-tag">\( i\)</span>-esimo vettore della base, ovvero         <span class="math-tag">\[             x_{i} = v \star v_{i}         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò, consideriamo la combinazione lineare del vettore <span class="math-tag">\( v\)</span> rispetto alla base <span class="math-tag">\( B\)</span><span class="math-tag">\[             v = x_{1} \cdot v_{1} + \ \ldots \ + x_{i} \cdot v_{i}  + \ \ldots \ + x_{n} \cdot v_{n}         \]</span>         Effettuando il prodotto scalare tra un qualsiasi vettore e l'<span class="math-tag">\( i\)</span>-esimo vettore della base <span class="math-tag">\( B\)</span>, si avrebbe che         <span class="math-tag">\begin{aligned}             & v \star v_{i} = (x_{1} \cdot v_{1} + \ \ldots \ + x_{i} \cdot v_{i}  + \ \ldots \ + x_{n} \cdot v_{n}) \star v_{i}             & \iff         \end{aligned}</span>         che per la linearità del prodotto scalare è         <span class="math-tag">\begin{aligned}             &= x_{1} \cdot v_{1} \star v_{i} + \ \ldots \ + x_{i} \cdot v_{i} \star v_{i} + \ \ldots \ + x_{n} \cdot v_{n} \star v_{i}             &          \end{aligned}</span>         Scrivendo in questo modo, è evidente che tutti i prodotto scalari tra vettori si annullano (in quanto sono vettori ortogonali tra loro) ad eccezione del vettore <span class="math-tag">\( v_{i}\)</span>, il cui prodotto scalare è per definizione la sua norma (ovvero <span class="math-tag">\( 1\)</span>).         <span class="math-tag">\begin{aligned}             & x_{i} \cdot v_{i} \star v_{i}             &          \end{aligned}</span>         Ottenendo quindi la <span class="math-tag">\( i\)</span>-esima coordinata.     </div></div></div><div class="demonstration environment" id="dem8-6" ><span class="demonstration-header environment-title">Dimostrazione 8.6 - Metodo di ortonormalizzazione di Gram-Schmidt</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         &Egrave; sempre possibile rendere una qualsiasi base <span class="math-tag">\( B = (v_{1}, \ \ldots \ , v_{n})\)</span> ortonormale (ottenendo così la base <span class="math-tag">\( B' = (w_{1}, \ \ldots \ , w_{n})\)</span>).          <br ></br>         Per farlo è necessario considerare il primo vettore <span class="math-tag">\( v_{1}\)</span> e renderlo di norma <span class="math-tag">\( 1\)</span>, ovvero dividerlo per la sua norma         <span class="math-tag">\[             w_{1} = \frac{v_{1}}{||v_{1}||}         \]</span>         A questo punto è necessario rendere i vettori della base ortogonali a <span class="math-tag">\( w_{1}\)</span> e per farlo sono necessari due passaggi: il primo è ottenere il vettore <span class="math-tag">\( \widehat{w_{k}}\)</span>, che è il vettore ortogonale e poi renderlo di norma <span class="math-tag">\( 1\)</span>. Quindi si ha che         <span class="math-tag">\[             \left\{             \begin{array}{lcl}                 \widehat{w_{k}} & = & v_{k} - (v_{k} \star w_{k - 1}) w_{k - 1} - \ \ldots \ - (v_{k} \star w_{1}) w_{1}                  \\                 w_{k} & = & \frac{\widehat{w_{k}}}{||\widehat{w_{k}}||}             \end{array}             \right.         \]</span>         Ciò significa che per ottenere il vettore <span class="math-tag">\( w_{k}\)</span> è necessario calcolare tutti i vettori <span class="math-tag">\( w_{k - 1}, \ \ldots \ , w_{1}\)</span> precedenti.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         &Egrave; quindi necessario dimostrare che i vettori <span class="math-tag">\( w_{1}, \ \ldots \ , w_{k}\)</span> sono ortogonali a due a due tra loro (<span class="math-tag">\( w_{1} \perp w_{2}\)</span>, <span class="math-tag">\( w_{1} \perp w_{k}\)</span>), ovvero che il prodotto scalare tra loro è nullo.         <br ></br>         Per il metodo utilizzato, dire che <span class="math-tag">\( w_{1} \perp w_{2}\)</span> è equivalente a dire che <span class="math-tag">\( w_{1} \perp \widehat{w_{2}}\)</span>, ovvero         <span class="math-tag">\begin{aligned}             & w_{1} \perp w_{2}             & \iff \\             & w_{1} \perp \widehat{w_{2}}              & \iff \\             & w_{1} \perp (v_{2} - (v_{2} \star w_{1}) \cdot w_{1})             & \iff \\             & w_{1} \star (v_{2} - (v_{2} \star w_{1}) \cdot w_{1}) = 0             & \iff         \end{aligned}</span>         che per la linearità del prodotto scalare è uguale a         <span class="math-tag">\begin{aligned}             & (w_{1} \star v_{2}) - (v_{2} \star w_{1}) \cdot (w_{1} \star w_{1}) = 0             & \iff         \end{aligned}</span>         Ora, noi sappiamo che <span class="math-tag">\( (w_{1} \star w_{1})\)</span> è la norma di <span class="math-tag">\( w_{1}\)</span>, il cui valore è proprio <span class="math-tag">\( 1\)</span>: ciò significa che nella moltiplicazione si semplifica e rimane la seguente differenza          <span class="math-tag">\begin{aligned}             & (w_{1} \star v_{2}) - (v_{2} \star w_{1}) = 0             & \iff         \end{aligned}</span>         che è proprio <span class="math-tag">\( 0\)</span> in quanto il prodotto scalare è simmetrico.         <span class="math-tag">\begin{aligned}             & 0 = 0             &         \end{aligned}</span>         Tale dimostrazione si può ripetere per tutti gli altri vettori, dimostrando così la proposizione.     </div></div></div><div class="demonstration environment" id="dem8-7" ><span class="demonstration-header environment-title">Dimostrazione 8.7 - Esistenza di una base ortogonale (e ortonormale) per ogni spazio vettoriale euclideo</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Ogni spazio vettoriale euclideo ammette almeno una base ortogonale (e ortonormale).     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando che ogni spazio vettoriale ha diritto ad avere una base e che si può applicare il procedimento di ortonormalizzazione di Gram-Schmidth, si è dimostrato il teorema.     </div></div></div><div class="demonstration environment" id="dem8-8" ><span class="demonstration-header environment-title">Dimostrazione 8.8 - Lineare indipendenza tra vettori ortogonali</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando <span class="math-tag">\( n\)</span> vettori <span class="math-tag">\( v_{1}, \ \ldots \ , v_{n}\)</span> a due a due ortogonali non nulli, essi sono linearmente indipendenti.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostare ciò è necessario dimostrare che l'unica combinazione lineare del vettore nullo utilizzando i vettori <span class="math-tag">\( v_{1}, \ \ldots \ , v_{n}\)</span> sia quella con solo coordinate nulle, ovvero         <span class="math-tag">\begin{aligned}             & x_{1} \cdot v_{1} + \ \ldots \ + x_{n} v_{n} = 0             & \iff          \end{aligned}</span>         Facendo il prodotto scalare tra questi vettori e il vettore <span class="math-tag">\( v_{1}\)</span> si ottiene         <span class="math-tag">\begin{aligned}             & (x_{1} \cdot v_{1} + \ \ldots \ + x_{n} v_{n}) \star v_{1}  = 0             & \iff          \end{aligned}</span>         e per la linearità del prodotto scalare si ottiene         <span class="math-tag">\begin{aligned}             & x_{1} \cdot (v_{1} \star v_{1}) + \ \ldots \ + x_{n} (v_{n} \star v_{1}) = 0             & \iff          \end{aligned}</span>         e, dato che il prodotto scalare tra vettori orgonali è <span class="math-tag">\( 0\)</span>, si ha che          <span class="math-tag">\begin{aligned}             & x_{1} \cdot (v_{1} \star v_{1}) = 0             & \iff         \end{aligned}</span>         e dato che <span class="math-tag">\( v_{1} \star v_{1}\)</span> non può essere nullo per ipotesi, allora abbiamo obbligatoriamente che <span class="math-tag">\( x_{1}\)</span> lo deve essere.         <br ></br>         Ripetendo tale procedimento per ogni vettore, si ha che tutte le coordinate <span class="math-tag">\( x_{1}, \ \ldots \ , x_{n}\)</span> sono nulle, dimostrando quindi la lineare indipendenza.     </div></div></div><div class="demonstration environment" id="dem8-9" ><span class="demonstration-header environment-title">Dimostrazione 8.9 - Teorema di completamento a una base ortogonale (o ortonormale)</span>     Dato il teorema     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Se <span class="math-tag">\( v_{1}, \ \ldots \ , v_{k}\)</span> sono vettori a due due ortogonali non nulli (e quindi sono linearmente indipendenti), allora è possibile aggiungere dei vettori <span class="math-tag">\( v_{k + 1}, \ \ldots \ , v_{n}\)</span> in modo da ottenere una base ortogonale (o ortonormale se la norma di ogni vettore è <span class="math-tag">\( 1\)</span>)     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò occorre considerare che <span class="math-tag">\( v_{1}, \ \ldots \ , v_{k}\)</span> sono linearmente indipendenti. Per il teorema di completamento a una base, è possibile aggiungere altri vettori <span class="math-tag">\( v_{k + 1}, \ \ldots \ , v_{n}\)</span>. A questo punto, è possibile applicare Gram-Schmidt per ottenere una base ortogonale (o ortonormale).     </div></div></div><div class="demonstration environment" id="dem8-10" ><span class="demonstration-header environment-title">Dimostrazione 8.10 - Complemento ortogonale come sottospazio vettoriale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il complemento ortogonale di un sottospazio vettoriale <span class="math-tag">\( W\)</span> di <span class="math-tag">\( V\)</span> (<span class="math-tag">\( W^{\perp}\)</span>), è anch'esso un sottospazio vettoriale di <span class="math-tag">\( V\)</span>.      </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per verificare tale proposizione è necessario dimostrare che dati i vettori <span class="math-tag">\( u_{1}, u_{2} \in W^{\perp}\)</span>, la loro somma e il prodotto per uno scalare sia interna a <span class="math-tag">\( W\)</span>, ovvero per ogni <span class="math-tag">\( w \in W\)</span><span class="math-tag">\[             w \star (\alpha \cdot u_{1} + \beta \cdot u_{2}) = 0         \]</span>         Partendo quindi da          <span class="math-tag">\begin{aligned}             & w \star (\alpha \cdot u_{1} + \beta \cdot u_{2})             & \iff         \end{aligned}</span>         e applicando la linearità si ottiene         <span class="math-tag">\begin{aligned}             & \alpha \cdot (w \star u_{1}) + \beta \cdot (w \star u_{2}) = 0             &          \end{aligned}</span>         che è uguale a <span class="math-tag">\( 0\)</span> in quanto <span class="math-tag">\( u_{1}\)</span> e <span class="math-tag">\( u_{2}\)</span> appartengono già a <span class="math-tag">\( W^{\perp}\)</span> e il prodotto scalare con <span class="math-tag">\( w\)</span> è nullo.     </div></div></div><div class="demonstration environment" id="dem8-11" ><span class="demonstration-header environment-title">Dimostrazione 8.11 - Dimensione del complemento ortogonale di un sottospazio vettoriale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La dimensione del complemento ortogonale <span class="math-tag">\( W^{\perp}\)</span> di un sottospazio vettoriale <span class="math-tag">\( W\)</span> dello spazio vettoriale euclideo <span class="math-tag">\( V\)</span> è dipendente dalle dimensioni di <span class="math-tag">\( W\)</span> e <span class="math-tag">\( V\)</span>, ovvero         <span class="math-tag">\[             dim(W^{\perp}) = dim(V) - dim(W)         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Considerando una base ortonormale <span class="math-tag">\( B_{W} = (w_{1}, \ \ldots \ , w_{k})\)</span> di <span class="math-tag">\( k\)</span> vettori per <span class="math-tag">\( W\)</span>, per il teorema di completamento ad una base ortonormale, è possibile ottenere una base <span class="math-tag">\( B_{V}\)</span> per <span class="math-tag">\( V\)</span> composta da <span class="math-tag">\( n\)</span> vettori         <span class="math-tag">\[             B_{V} = (w_{1}, \ \ldots \ , w_{k}, w_{k + 1}, \ \ldots \ , w_{n})         \]</span>         Per dimostrare la proposizione, è sufficiente dimostrare che i vettori (<span class="math-tag">\( w_{k + 1}, \ \ldots \ , w_{n}\)</span>) siano una base ortonormale per <span class="math-tag">\( W^{\perp}\)</span>.         <br ></br>         Innanzitutto, sappiamo che ogni vettore (<span class="math-tag">\( w_{k + 1}, \ \ldots \ , w_{n}\)</span>) appartiene a <span class="math-tag">\( W^{\perp}\)</span> in quanto sono ortogonali a tutti i vettori di <span class="math-tag">\( W\)</span> (grazie al fatto che abbiamo considerato una base ortonormale) e che sono linearmente indipendenti (in quanto sono ortogonali tra loro).          <br ></br>         &Egrave; quindi necessario dimostrare che i vettori  (<span class="math-tag">\( w_{k + 1}, \ \ldots \ , w_{n}\)</span>) siano un sistema di generatori per <span class="math-tag">\( W^{\perp}\)</span>. Per farlo consideriamo che <span class="math-tag">\( W^{\perp}\)</span> è un sottospazio vettoriale di <span class="math-tag">\( V\)</span>, allora è possibile esprimere un qualsiasi vettore <span class="math-tag">\( u \in W^{\perp}\)</span> come combinazione lineare di <span class="math-tag">\( B_{V}\)</span>, ovvero         <span class="math-tag">\[             u = x_{1} \cdot w_{1} + \ \ldots \ + x_{k} \cdot w_{k} + x_{k + 1} \cdot w_{k + 1} + \ \ldots \ + x_{n} \cdot w_{n}         \]</span>         Ora, è sufficiente dimostrare che le coordinate <span class="math-tag">\( (x_{1}, \ \ldots \ , x_{k})\)</span> sono nulle (perchè ciò significa che non contribuiscono alla combinazione lineare dei vettori di <span class="math-tag">\( W^{\perp}\)</span>)         Per definizione, si ha che <span class="math-tag">\( u \star w_{i} = 0\)</span> (ovvero che il prodotto tra <span class="math-tag">\( u\)</span> e i vettori <span class="math-tag">\( (w_{1}, \ \ldots \ , w_{k})\)</span> è <span class="math-tag">\( 0\)</span>) e sappiamo che il prodotto scalare tra un vettore e un vettore di una base ortonormale fornisce la coordinata associata a tale vettore (rispetto a tale base ortonormale): sappiamo quindi che le coordinate <span class="math-tag">\( x_{1}, \ \ldots \ , x_{k}\)</span> (ovvero quelle associate ai vettori di <span class="math-tag">\( B_{W}\)</span>) sono tutte nulle. Quindi, è possibile scrivere <span class="math-tag">\( u\)</span> come         <span class="math-tag">\[             u = x_{k + 1} \cdot w_{k + 1} + \ \ldots \ + x_{n} \cdot w_{n}         \]</span>         combinazione lineare dei vettori <span class="math-tag">\( w_{k + 1}, \ \ldots \ , w_{n}\)</span>.         <br ></br>         Si è dimostrato che tali vettori sono una base per <span class="math-tag">\( W^{\perp}\)</span>.     </div></div></div><div class="demonstration environment" id="dem8-12" ><span class="demonstration-header environment-title">Dimostrazione 8.12 - Complemento ortogonale del complemento ortogonale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il complemento ortogonale <span class="math-tag">\( (W^{\perp})^{\perp}\)</span> del complemento ortogonale <span class="math-tag">\( W^{\perp}\)</span> del sottospazio vettoriale <span class="math-tag">\( W\)</span> è il sottospazio vettoriale <span class="math-tag">\( W\)</span>, ovvero         <span class="math-tag">\[             (W^{\perp})^{\perp} = W         \]</span></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - In altre parole</span>         Ciò significa che l'insieme dei vettori di <span class="math-tag">\( V\)</span> che sono ortogonali a tutti i vettori di <span class="math-tag">\( W^{\perp}\)</span> sono i vettori di <span class="math-tag">\( W\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Dimostrare tale proposizione significa dimostrare che <span class="math-tag">\( W\)</span> e <span class="math-tag">\( (W^{\perp})^{\perp}\)</span> sono lo stesso spazio vettoriale: per farlo è sufficiente dimostrare che una base per <span class="math-tag">\( W\)</span> è equivalente a una base per <span class="math-tag">\( (W^{\perp})^{\perp}\)</span>.         <br ></br>         Per farlo consideriamo una base ortonormale <span class="math-tag">\( B_{W} = (w_{1}, \ \ldots \ , w_{k})\)</span> e completiamola ad una base ortonormale per <span class="math-tag">\( B_{V}\)</span>, ovvero otteniamo la base         <span class="math-tag">\[             B_{V} = (w_{1}, \ \ldots \ , w_{k}, w_{k + 1}, \ \ldots \ , w_{n})         \]</span>         Per definizione, sappiamo quindi che i vettori aggiunti sono vettori linearmente indipendenti e ortogonali a tutti i vettori di <span class="math-tag">\( B_{W}\)</span>, inoltre sono esattamente <span class="math-tag">\( n - k\)</span> che è esattamente la dimensione del complemento ortogonale (ovvero <span class="math-tag">\( dim(W^{\perp}) = dim(V) - dim(W) = n - k\)</span>): per questo motivo possiamo dire che sono una base per <span class="math-tag">\( W^{\perp}\)</span>.         <br ></br>         Ora considerando <span class="math-tag">\( U = W^{\perp}\)</span>, per dimostrare la proposizione è necessario cercare <span class="math-tag">\( U^{\perp}\)</span>. Sempre considerando la relazione <span class="math-tag">\( dim(U^{\perp}) = dim(V) - dim(U)\)</span> otteniamo che <span class="math-tag">\( dim(U^{\perp}) = n - (n - k) = k\)</span>: è sufficiente quindi trovare <span class="math-tag">\( k\)</span> vettori ortogonali (in quanto l'ortogonalità implica la lineare indipendenza) tra loro. Ricordando che la base <span class="math-tag">\( B_{W}\)</span> contiene <span class="math-tag">\( k\)</span> vettori ortogonali tra loro, possiamo dire che <span class="math-tag">\( B_{W} = B_{U^{\perp}}\)</span> che dimostra la proposizione.      </div></div></div><div class="demonstration environment" id="dem8-13" ><span class="demonstration-header environment-title">Dimostrazione 8.13 - Determinante di una matrice ortogonale</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Il determinante di una matrice ortogonale <span class="math-tag">\( A\)</span> è <span class="math-tag">\( \pm 1\)</span>, ovvero         <span class="math-tag">\[             det(A) = \pm 1         \]</span></div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare questa proposizione, consideriamo che <span class="math-tag">\( 1\)</span> è il determinante della matrice identica, ovvero         <span class="math-tag">\begin{aligned}             & 1 = det(I)             & \iff         \end{aligned}</span>         e, dato che <span class="math-tag">\( A\)</span> è ortogonale, è possibile scrivere la matrice identica come         <span class="math-tag">\begin{aligned}             & det(A \odot {}^{t} \! A)             & \iff         \end{aligned}</span>         e per Binet         <span class="math-tag">\begin{aligned}             & det(A) \cdot det({}^{t} \! A)              & \iff         \end{aligned}</span>         Ricordandosi che il determinante di una matrice è uguale a quello della sua trasposta, si ottiene che         <span class="math-tag">\begin{aligned}            & det(A) \cdot det(A) = (det(A))^{2}            & \iff         \end{aligned}</span>         e paragonando il risultato con il valore di partenza si ottiene         <span class="math-tag">\begin{aligned}             & 1 = (det(A))^{2}             & \iff \\             & \pm 1 = det(A)         \end{aligned}</span>         ovvero che il determinante di una matrice ortogonale può essere solo <span class="math-tag">\( \pm 1\)</span>.     </div></div></div><div class="demonstration environment" id="dem8-14" ><span class="demonstration-header environment-title">Dimostrazione 8.14 - Matrice associata ad una isometria</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         La matrice associata ad un'isometria <span class="math-tag">\( f: V \to V\)</span> è ortogonale.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Per dimostrare ciò, consideriamo la matrice <span class="math-tag">\( A \in M_{n \times n}(\mathbb{R})\)</span> associata all'isometria <span class="math-tag">\( f: V \to V\)</span>, le coordinate di due vettori <span class="math-tag">\( x = (x_{1}, \ \ldots \ , x_{n})\)</span> e <span class="math-tag">\( y = (y_{1}, \ \ldots \ , y_{n})\)</span>, si avrà che l'immagine dei due vettori può essere così scritta         <span class="math-tag">\begin{aligned}             & f(x) = A \odot {}^{t} \! x             & f(y) = A \odot {}^{t} \! y         \end{aligned}</span>         Dove <span class="math-tag">\( x\)</span> e <span class="math-tag">\( y\)</span> sono trasposti per poterli moltiplicare alla matrice <span class="math-tag">\( A\)</span>. Tuttavia, le immagini risultano essere vettori colonna mentre per fare il prodotto è necessario che <span class="math-tag">\( f(x)\)</span> sia un vettore riga. Ciò significa che si deve trasporre <span class="math-tag">\( f(x)\)</span> per cui si ottiene          <span class="math-tag">\begin{aligned}             & {}^{t} \! (A \odot {}^{t} \! x) = x \odot {}^{t} \! A         \end{aligned}</span>         Dunque, ricordando che ogni prodotto scalare può essere associato alla matrice identica (se si scelgono le basi giuste), si ha che la definizione di isometria può essere così scritta         <span class="math-tag">\[              x \odot {}^{t} \! A \odot A \odot {}^{t} \! y = x \odot I \odot {}^{t} \! y         \]</span>         Ciò può avvenire se e solo se <span class="math-tag">\( {}^{t} \! A \odot A = I\)</span>, ovvero se e solo se <span class="math-tag">\( A^{-1} = {}^{t} \! A\)</span> (che è la definizione di matrice ortogonale). Ciò significa che le matrici associate alle isometrie sono esattamente le matrici ortogonali.     </div></div></div><div class="demonstration environment" id="dem8-15" ><span class="demonstration-header environment-title">Dimostrazione 8.15 - 1 autovalore di ogni matrice <span class="math-tag">\( n \times n\)</span> ortogonale, con determinante positivo e <span class="math-tag">\( n\)</span> dispari</span>     Data la proposizione     <div class="proposition environment" ><span class="proposition-header environment-title">Enunciato</span>         Considerando una matrice <span class="math-tag">\( A \in M_{n \times n}(\mathbb{R})\)</span> ortogonale, con il determinante positivo (<span class="math-tag">\( det(A) \gt  0\)</span>) e <span class="math-tag">\( n\)</span> dispari, allora un autovalore di <span class="math-tag">\( A\)</span> è <span class="math-tag">\( 1\)</span>.     </div><div class="proof environment" ><span class="proof-header environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="set_proof_state(event)">visibility_off</span></span><div class="proof-body hyde">         Dimostrare che <span class="math-tag">\( 1\)</span> è autovalore di <span class="math-tag">\( A\)</span>, significa dire che il polinomio caratterisco di <span class="math-tag">\( A\)</span> con <span class="math-tag">\( \lambda = 1\)</span> è uguale a <span class="math-tag">\( 0\)</span>, ovvero         <span class="math-tag">\begin{aligned}             & p_{A}(1) = 0             & \iff         \end{aligned}</span>         che è equivalente a scrivere         <span class="math-tag">\begin{aligned}             & 0 = det(A - 1 \cdot I)             & \iff         \end{aligned}</span>         Allora, ricordando che la trasposta di una matrice ortogonale è anche la sua inversa, è possibile scrivere         <span class="math-tag">\begin{aligned}             & det(A - A \odot {}^{t} \! A)             & \iff         \end{aligned}</span>         e raccogliendo <span class="math-tag">\( A\)</span> si può ottenere          <span class="math-tag">\begin{aligned}             & det(A \odot (I - {}^{t} \! A)             & \iff         \end{aligned}</span>         e per Binet         <span class="math-tag">\begin{aligned}             & det(A) \cdot det(I - {}^{t} \! A)             & \iff         \end{aligned}</span>         e, dato che il determinante di una matrice ortogonale può essere solo <span class="math-tag">\( \pm 1\)</span>, e che per ipotesi il determinante è positivo, si ha che il <span class="math-tag">\( det(A) = 1\)</span>, quindi è equivalente scrivere         <span class="math-tag">\begin{aligned}             & det(I - {}^{t} \! A)             & \iff         \end{aligned}</span>         Dato che il determinante di una matrice trasposta non cambia, si può trasporre la matrice <span class="math-tag">\( A - I\)</span> (che è uguale alla differenza delle trasposte), quindi         <span class="math-tag">\begin{aligned}             & det({}^{t} \! (I - {}^{t} \! A))             & \iff \\             & det(I - A)              & \iff         \end{aligned}</span>          e raccogliendo <span class="math-tag">\( -1\)</span> si ottiene         <span class="math-tag">\begin{aligned}             & det(-(A - I))             & \iff         \end{aligned}</span>          Ricordando che la moltiplicazione di una qualsiasi matrice per <span class="math-tag">\( -1\)</span> inverte il determinante, possiamo vedere <span class="math-tag">\( -(A - I)\)</span> come la moltiplicazione di <span class="math-tag">\( n\)</span> volte per <span class="math-tag">\( -1\)</span>, per cui scrivere <span class="math-tag">\( det(-(A - I))\)</span> è come scrivere         <span class="math-tag">\begin{aligned}             & (-1)^{n} \cdot det(A - I)             & \iff         \end{aligned}</span>          Nel caso quindi <span class="math-tag">\( n\)</span> sia dispari, avremmo che         <span class="math-tag">\[             det(A - I) = -det(A - I)         \]</span>         ovvero che i due determinanti sono uguali, ovvero che il determinante è nullo. Si è quindi dimostrato che <span class="math-tag">\( 1\)</span> è autovalore.     </div></div><div class="mynote environment" ><span class="mynote-header environment-title">Osservazioni personali - Una visione sul mondo</span>         Dire che <span class="math-tag">\( 1\)</span> è un autovalore, significa dire che esiste un vettore la cui immagine è sè stesso per ogni isometria associata ad una matrice con <span class="math-tag">\( n\)</span> dispari.     </div></div></div></div><div class="section part" id="sec9" ><span class="section-header part-title">9 - Geometria analitica nello spazio</span><div class="subsection part" id="subsec9-1" ><span class="subsection-header part-title">9.1 - Rette nello spazio</span></div><div class="subsection part" id="subsec9-2" ><span class="subsection-header part-title">9.2 - Piani nello spazio</span></div><div class="subsection part" id="subsec9-3" ><span class="subsection-header part-title">9.3 - Prodotto vettoriale</span></div></div>
        </div>
    </div>
    <footer class="footer-container">
        <span>
            Created by 
            <typewriting-text class="credits-subtitle" still-time="1000" erasing-speed="150" >
                <word>lorenzoarlo</word>
                <word>Lorenzo Arlotti</word>
            </typewriting-text>
        </span>
    </footer>
</body>

</html>